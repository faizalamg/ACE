{
  "timestamp": "2026-01-06T07:43:02.672608",
  "stats": {
    "total": 152,
    "ace_wins": 124,
    "auggie_wins": 18,
    "ties": 10,
    "ace_errors": 0,
    "auggie_errors": 0,
    "by_category": {
      "ClassDefinitions": {
        "ace": 29,
        "auggie": 0,
        "tie": 1
      },
      "TechnicalIdentifiers": {
        "ace": 13,
        "auggie": 3,
        "tie": 4
      },
      "FunctionPatterns": {
        "ace": 16,
        "auggie": 4,
        "tie": 0
      },
      "Configuration": {
        "ace": 8,
        "auggie": 4,
        "tie": 4
      },
      "ErrorHandling": {
        "ace": 10,
        "auggie": 3,
        "tie": 1
      },
      "AsyncPatterns": {
        "ace": 14,
        "auggie": 1,
        "tie": 0
      },
      "ImportPatterns": {
        "ace": 15,
        "auggie": 0,
        "tie": 0
      },
      "DocumentationPatterns": {
        "ace": 8,
        "auggie": 0,
        "tie": 0
      },
      "EdgeCases": {
        "ace": 11,
        "auggie": 3,
        "tie": 0
      }
    },
    "auggie_wins_detail": [
      {
        "query": "voyage-3 embedding generation",
        "category": "TechnicalIdentifiers",
        "expected": [
          "ace/code_retrieval.py"
        ],
        "ace_files": [
          "ace/hyde_retrieval.py",
          "ace/config.py",
          "ace/gemini_embeddings.py"
        ],
        "auggie_files": [
          "ace/code_retrieval.py"
        ],
        "reason": "Auggie found expected file at rank 1, ACE missed it",
        "ace_advantages": [],
        "auggie_advantages": [
          "Found expected file at rank 1"
        ]
      },
      {
        "query": "pytest-fixture setup",
        "category": "TechnicalIdentifiers",
        "expected": [
          "tests/"
        ],
        "ace_files": [
          "pyproject.toml",
          "tests/integrations/__init__.py",
          "ace/__init__.py"
        ],
        "auggie_files": [
          "tests/conftest.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 2",
        "ace_advantages": [
          "More unique files (5 vs 1)",
          "Better chunk size (49 lines avg)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)"
        ]
      },
      {
        "query": "asyncio-gather parallel",
        "category": "TechnicalIdentifiers",
        "expected": [
          "ace/async_retrieval.py"
        ],
        "ace_files": [
          "ace/async_adaptation.py",
          "ace/async_retrieval.py",
          "compare_ace_auggie_headtohead.py"
        ],
        "auggie_files": [
          "ace/async_retrieval.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 1",
        "ace_advantages": [
          "More unique files (4 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)"
        ]
      },
      {
        "query": "def search function in retrieval",
        "category": "FunctionPatterns",
        "expected": [
          "ace/code_retrieval.py",
          "ace/retrieval.py"
        ],
        "ace_files": [
          "ace/retrieval_optimized.py",
          "ace/code_retrieval.py",
          "ace/retrieval_optimized.py"
        ],
        "auggie_files": [
          "ace/retrieval.py"
        ],
        "reason": "Auggie wins with 4 advantages vs 2",
        "ace_advantages": [
          "More unique files (4 vs 0)",
          "High confidence top score (0.922)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)",
          "Better chunk size (41 lines avg)"
        ]
      },
      {
        "query": "async def retrieve_async",
        "category": "FunctionPatterns",
        "expected": [
          "ace/async_retrieval.py"
        ],
        "ace_files": [
          "ace/unified_memory.py",
          "ace/retrieval.py",
          "ace/async_retrieval.py"
        ],
        "auggie_files": [
          "ace/async_retrieval.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 1",
        "ace_advantages": [
          "More unique files (4 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 3)"
        ]
      },
      {
        "query": "def handle_exception",
        "category": "FunctionPatterns",
        "expected": [
          "ace/resilience.py"
        ],
        "ace_files": [
          "ace/security.py",
          "debug_docs.py",
          "ace/typo_correction.py"
        ],
        "auggie_files": [
          "ace/resilience.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 1",
        "ace_advantages": [
          "More unique files (4 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 4)"
        ]
      },
      {
        "query": "asyncio gather parallel execution",
        "category": "FunctionPatterns",
        "expected": [
          "ace/async_retrieval.py"
        ],
        "ace_files": [
          "ace/async_adaptation.py",
          "ace/async_retrieval.py",
          "compare_ace_auggie_headtohead.py"
        ],
        "auggie_files": [
          "ace/async_retrieval.py",
          "ace/hyde.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 1",
        "ace_advantages": [
          "More unique files (3 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)"
        ]
      },
      {
        "query": "QDRANT_URL connection string",
        "category": "Configuration",
        "expected": [
          "ace/config.py"
        ],
        "ace_files": [
          "ace/scaling.py",
          "ace/hyde_retrieval.py",
          "ace/deduplication.py"
        ],
        "auggie_files": [
          "ace/config.py"
        ],
        "reason": "Auggie found expected file at rank 1, ACE missed it",
        "ace_advantages": [],
        "auggie_advantages": [
          "Found expected file at rank 1"
        ]
      },
      {
        "query": "EMBEDDING_DIMENSION size",
        "category": "Configuration",
        "expected": [
          "ace/config.py"
        ],
        "ace_files": [
          "ace/openai_embeddings.py",
          "ace/gemini_embeddings.py",
          "ace/semantic_scorer.py"
        ],
        "auggie_files": [
          "ace/config.py"
        ],
        "reason": "Auggie found expected file at rank 1, ACE missed it",
        "ace_advantages": [],
        "auggie_advantages": [
          "Found expected file at rank 1"
        ]
      },
      {
        "query": "defaults dict configuration",
        "category": "Configuration",
        "expected": [
          "ace/config.py"
        ],
        "ace_files": [
          "ace/retrieval_presets.py",
          "ace/config.py",
          "ace/retrieval_presets.py"
        ],
        "auggie_files": [
          "ace/config.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 1",
        "ace_advantages": [
          "More unique files (4 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)"
        ]
      },
      {
        "query": "config from dict",
        "category": "Configuration",
        "expected": [
          "ace/config.py"
        ],
        "ace_files": [
          "ace/unified_memory.py",
          "tenant_data/learned_typos.json",
          "benchmarks/manager.py"
        ],
        "auggie_files": [
          "ace/config.py"
        ],
        "reason": "Auggie wins with 4 advantages vs 1",
        "ace_advantages": [
          "More unique files (4 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 5)",
          "Better chunk size (41 lines avg)"
        ]
      },
      {
        "query": "connection error handling",
        "category": "ErrorHandling",
        "expected": [
          "ace/resilience.py"
        ],
        "ace_files": [
          "ace/scaling.py",
          "ace/resilience.py",
          "ace/observability/health.py"
        ],
        "auggie_files": [
          "ace/resilience.py"
        ],
        "reason": "Auggie wins with 4 advantages vs 1",
        "ace_advantages": [
          "More unique files (4 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)",
          "Better chunk size (41 lines avg)"
        ]
      },
      {
        "query": "file not found error handling",
        "category": "ErrorHandling",
        "expected": [
          "ace/code_indexer.py"
        ],
        "ace_files": [
          "ace/code_retrieval.py",
          "analyze_false_positives.py",
          "compare_ace_auggie_headtohead.py"
        ],
        "auggie_files": [
          "ace/code_indexer.py"
        ],
        "reason": "Auggie found expected file at rank 1, ACE missed it",
        "ace_advantages": [],
        "auggie_advantages": [
          "Found expected file at rank 1"
        ]
      },
      {
        "query": "error recovery strategy",
        "category": "ErrorHandling",
        "expected": [
          "ace/resilience.py"
        ],
        "ace_files": [
          "ace/prompts_v2.py",
          "ace/typo_correction.py",
          "ace/chain_of_verification.py"
        ],
        "auggie_files": [
          "ace/resilience.py",
          "ace/roles.py",
          "ace/retrieval_presets.py"
        ],
        "reason": "Auggie found expected file at rank 1, ACE missed it",
        "ace_advantages": [],
        "auggie_advantages": [
          "Found expected file at rank 1"
        ]
      },
      {
        "query": "asyncio gather parallel",
        "category": "AsyncPatterns",
        "expected": [
          "ace/async_retrieval.py"
        ],
        "ace_files": [
          "ace/async_adaptation.py",
          "ace/async_retrieval.py",
          "compare_ace_auggie_headtohead.py"
        ],
        "auggie_files": [
          "ace/async_retrieval.py",
          "ace/async_retrieval.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 1",
        "ace_advantages": [
          "More unique files (4 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)"
        ]
      },
      {
        "query": "email validation regex pattern",
        "category": "EdgeCases",
        "expected": [
          "email_validator.py"
        ],
        "ace_files": [
          "demo_email_validation.py",
          "email_validator.py",
          "email_validator.py"
        ],
        "auggie_files": [
          "email_validator.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 2",
        "ace_advantages": [
          "More unique files (3 vs 0)",
          "High confidence top score (1.165)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)"
        ]
      },
      {
        "query": "metadata filtering namespace",
        "category": "EdgeCases",
        "expected": [
          "ace/unified_memory.py"
        ],
        "ace_files": [
          "ace/retrieval.py",
          "ace/unified_memory.py",
          "tenant_data/learned_typos.json"
        ],
        "auggie_files": [
          "ace/unified_memory.py",
          "ace/unified_memory.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 1",
        "ace_advantages": [
          "More unique files (4 vs 0)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)"
        ]
      },
      {
        "query": "indexing chunking embedding storage",
        "category": "EdgeCases",
        "expected": [
          "ace/code_indexer.py"
        ],
        "ace_files": [
          "ace/gemini_embeddings.py",
          "ace/code_indexer.py",
          "ace/code_retrieval.py"
        ],
        "auggie_files": [
          "ace/code_indexer.py",
          "docs/Fortune100.md",
          "ace/code_chunker.py"
        ],
        "reason": "Auggie wins with 3 advantages vs 2",
        "ace_advantages": [
          "More unique files (4 vs 2)",
          "High confidence top score (0.778)"
        ],
        "auggie_advantages": [
          "Higher rank for expected file (1 vs 2)"
        ]
      }
    ]
  },
  "results": [
    {
      "query": "CodeRetrieval class definition",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/code_retrieval.py",
        "auggie_query.json",
        "compare_auggie_ace.py",
        "debug_blended.py",
        "analyze_false_positives.py"
      ],
      "ace_scores": [
        1.165982,
        0.7632597,
        0.5907428,
        0.44365430000000006,
        0.38992062
      ],
      "ace_contents": [
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "{\"jsonrpc\":\"2.0\",\"method\":\"tools/call\",\"params\":{\"name\":\"codebase-retrieval\",\"arguments\":{\"information_request\":\"CodeRetrieval class search method\"}},\"id\":2}\n",
        "\"\"\"Compare Auggie vs ACE code retrieval quality.\"\"\"\nimport subprocess\nimport sys\n\nquery = \"CodeRetrieval class search method\"\n\n# Get Auggie results\nprint(\"=== AUGGIE OUTPUT ===\")\nresult = subprocess.run(f'auggie context \"{query}\"', capture_output=True, text=True, timeout=30, shell=True)\nprint(result.stdout[:3000] if result.stdout else f\"Error: {result.stderr}\")\n\n# Get ACE results\nprint(\"\\n=== ACE OUTPUT ===\")\nfrom ace.code_retrieval import CodeRetrieval\nretriever = CodeRetrieval()\nresults = retr",
        "#!/usr/bin/env python3\n\"\"\"Debug why code retrieval isn't working in MCP server.\"\"\"\n\nimport os\n\nprint(\"=== ENV CHECK ===\")\nprint(f\"ACE_WORKSPACE_PATH: {os.environ.get('ACE_WORKSPACE_PATH', 'NOT SET')}\")\nprint(f\"QDRANT_URL: {os.environ.get('QDRANT_URL', 'NOT SET')}\")\nprint(f\"ACE_CODE_COLLECTION: {os.environ.get('ACE_CODE_COLLECTION', 'NOT SET')}\")\n\nprint(\"\\n=== QDRANT COLLECTIONS ===\")\nfrom qdrant_client import QdrantClient\nclient = QdrantClient(url=\"http://localhost:6333\")\ncollections = [c.name f",
        "\"\"\"Analyze ACE false positives and memory performance.\"\"\"\n\nfrom ace.code_retrieval import CodeRetrieval\nfrom ace.unified_memory import UnifiedMemoryIndex\n"
      ],
      "ace_line_counts": [
        147,
        2,
        19,
        55,
        5
      ],
      "auggie_files": [
        "ace/code_retrieval.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"\n     2\tCode Retrieval - Semantic search for code with Auggie-style output formatting.\n     3\t\n     4\tThis module provides:\n     5\t1. Semantic search over indexed code chunks\n     6\t2. Auggie MCP-compatible output formatting\n     7\t3. Blended results (code + memory)\n     8\t4. Result deduplication and ranking\n     9\t\n    10\tExample usage:\n    11\t    retriever = CodeRetrieval()\n    12\t    results = retriever.search(\"unified memory index\")\n    13\t    formatted = retriever.format_auggie_style(results)\n... (452 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 3 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.166)",
        "Better chunk size (46 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "UnifiedMemoryIndex class search method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/unified_memory.py"
      ],
      "ace_files": [
        "ace/unified_memory.py",
        "ace/retrieval_optimized.py",
        "ace/unified_memory.py",
        "ace/retrieval.py",
        "ace/retrieval_optimized.py"
      ],
      "ace_scores": [
        1.18593404,
        0.64321045,
        0.60837655,
        0.5696448599999999,
        0.501899
      ],
      "ace_contents": [
        "class UnifiedMemoryIndex:\n    \"\"\"\n    Unified memory index using Qdrant with namespace support.\n\n    Provides:\n    - Hybrid search (dense + sparse/BM25)\n    - Namespace filtering\n    - Batch operations\n    - Integration with ACE SmartBulletIndex\n\n    Usage:\n        >>> index = UnifiedMemoryIndex(qdrant_url=\"http://localhost:6333\")\n        >>> index.create_collection()\n        >>> index.index_bullet(bullet)\n        >>> results = index.retrieve(\"query\", namespace=UnifiedNamespace.USER_PREFS)\n    \"",
        "    def search(\n        self,\n        query: str,\n        limit: int = None,\n        return_metrics: bool = False\n    ) -> List[RetrievalResult] | Tuple[List[RetrievalResult], SearchMetrics]:\n        \"\"\"\n        Search for relevant memories.\n\n        Args:\n            query: Search query\n            limit: Maximum results (default from config)\n            return_metrics: Whether to return search metrics\n\n        Returns:\n            List of RetrievalResult, optionally with SearchMetrics\n        ",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non",
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============"
      ],
      "ace_line_counts": [
        114,
        116,
        693,
        393,
        87
      ],
      "auggie_files": [
        "ace/unified_memory.py"
      ],
      "auggie_contents": [
        "...\n    16\t\n    17\tUsage:\n    18\t    >>> from ace.unified_memory import UnifiedMemoryIndex, UnifiedBullet, UnifiedNamespace\n    19\t    >>> index = UnifiedMemoryIndex(qdrant_url=\"http://localhost:6333\")\n    20\t    >>> bullet = UnifiedBullet(\n    21\t    ...     id=\"test-001\",\n    22\t    ...     namespace=UnifiedNamespace.USER_PREFS,\n    23\t    ...     source=UnifiedSource.USER_FEEDBACK,\n    24\t    ...     content=\"User prefers TypeScript\",\n    25\t    ...     section=\"preferences\"\n    26\t    ... )\n    27\t    >>> index.index_bullet(bullet)\n... (444 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (1.186)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "ASTChunker class chunk method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/code_chunker.py"
      ],
      "ace_files": [
        "ace/code_chunker.py",
        "ace/code_indexer.py",
        "ace/code_indexer.py",
        "ace/code_indexer.py",
        "ace/code_analysis.py"
      ],
      "ace_scores": [
        1.1808355,
        0.41830033,
        0.34028438000000005,
        0.33819380000000004,
        0.2418785
      ],
      "ace_contents": [
        "\"\"\"AST-based semantic code chunking module.\n\nThis module provides intelligent code chunking that respects language syntax\nboundaries (functions, classes, methods) rather than arbitrary line counts.\n\nSupports multiple languages via tree-sitter:\n- Python (via built-in ast module or tree-sitter)\n- JavaScript/TypeScript (via tree-sitter)\n- Go (via tree-sitter)\n\nConfiguration:\n    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    ACE_AST_MAX_LINES: Maximum lines per chunk (def",
        "\"\"\"Code indexer module for workspace code indexing.\n\nThis module provides code indexing capabilities that scan a workspace,\nparse code files using ASTChunker, and store indexed chunks in Qdrant\nfor semantic code search.\n\nConfiguration:\n    ACE_CODE_COLLECTION: Qdrant collection name (default: ace_code_context)\n    ACE_CODE_EMBEDDING_DIM: Embedding dimension (default: from EmbeddingConfig)\n    QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n\nThe indexer supports:\n- Multi-language p",
        "class CodeIndexer:\n    \"\"\"\n    Index workspace code files for semantic search.\n    \n    Scans workspace directories, parses code using ASTChunker,\n    generates embeddings, and stores in Qdrant for retrieval.\n    \n    Features:\n    - Multi-language support via ASTChunker\n    - Incremental updates on file changes\n    - File watching for auto-updates\n    - Gitignore and exclude pattern support\n    - Relative path storage for portability\n    \n    Example:\n        indexer = CodeIndexer(workspace_pat",
        "    def chunk_file(self, file_path: str) -> List[CodeChunkIndexed]:\n        \"\"\"\n        Parse and chunk a code file.\n        \n        Args:\n            file_path: Absolute path to code file\n            \n        Returns:\n            List of CodeChunkIndexed instances\n        \"\"\"\n        chunks = []\n        \n        # Check file exists\n        if not os.path.exists(file_path):\n            logger.warning(f\"File not found: {file_path}\")\n            return chunks\n        \n        # Read file content\n",
        "        def visit_node(node, parent_class=None):\n            \"\"\"Recursively visit AST nodes.\"\"\"\n            # Extract interface definitions\n            if node.type == \"interface_declaration\":\n                name_node = node.child_by_field_name(\"name\")\n                if name_node:\n                    name = code[name_node.start_byte : name_node.end_byte]\n                    symbol = CodeSymbol(\n                        name=name,\n                        kind=\"interface\",\n                       "
      ],
      "ace_line_counts": [
        553,
        55,
        101,
        86,
        95
      ],
      "auggie_files": [
        "ace/code_chunker.py"
      ],
      "auggie_contents": [
        "...\n   103\t\n   104\t\n   105\tclass ASTChunker:\n   106\t    \"\"\"AST-based semantic code chunker.\n   107\t    \n   108\t    Chunks code by language syntax boundaries (functions, classes) rather than\n   109\t    arbitrary line counts. Uses tree-sitter for multi-language support with\n   110\t    fallback to line-based chunking for unsupported languages.\n   111\t    \n   112\t    Configuration via environment variables:\n   113\t        ACE_ENABLE_AST_CHUNKING: \"true\" to enable, anything else to disable\n   114\t        ACE_AST_MAX_LINES: Maximum lines per chunk (default: 120)\n... (397 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.181)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "SmartBulletIndex retrieve method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/retrieval.py"
      ],
      "ace_files": [
        "ace/retrieval.py",
        "ace/qdrant_retrieval.py",
        "ace/retrieval.py",
        "ace/unified_memory.py",
        "ace/scaling.py"
      ],
      "ace_scores": [
        1.2197608999999998,
        0.4532729,
        0.41364779999999995,
        0.3971811,
        0.34705936
      ],
      "ace_contents": [
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"Vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides QdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with hybrid search (dense + BM25 sparse).\n\nPhase 1: Vector Search Integration for ACE Fortune 100 Production Readiness.\n\nKey features:\n- Dense embeddings via LM Studio (nomic-embed-text-v1.5, 768-dim)\n- BM25 sparse vectors for keyword matching (technical terms)\n- Hybrid search with RRF fusion for best of both approaches\n",
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "    def _get_or_create_index(self, collection_name: str) -> QdrantBulletIndex:\n        \"\"\"Get existing index or create new one for collection.\n\n        Args:\n            collection_name: Name of collection\n\n        Returns:\n            QdrantBulletIndex for the collection.\n        \"\"\"\n        if collection_name not in self._indexes:\n            # Production mode - use real QdrantBulletIndex\n            if self._qdrant_client is None:\n                self._indexes[collection_name] = QdrantBulletI"
      ],
      "ace_line_counts": [
        137,
        165,
        393,
        471,
        103
      ],
      "auggie_files": [
        "ace/retrieval.py"
      ],
      "auggie_contents": [
        "...\n    62\t\n    63\t\n    64\tclass SmartBulletIndex:\n    65\t    \"\"\"Purpose-aware retrieval index for playbook bullets.\n    66\t\n    67\t    SmartBulletIndex enables intelligent retrieval of bullets based on:\n    68\t    - Task type filtering (debugging, reasoning, etc.)\n    69\t    - Domain filtering (math, software, etc.)\n    70\t    - Complexity level filtering\n    71\t    - Trigger pattern matching\n    72\t    - Intent-based routing (analytical/factual/procedural)\n    73\t    - Effectiveness-based ranking\n... (508 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (1.220)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "Playbook class initialization",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/playbook.py"
      ],
      "ace_files": [
        "ace/playbook.py",
        "ace/integrations/browser_use.py",
        "ace/playbook.py",
        "ace/security.py",
        "ace/audit.py"
      ],
      "ace_scores": [
        1.0087136,
        0.4900363999999999,
        0.40482408000000003,
        0.39303111999999996,
        0.39143344999999996
      ],
      "ace_contents": [
        "class Playbook:\n    \"\"\"Structured context store as defined by ACE.\"\"\"\n\n    def __init__(self) -> None:\n        self._bullets: Dict[str, Bullet] = {}\n        self._sections: Dict[str, List[str]] = {}\n        self._next_id = 0\n\n    def __repr__(self) -> str:\n        \"\"\"Concise representation for debugging and object inspection.\"\"\"\n        return f\"Playbook(bullets={len(self._bullets)}, sections={list(self._sections.keys())})\"\n\n    def __str__(self) -> str:\n        \"\"\"\n        Human-readable repres",
        "    def load_playbook(self, path: str):\n        \"\"\"Load playbook from file.\"\"\"\n        self.playbook = Playbook.load_from_file(path)\n\n    def get_strategies(self) -> str:\n        \"\"\"Get current playbook strategies as formatted text.\"\"\"\n        if not self.playbook:\n            return \"\"\n        return wrap_playbook_context(self.playbook)",
        "\"\"\"Playbook storage and mutation logic for ACE.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import asdict, dataclass, field\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Union, cast\n\nfrom .delta import DeltaBatch, DeltaOperation\n\n\n# Phase 1C: Asymmetric penalty weights for bullet tagging\n# Harmful tags penalized 2x to suppress bad strategies faster\nPENALTY_WEIGHTS = {\"helpful\": 1, \"harmf",
        "class RoleBasedAccessControl:\n    \"\"\"Role-based access control with hierarchical permissions.\"\"\"\n\n    # Default role permissions\n    DEFAULT_PERMISSIONS = {\n        \"admin\": [\n            \"playbook:read\",\n            \"playbook:write\",\n            \"playbook:delete\",\n            \"delta:create\",\n            \"role:assign\"\n        ],\n        \"editor\": [\n            \"playbook:read\",\n            \"playbook:write\",\n            \"delta:create\"\n        ],\n        \"user\": [\n            \"playbook:read\"\n      ",
        "    def log_playbook(\n        self,\n        playbook_id: str,\n        action: str,\n        bullet_count: int,\n        user_id: Optional[str] = None,\n    ) -> AuditEntry:\n        \"\"\"Log a playbook operation.\n\n        Args:\n            playbook_id: Playbook identifier\n            action: Operation action (e.g., \"load\", \"save\")\n            bullet_count: Number of bullets in playbook\n            user_id: Optional user identifier\n\n        Returns:\n            Created audit entry\n        \"\"\"\n        e"
      ],
      "ace_line_counts": [
        201,
        9,
        108,
        117,
        104
      ],
      "auggie_files": [
        "ace/playbook.py"
      ],
      "auggie_contents": [
        "...\n    17\t\n    18\t\n    19\t@dataclass\n    20\tclass Bullet:\n    21\t    \"\"\"Single playbook entry.\"\"\"\n    22\t\n    23\t    id: str\n    24\t    section: str\n    25\t    content: str\n    26\t    helpful: int = 0\n    27\t    harmful: int = 0\n    28\t    neutral: int = 0\n... (486 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace/playbook.py\"\n   search_query_regex: \"class Playbook|def __init__\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (1.009)"
      ],
      "auggie_advantages": [
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "EmbeddingConfig dataclass definition",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/semantic_scorer.py",
        "ace/gemini_embeddings.py",
        "ace/openai_embeddings.py",
        "ace/gemini_embeddings.py"
      ],
      "ace_scores": [
        1.1980787,
        0.4495734,
        0.36417596,
        0.36216215999999996,
        0.31660406999999996
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "def get_gemini_embedding(\n    text: str,\n    task_type: TaskType = \"SEMANTIC_SIMILARITY\",\n    api_key: Optional[str] = None,\n) -> List[float]:\n    \"\"\"Quick embedding without client management.\n\n    Args:\n        text: Text to embed\n        task_type: Embedding task type\n        api_key: Optional API key\n\n    Returns:\n        Embedding vector\n    \"\"\"\n    with GeminiEmbeddingClient(api_key=api_key) as client:\n        return client._embed_single(text, task_type)"
      ],
      "ace_line_counts": [
        117,
        22,
        88,
        113,
        17
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "...\n    41\t\n    42\t\n    43\t@dataclass\n    44\tclass EmbeddingConfig:\n    45\t    \"\"\"Embedding model configuration for memory/lessons (general-purpose).\"\"\"\n    46\t\n    47\t    # LM Studio server\n    48\t    url: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_URL\", \"http://192.168.10.64:1234\"))\n    49\t\n    50\t    # Model name (Qwen3-Embedding-8B - proper embedding model, 4096 dims)\n    51\t    model: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_MODEL\", \"text-embedding-qwen3-embedding-8b\"))\n    52\t\n... (524 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 3 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.198)",
        "Better chunk size (71 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "QdrantConfig class definition",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/unified_memory.py",
        "ace/qdrant_retrieval.py",
        "ace/unified_memory.py",
        "ace/hyde_retrieval.py"
      ],
      "ace_scores": [
        1.2258284,
        0.4861826,
        0.4811639,
        0.44512430000000003,
        0.43687346000000005
      ],
      "ace_contents": [
        "class QdrantConfig:\n    \"\"\"Qdrant vector database configuration.\"\"\"\n\n    # Qdrant server URL\n    url: str = field(default_factory=lambda: _get_env(\"ACE_QDRANT_URL\", \"http://localhost:6333\"))\n\n    # Collection names (ace_memories_hybrid is the canonical 4096-dim collection)\n    memories_collection: str = field(default_factory=lambda: _get_env(\"ACE_MEMORIES_COLLECTION\", \"ace_memories_hybrid\"))\n    unified_collection: str = field(default_factory=lambda: _get_env(\"ACE_UNIFIED_COLLECTION\", \"ace_memor",
        "class UnifiedMemoryIndex:\n    \"\"\"\n    Unified memory index using Qdrant with namespace support.\n\n    Provides:\n    - Hybrid search (dense + sparse/BM25)\n    - Namespace filtering\n    - Batch operations\n    - Integration with ACE SmartBulletIndex\n\n    Usage:\n        >>> index = UnifiedMemoryIndex(qdrant_url=\"http://localhost:6333\")\n        >>> index.create_collection()\n        >>> index.index_bullet(bullet)\n        >>> results = index.retrieve(\"query\", namespace=UnifiedNamespace.USER_PREFS)\n    \"",
        "\"\"\"Vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides QdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with hybrid search (dense + BM25 sparse).\n\nPhase 1: Vector Search Integration for ACE Fortune 100 Production Readiness.\n\nKey features:\n- Dense embeddings via LM Studio (nomic-embed-text-v1.5, 768-dim)\n- BM25 sparse vectors for keyword matching (technical terms)\n- Hybrid search with RRF fusion for best of both approaches\n",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\"",
        "\"\"\"HyDE-enhanced retrieval pipeline for ACE memory system.\n\nIntegrates HyDE (Hypothetical Document Embeddings) with existing hybrid search\ninfrastructure for improved retrieval accuracy on ambiguous/implicit queries.\n\nPipeline:\n1. Query -> HyDE expansion -> Generate hypothetical documents\n2. Embed hypotheticals -> Average embeddings\n3. Search Qdrant with averaged embedding + BM25 sparse\n4. Return results with hybrid RRF fusion\n\nPerformance target: +5-10% for implicit/scenario/template queries\n\"\""
      ],
      "ace_line_counts": [
        101,
        114,
        165,
        118,
        39
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "...\n    98\t\n    99\t    # API key (required)\n   100\t    api_key: str = field(default_factory=lambda: _get_env(\"VOYAGE_API_KEY\", \"\"))\n   101\t\n   102\t    # Model name (voyage-code-3 - code-optimized)\n   103\t    model: str = field(default_factory=lambda: _get_env(\"ACE_VOYAGE_MODEL\", \"voyage-code-3\"))\n   104\t\n   105\t    # Embedding dimension (1024d default, options: 256, 512, 1024, 2048)\n   106\t    dimension: int = field(default_factory=lambda: _get_env_int(\"ACE_VOYAGE_DIMENSION\", 1024))\n   107\t\n   108\t    # Max input length (tokens) - 32K for voyage-code-3\n   109\t    max_input_tokens: int = field(default_factory=lambda: _get_env_int(\"ACE_VOYAGE_MAX_TOKENS\", 32000))\n... (501 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.226)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "BM25Config k1 b parameters",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/unified_memory.py",
        "ace/retrieval_presets.py",
        "benchmark_results/ab_test_results.json",
        "ace/retrieval_presets.py"
      ],
      "ace_scores": [
        0.808977,
        0.5387472999999999,
        0.5190691000000001,
        0.51102534,
        0.48792126999999996
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\"",
        "\"\"\"\nRetrieval Presets - Optimized configurations for different query types.\n\nBased on empirical testing:\n- Baseline precision: 75.6%\n- Architecture queries: 33.3% (worst)\n- Target: 95%+ precision across all categories\n\nWinning optimizations:\n1. BM25-heavy weighting (dense=0.3, sparse=0.7) -> +50% P@3\n2. Post-retrieval deduplication (0.90 threshold) -> +2.7%\n3. Query expansion with domain synonyms -> +3% (conditional)\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing",
        "{\n  \"timestamp\": \"2026-01-02 14:27:04\",\n  \"n_queries\": 30,\n  \"results\": [\n    {\n      \"config\": {\n        \"name\": \"A: Baseline\",\n        \"description\": \"Pure vector search, no optimizations\",\n        \"use_cross_encoder\": false,\n        \"use_llm_expansion\": false,\n        \"auto_detect_preset\": false\n      },\n      \"metrics\": {\n        \"recall_at_1\": 66.66666666666666,\n        \"recall_at_5\": 83.33333333333334,\n        \"precision_at_3\": 58.88888888888888,\n        \"avg_latency_ms\": 565.6736214955648",
        "def get_preset_config(preset: RetrievalPreset) -> RetrievalConfig:\n    \"\"\"Get configuration for a preset.\"\"\"\n    return PRESET_CONFIGS.get(preset, PRESET_CONFIGS[RetrievalPreset.BALANCED])\n\n\ndef cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0\n\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    norm1 = math.sqrt(sum(a * a for a in vec1))\n"
      ],
      "ace_line_counts": [
        547,
        222,
        58,
        66,
        87
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized ACE configuration.\n     2\t\n     3\tAll embedding and retrieval settings in one place.\n     4\tOverride via environment variables or .env file.\n     5\t\"\"\"\n     6\t\n     7\timport os\n     8\tfrom dataclasses import dataclass, field\n     9\tfrom typing import Optional\n    10\tfrom pathlib import Path\n    11\t\n    12\t# Load .env if python-dotenv is available\n    13\ttry:\n... (523 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.809)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "CodeIndexer index_workspace method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/code_indexer.py"
      ],
      "ace_files": [
        "ace/code_indexer.py",
        "ace/code_indexer.py",
        "ace_mcp_server.py",
        "ace/code_indexer.py",
        "ace/file_watcher_daemon.py"
      ],
      "ace_scores": [
        1.20931265,
        0.5353975999999999,
        0.41589817000000007,
        0.38805644999999994,
        0.36180576000000003
      ],
      "ace_contents": [
        "class CodeIndexer:\n    \"\"\"\n    Index workspace code files for semantic search.\n    \n    Scans workspace directories, parses code using ASTChunker,\n    generates embeddings, and stores in Qdrant for retrieval.\n    \n    Features:\n    - Multi-language support via ASTChunker\n    - Incremental updates on file changes\n    - File watching for auto-updates\n    - Gitignore and exclude pattern support\n    - Relative path storage for portability\n    \n    Example:\n        indexer = CodeIndexer(workspace_pat",
        "    def index_workspace(self) -> Dict[str, Any]:\n        \"\"\"\n        Index entire workspace with batch embedding for speed.\n        \n        Uses Voyage batch API to embed up to 128 chunks per request,\n        making indexing ~100x faster than individual API calls.\n        \n        Returns:\n            Statistics dict with files_indexed, chunks_indexed, etc.\n        \"\"\"\n        stats = {\n            \"files_indexed\": 0,\n            \"chunks_indexed\": 0,\n            \"files_skipped\": 0,\n            ",
        "async def ace_onboard(\n    workspace_name: str = \"\",\n    ctx: Context = None,\n) -> str:\n    \"\"\"Onboard the current workspace to ACE.\n\n    This tool should be called when a workspace is detected but not yet onboarded\n    (no .ace/.ace.json file exists).\n\n    Onboarding will:\n    1. Create .ace/.ace.json configuration file with workspace name\n    2. Index all code in the workspace for semantic search\n    3. Enable workspace-specific code retrieval (isolated from other projects)\n\n    After onboardi",
        "\"\"\"Code indexer module for workspace code indexing.\n\nThis module provides code indexing capabilities that scan a workspace,\nparse code files using ASTChunker, and store indexed chunks in Qdrant\nfor semantic code search.\n\nConfiguration:\n    ACE_CODE_COLLECTION: Qdrant collection name (default: ace_code_context)\n    ACE_CODE_EMBEDDING_DIM: Embedding dimension (default: from EmbeddingConfig)\n    QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n\nThe indexer supports:\n- Multi-language p",
        "#!/usr/bin/env python3\n\"\"\"ACE File Watcher Daemon - Persistent background file watcher.\n\nThis daemon runs as a background process and monitors workspace files\nfor changes, automatically reindexing when code is modified.\n\nUsage:\n    python -m ace.file_watcher_daemon start /path/to/workspace\n    python -m ace.file_watcher_daemon stop /path/to/workspace\n    python -m ace.file_watcher_daemon status /path/to/workspace\n\nThe daemon stores its PID in .ace/.watcher.pid and logs to .ace/.watcher.log\n\"\"\"\n\n"
      ],
      "ace_line_counts": [
        101,
        224,
        97,
        55,
        114
      ],
      "auggie_files": [
        "ace/code_indexer.py"
      ],
      "auggie_contents": [
        "...\n   193\t\n   194\tclass CodeIndexer:\n   195\t    \"\"\"\n   196\t    Index workspace code files for semantic search.\n   197\t    \n   198\t    Scans workspace directories, parses code using ASTChunker,\n   199\t    generates embeddings, and stores in Qdrant for retrieval.\n   200\t    \n   201\t    Features:\n   202\t    - Multi-language support via ASTChunker\n   203\t    - Incremental updates on file changes\n   204\t    - File watching for auto-updates\n... (472 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (2 vs 0)",
        "High confidence top score (1.209)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "HyDEGenerator class generate method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/hyde.py"
      ],
      "ace_files": [
        "ace/hyde.py",
        "ace/roles.py",
        "ace/hyde_retrieval.py",
        "ace/self_consistency.py",
        "examples/hyde_example.py"
      ],
      "ace_scores": [
        0.79866727,
        0.7027561499999999,
        0.44137126000000004,
        0.39485977999999994,
        0.26493878
      ],
      "ace_contents": [
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"Generator, Reflector, and Curator components.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom .delta import DeltaBatch\nfrom .llm import LLMClient\nfrom .playbook import Playbook\nfrom .prompts import CURATOR_PROMPT, GENERATOR_PROMPT, REFLECTOR_PROMPT\n\n# Import Opik tracing with graceful degradation\ntry:\n    from .observability.tracers import maybe_track\nexcept ImportError:",
        "\"\"\"HyDE-enhanced retrieval pipeline for ACE memory system.\n\nIntegrates HyDE (Hypothetical Document Embeddings) with existing hybrid search\ninfrastructure for improved retrieval accuracy on ambiguous/implicit queries.\n\nPipeline:\n1. Query -> HyDE expansion -> Generate hypothetical documents\n2. Embed hypotheticals -> Average embeddings\n3. Search Qdrant with averaged embedding + BM25 sparse\n4. Return results with hybrid RRF fusion\n\nPerformance target: +5-10% for implicit/scenario/template queries\n\"\"",
        "\"\"\"Self-consistency sampling for improved generation accuracy.\n\nThis module implements self-consistency decoding, which generates multiple\nresponses for the same prompt and selects the most consistent answer via\nmajority voting. This technique improves accuracy for tasks where reasoning\npaths can vary but the final answer should converge.\n\nReference: Wang et al., \"Self-Consistency Improves Chain of Thought Reasoning\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom collections import Co",
        "\"\"\"HyDE (Hypothetical Document Embeddings) Usage Example.\n\nThis example demonstrates how to use HyDE to improve retrieval accuracy\nfor short, ambiguous queries by generating hypothetical answer documents.\n\nRequirements:\n- Qdrant running at http://localhost:6333\n- LM Studio embeddings at http://192.168.10.64:1234\n- ZAI_API_KEY or OPENAI_API_KEY in .env\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\n\n# Add parent directory to path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom ace"
      ],
      "ace_line_counts": [
        242,
        450,
        419,
        189,
        135
      ],
      "auggie_files": [
        "ace/hyde.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n     2\t\n     3\tThis module implements HyDE for bridging the semantic gap between short queries\n     4\tand detailed memory documents. HyDE transforms queries into hypothetical documents\n     5\tthat would answer the query, then uses their embeddings for more accurate retrieval.\n     6\t\n     7\tKey Features:\n     8\t- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n     9\t- Configurable number of hypothetical documents (default: 3-5)\n    10\t- Caching for repeated queries\n    11\t- Async support for batch processing\n    12\t- Optimized for memory retrieval domain\n    13\t\n... (472 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.799)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "VoyageCodeEmbeddingConfig api_key model",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/code_retrieval.py",
        "docs/CODE_EMBEDDING_CONFIG.md",
        "ace/gemini_embeddings.py",
        "ace/semantic_scorer.py"
      ],
      "ace_scores": [
        1.245946,
        0.4830078,
        0.32415869999999997,
        0.3151724,
        0.29002182
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "class CodeRetrieval:\n    \"\"\"Semantic code search with Auggie-style output formatting.\n    \n    Queries indexed code chunks and formats results in a way compatible\n    with Auggie MCP output, supporting blended code + memory results.\n    \n    Uses Voyage-code-3 embeddings (1024d) for optimal code semantic \n    understanding compared to general-purpose embeddings.\n    \n    Configuration via environment variables:\n        QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n        ACE_CO",
        "# ACE Code Embedding Configuration\n\n## Overview\n\nACE uses a **dual-embedding architecture** for optimal retrieval quality:\n\n1. **Memory Embeddings**: `Qwen3-Embedding-8B` (4096d) for lessons, preferences, corrections\n2. **Code Embeddings**: `Voyage-code-3` (1024d) for code context retrieval\n\nThis separation allows each domain to use embeddings specifically trained for its content type.\n\n## Why Voyage-code-3 for Code?\n\nAfter benchmarking multiple embedding models for code retrieval:\n\n| Model | Di",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_"
      ],
      "ace_line_counts": [
        117,
        108,
        126,
        88,
        22
      ],
      "auggie_files": [
        "docs/CODE_EMBEDDING_CONFIG.md"
      ],
      "auggie_contents": [
        "     1\t# ACE Code Embedding Configuration\n     2\t\n     3\t## Overview\n     4\t\n     5\tACE uses a **dual-embedding architecture** for optimal retrieval quality:\n     6\t\n     7\t1. **Memory Embeddings**: `Qwen3-Embedding-8B` (4096d) for lessons, preferences, corrections\n     8\t2. **Code Embeddings**: `Voyage-code-3` (1024d) for code context retrieval\n     9\t\n    10\tThis separation allows each domain to use embeddings specifically trained for its content type.\n    11\t\n    12\t## Why Voyage-code-3 for Code?\n    13\t\n... (481 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "LLMConfig provider model settings",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/llm_providers/litellm_client.py",
        "ace/llm_providers/__init__.py",
        "ace/retrieval_optimized.py",
        "ace/retrieval_presets.py"
      ],
      "ace_scores": [
        1.14325426,
        0.83997247,
        0.45755014000000005,
        0.4258807,
        0.37242814
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L",
        "\"\"\"Production LLM client implementations for ACE.\"\"\"\n\nfrom typing import Optional\nfrom .litellm_client import LiteLLMClient, LiteLLMConfig\n\nLangChainLiteLLMClient: Optional[type]\n\ntry:\n    from .langchain_client import LangChainLiteLLMClient as _LangChainLiteLLMClient\n\n    LangChainLiteLLMClient = _LangChainLiteLLMClient  # type: ignore[assignment]\nexcept ImportError:\n    LangChainLiteLLMClient = None  # Optional dependency  # type: ignore[assignment]\n\n__all__ = [\n    \"LiteLLMClient\",\n    \"LiteL",
        "class LLMQueryRewriter:\n    \"\"\"\n    LLM-based query rewriting for short, ambiguous queries.\n    Uses Z.ai GLM to expand short queries into richer semantic variations.\n    \"\"\"\n\n    # Domain context for the ACE memory knowledge base\n    DOMAIN_CONTEXT = \"\"\"The knowledge base contains:\n- User preferences (coding style, tool choices, workflow patterns)\n- Task strategies (debugging approaches, optimization techniques)\n- Error patterns and fixes (common bugs, solutions, root causes)\n- Configuration be",
        "def expand_query_with_llm(\n    query: str,\n    llm_url: Optional[str] = None,\n    model: Optional[str] = None,\n    timeout: Optional[float] = None,\n) -> List[str]:\n    \"\"\"\n    Use LLM (GLM 4.6) to semantically expand query for better retrieval.\n\n    This generates related terms and rephrased queries that capture\n    the semantic intent beyond simple synonym matching.\n\n    All settings default to values from ace.config.LLMConfig.\n\n    Args:\n        query: Original query string\n        llm_url: Z."
      ],
      "ace_line_counts": [
        611,
        300,
        20,
        67,
        382
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "RetrievalConfig limit threshold",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/retrieval_presets.py",
        "ace/config.py",
        "ace/retrieval_presets.py",
        "ace/retrieval_optimized.py",
        "ace/retrieval_bandit.py"
      ],
      "ace_scores": [
        1.1682625999999998,
        1.12790225,
        0.4969671,
        0.4280176,
        0.31555145
      ],
      "ace_contents": [
        "\"\"\"\nRetrieval Presets - Optimized configurations for different query types.\n\nBased on empirical testing:\n- Baseline precision: 75.6%\n- Architecture queries: 33.3% (worst)\n- Target: 95%+ precision across all categories\n\nWinning optimizations:\n1. BM25-heavy weighting (dense=0.3, sparse=0.7) -> +50% P@3\n2. Post-retrieval deduplication (0.90 threshold) -> +2.7%\n3. Query expansion with domain synonyms -> +3% (conditional)\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing",
        "class QdrantConfig:\n    \"\"\"Qdrant vector database configuration.\"\"\"\n\n    # Qdrant server URL\n    url: str = field(default_factory=lambda: _get_env(\"ACE_QDRANT_URL\", \"http://localhost:6333\"))\n\n    # Collection names (ace_memories_hybrid is the canonical 4096-dim collection)\n    memories_collection: str = field(default_factory=lambda: _get_env(\"ACE_MEMORIES_COLLECTION\", \"ace_memories_hybrid\"))\n    unified_collection: str = field(default_factory=lambda: _get_env(\"ACE_UNIFIED_COLLECTION\", \"ace_memor",
        "def get_preset_config(preset: RetrievalPreset) -> RetrievalConfig:\n    \"\"\"Get configuration for a preset.\"\"\"\n    return PRESET_CONFIGS.get(preset, PRESET_CONFIGS[RetrievalPreset.BALANCED])\n\n\ndef cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0\n\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    norm1 = math.sqrt(sum(a * a for a in vec1))\n",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"\nLinUCB Contextual Bandit for Adaptive Retrieval Strategy Selection\n\nPart of P7 ARIA (Adaptive Retrieval Intelligence Architecture).\n\nImplements P7.3 LinUCB algorithm with 4-arm retrieval strategy:\n- FAST: Low latency, minimal context\n- BALANCED: Moderate speed/quality tradeoff (cold start default)\n- DEEP: Maximum semantic depth\n- DIVERSE: Multi-perspective retrieval\n\nFormula: UCB(arm) = theta^T * x + alpha * sqrt(x^T * A^-1 * x)\nwhere:\n- theta = A^-1 * b (parameter estimate)\n- A = sum of x*x"
      ],
      "ace_line_counts": [
        58,
        101,
        87,
        91,
        120
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "...\n   184\t\n   185\t\n   186\t@dataclass\n   187\tclass RetrievalConfig:\n   188\t    \"\"\"Retrieval pipeline configuration.\"\"\"\n   189\t\n   190\t    # Number of candidates per query\n   191\t    candidates_per_query: int = field(default_factory=lambda: _get_env_int(\"ACE_CANDIDATES_PER_QUERY\", 20))\n   192\t\n   193\t    # First stage retrieval limit (initial_k - before reranking/filtering)\n   194\t    first_stage_k: int = field(default_factory=lambda: _get_env_int(\"ACE_FIRST_STAGE_K\", 40))\n   195\t    initial_k: int = field(default_factory=lambda: _get_env_int(\"ACE_INITIAL_K\", 100))  # Alias for first_stage_k\n... (424 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.168)",
        "Better chunk size (91 lines avg)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "HyDEConfig num_hypotheticals temperature",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py",
        "ace/hyde.py"
      ],
      "ace_files": [
        "ace/hyde.py",
        "ace/hyde_retrieval.py",
        "examples/hyde_example.py",
        "rag_training/optimizations/v6_hyde.py",
        "rag_training/optimizations/v7_fortune100_combined.py"
      ],
      "ace_scores": [
        1.19252615,
        0.6736336,
        0.48521559999999997,
        0.41966254,
        0.35007439999999995
      ],
      "ace_contents": [
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"HyDE-enhanced retrieval pipeline for ACE memory system.\n\nIntegrates HyDE (Hypothetical Document Embeddings) with existing hybrid search\ninfrastructure for improved retrieval accuracy on ambiguous/implicit queries.\n\nPipeline:\n1. Query -> HyDE expansion -> Generate hypothetical documents\n2. Embed hypotheticals -> Average embeddings\n3. Search Qdrant with averaged embedding + BM25 sparse\n4. Return results with hybrid RRF fusion\n\nPerformance target: +5-10% for implicit/scenario/template queries\n\"\"",
        "\"\"\"HyDE (Hypothetical Document Embeddings) Usage Example.\n\nThis example demonstrates how to use HyDE to improve retrieval accuracy\nfor short, ambiguous queries by generating hypothetical answer documents.\n\nRequirements:\n- Qdrant running at http://localhost:6333\n- LM Studio embeddings at http://192.168.10.64:1234\n- ZAI_API_KEY or OPENAI_API_KEY in .env\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\n\n# Add parent directory to path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom ace",
        "\"\"\"\nV6: HyDE (Hypothetical Document Embeddings) Optimization\n=========================================================\n\n**Technique**: HyDE - Generate hypothetical documents that would answer the query,\nthen use averaged embeddings for semantic search.\n\n**Pipeline**:\n1. Query -> LLM generates 3-5 hypothetical answer documents\n2. Embed each hypothetical document\n3. Average embeddings into single vector\n4. Search Qdrant with averaged embedding + BM25 sparse (RRF fusion)\n\n**Expected Improvement**: ",
        "\"\"\"\nV7: Fortune 100 Combined Pipeline - Target 95%+ Recall@5\n=========================================================\n\nCombines ALL optimizations for maximum retrieval accuracy:\n1. HyDE (Hypothetical Document Embeddings) for query expansion\n2. Fine-tuned BGE-large embeddings (1024 dims) for domain adaptation\n3. BGE cross-encoder reranker for precision\n\nPipeline:\n  Query -> HyDE hypotheticals -> BGE-large embeddings -> Qdrant hybrid search\n       -> RRF fusion -> BGE reranker -> Final top-K\n\nTar"
      ],
      "ace_line_counts": [
        318,
        419,
        135,
        163,
        119
      ],
      "auggie_files": [
        "ace/hyde.py"
      ],
      "auggie_contents": [
        "...\n    31\t\n    32\t\n    33\t@dataclass\n    34\tclass HyDEConfig:\n    35\t    \"\"\"Configuration for HyDE hypothetical document generation.\"\"\"\n    36\t\n    37\t    # Generation parameters\n    38\t    num_hypotheticals: int = 3  # Number of hypothetical documents to generate\n    39\t    max_tokens: int = 150  # Max tokens per hypothetical document\n    40\t    temperature: float = 0.7  # Higher temperature for diversity\n    41\t\n    42\t    # LLM configuration\n... (495 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.193)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "SemanticScorer score calculation method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/semantic_scorer.py"
      ],
      "ace_files": [
        "ace/semantic_scorer.py",
        "ace/retrieval.py",
        "ace/retrieval_optimized.py",
        "ace/embedding_finetuning/models/ace_finetuned/tokenizer.json",
        "rag_training/optimizations/v7_fortune100_combined.py"
      ],
      "ace_scores": [
        0.33538739999999995,
        0.27225684,
        0.26500164,
        0.15964195000000003,
        0.15312966000000006
      ],
      "ace_contents": [
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "    def semantic_search(\n        self,\n        query: str,\n        threshold: float = 0.0,\n        limit: Optional[int] = None,\n    ) -> List[ScoredBullet]:\n        \"\"\"Search bullets using semantic similarity.\n\n        Note: This is a simplified implementation using keyword overlap.\n        For production use, integrate with an embedding model.\n\n        Args:\n            query: Natural language query\n            threshold: Minimum similarity threshold (0.0 to 1.0)\n            limit: Maximum numb",
        "    def _score_entities(self, query: str) -> float:\n        \"\"\"Score based on presence of specific entities.\"\"\"\n        score = 0.0\n        \n        # Version numbers are highly specific\n        if self.VERSION_PATTERN.search(query):\n            score += 0.4\n        \n        # File paths are highly specific\n        if self.PATH_PATTERN.search(query):\n            score += 0.3\n        \n        # Error codes are highly specific\n        if self.ERROR_CODE_PATTERN.search(query):\n            score += ",
        "      \"semantic\": 21641,\n      \"taut\": 21642,\n      \"dune\": 21643,\n      \"inventions\": 21644,\n      \"succeeds\": 21645,\n      \"##iter\": 21646,\n      \"replication\": 21647,\n      \"branched\": 21648,\n      \"##pired\": 21649,\n      \"jul\": 21650,\n      \"prosecuted\": 21651,\n      \"kangaroo\": 21652,\n      \"penetrated\": 21653,\n      \"##avian\": 21654,\n      \"middlesbrough\": 21655,\n      \"doses\": 21656,\n      \"bleak\": 21657,\n      \"madam\": 21658,\n      \"predatory\": 21659,\n      \"relentless\": 21660,\n      \"##",
        "def calculate_mrr(retrieved_ids: List[str], ground_truth_id: str) -> float:\n    try:\n        rank = retrieved_ids.index(ground_truth_id) + 1\n        return 1.0 / rank\n    except ValueError:\n        return 0.0\n\n\ndef calculate_ndcg_at_k(retrieved_ids: List[str], ground_truth_id: str, k: int) -> float:\n    if ground_truth_id not in retrieved_ids[:k]:\n        return 0.0\n    rank = retrieved_ids[:k].index(ground_truth_id) + 1\n    dcg = 1.0 / math.log2(rank + 1)\n    idcg = 1.0 / math.log2(2)\n    retur"
      ],
      "ace_line_counts": [
        159,
        120,
        26,
        120,
        71
      ],
      "auggie_files": [
        "ace/semantic_scorer.py"
      ],
      "auggie_contents": [
        "     1\t#!/usr/bin/env python\n     2\t\"\"\"\n     3\tSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n     4\t\n     5\tUses embedding cosine similarity instead of keyword matching to measure\n     6\thow relevant retrieved results are to the original query.\n     7\t\n     8\tThis provides a more accurate quality metric than keyword-based precision.\n     9\t\"\"\"\n    10\t\n    11\timport os\n    12\timport sys\n    13\timport numpy as np\n... (464 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "Better chunk size (99 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "DependencyGraph build method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/dependency_graph.py"
      ],
      "ace_files": [
        "ace/dependency_graph.py",
        "ace/dependency_graph.py",
        "ace/embedding_finetuning/models/ace_finetuned/vocab.txt",
        "docs/Fortune100.md",
        "ace/embedding_finetuning/training_data.json"
      ],
      "ace_scores": [
        1.02611277,
        0.26710239999999996,
        0.07825435000000003,
        0.062240799999999985,
        0.060070780000000046
      ],
      "ace_contents": [
        "\"\"\"Dependency graph analysis for code understanding.\n\nExtracts imports, function calls, and dependency relationships from source code\nusing tree-sitter for multiple programming languages.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nimport re\n\n\n@dataclass\nclass Import:\n    \"\"\"Represents an import statement in source code.\"\"\"\n\n    module: str\n    names: List[str] = field(default_factory=list)\n    alias: Optional[str] = None\n    lin",
        "    def build_call_graph(self, code: str, language: str) -> List[CallEdge]:\n        \"\"\"Build function call graph from code.\n\n        Args:\n            code: Source code string\n            language: Programming language\n\n        Returns:\n            List of CallEdge objects representing caller->callee relationships\n        \"\"\"\n        language = language.lower()\n\n        if language == \"python\":\n            return self._build_python_call_graph(code)\n        elif language in (\"javascript\", \"typesc",
        "solutions\n##ery\npointing\nrequested\nperu\nreed\nchancellor\nknights\nmask\nworker\neldest\nflames\nreduction\n1860\nvolunteers\n##tis\nreporting\n##hl\nwire\nadvisory\nendemic\norigins\nsettlers\npursue\nknock\nconsumer\n1876\neu\ncompound\ncreatures\nmansion\nsentenced\nivan\ndeployed\nguitars\nfrowned\ninvolves\nmechanism\nkilometers\nperspective\nshops\nmaps\nterminus\nduncan\nalien\nfist\nbridges\n##pers\nheroes\nfed\nderby\nswallowed\n##ros\npatent\nsara\nillness\ncharacterized\nadventures\nslide\nhawaii\njurisdiction\n##op\norganised\n##side\nadelai",
        "\n## Phase 2 Implementation Summary\n\n### Deliverables\n\n| File | Description | Tests |\n|------|-------------|-------|\n| `ace/code_analysis.py` | **NEW** - CodeAnalyzer with tree-sitter multi-language AST parsing | 35 tests |\n| `ace/code_enrichment.py` | **NEW** - CodeAwareEnricher for code-specific bullet enrichment | 20 tests |\n| `ace/dependency_graph.py` | **NEW** - DependencyGraph for import/call analysis | 19 tests |\n| `tests/test_code_analysis.py` | **NEW** - Comprehensive tree-sitter test su",
        "        \"Update dependencies before building to patch known vulnerabilities.\",\n        \"Create symlinks for local packages instead of npm link to avoid dependency conflicts.\",\n        \"Validate all function return types to prevent type-related bugs early.\"\n      ],\n      \"metadata\": {\n        \"memory_id\": 3215809140,\n        \"category\": \"ARCHITECTURE\",\n        \"difficulty\": \"medium\",\n        \"query_category\": \"template\"\n      }\n    },\n    {\n      \"query\": \"I'm working on a project and need to kn"
      ],
      "ace_line_counts": [
        136,
        214,
        120,
        120,
        220
      ],
      "auggie_files": [
        "ace/dependency_graph.py"
      ],
      "auggie_contents": [
        "...\n    30\t\n    31\t\n    32\tclass DependencyGraph:\n    33\t    \"\"\"Analyzes code dependencies and call graphs across multiple languages.\"\"\"\n    34\t\n    35\t    def __init__(self, analyzer=None):\n    36\t        \"\"\"Initialize with optional CodeAnalyzer.\n    37\t\n    38\t        Args:\n    39\t            analyzer: Optional CodeAnalyzer instance (lazy-loaded if None)\n    40\t        \"\"\"\n    41\t        self._analyzer = analyzer\n... (498 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (1.026)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "FileWatcher callback handler",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/file_watcher_daemon.py"
      ],
      "ace_files": [
        "ace/code_indexer.py",
        "ace/file_watcher_daemon.py",
        "ace/code_indexer.py",
        "ace/code_indexer.py",
        ".serena/memories/ace_file_watcher_daemon.md"
      ],
      "ace_scores": [
        0.32110046,
        0.32108659999999994,
        0.27055155,
        0.2596892,
        0.2427512
      ],
      "ace_contents": [
        "    def stop_watching(self) -> None:\n        \"\"\"Stop file watcher.\"\"\"\n        self._watcher_stop_event.set()\n        self._watching = False\n        \n        if hasattr(self, \"_observer\"):\n            try:\n                self._observer.stop()\n                self._observer.join(timeout=2)\n            except Exception:\n                pass\n        \n        logger.info(\"Stopped file watcher\")\n\n\n# =============================================================================\n# CONVENIENCE FUNCTIONS\n",
        "#!/usr/bin/env python3\n\"\"\"ACE File Watcher Daemon - Persistent background file watcher.\n\nThis daemon runs as a background process and monitors workspace files\nfor changes, automatically reindexing when code is modified.\n\nUsage:\n    python -m ace.file_watcher_daemon start /path/to/workspace\n    python -m ace.file_watcher_daemon stop /path/to/workspace\n    python -m ace.file_watcher_daemon status /path/to/workspace\n\nThe daemon stores its PID in .ace/.watcher.pid and logs to .ace/.watcher.log\n\"\"\"\n\n",
        "    def update_file(self, file_path: str) -> int:\n        \"\"\"\n        Update index for a single file (after modification).\n        \n        Args:\n            file_path: Absolute path to file\n            \n        Returns:\n            Number of chunks indexed\n        \"\"\"\n        # Remove old chunks for this file first\n        rel_path = os.path.relpath(file_path, self.workspace_path)\n        rel_path = rel_path.replace(\"\\\\\", \"/\")\n        \n        if self._client:\n            try:\n                f",
        "\"\"\"Code indexer module for workspace code indexing.\n\nThis module provides code indexing capabilities that scan a workspace,\nparse code files using ASTChunker, and store indexed chunks in Qdrant\nfor semantic code search.\n\nConfiguration:\n    ACE_CODE_COLLECTION: Qdrant collection name (default: ace_code_context)\n    ACE_CODE_EMBEDDING_DIM: Embedding dimension (default: from EmbeddingConfig)\n    QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n\nThe indexer supports:\n- Multi-language p",
        "LESSON LEARNED: ACE File Watcher Daemon Implementation\n\nPROBLEM: File watching in CodeIndexer.start_watching() only works within the same process. Hooks are short-lived processes that exit, so the watcher thread dies.\n\nSOLUTION: Created persistent background daemon (ace/file_watcher_daemon.py) that:\n1. Runs as independent subprocess with its own PID\n2. Stores PID in .ace/.watcher.pid for lifecycle management\n3. Auto-starts from SessionStart hook (ensure_file_watcher_running)\n4. Auto-starts from "
      ],
      "ace_line_counts": [
        32,
        445,
        107,
        55,
        26
      ],
      "auggie_files": [
        "ace/code_indexer.py"
      ],
      "auggie_contents": [
        "...\n   193\t\n   194\tclass CodeIndexer:\n   195\t    \"\"\"\n   196\t    Index workspace code files for semantic search.\n   197\t    \n   198\t    Scans workspace directories, parses code using ASTChunker,\n   199\t    generates embeddings, and stores in Qdrant for retrieval.\n   200\t    \n   201\t    Features:\n   202\t    - Multi-language support via ASTChunker\n   203\t    - Incremental updates on file changes\n   204\t    - File watching for auto-updates\n... (462 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"Python FileWatcher callback handler pattern best practices 2026\"\n   num_results: 5\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        39
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "CircuitBreaker is_open method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "benchmark_results/quality_comparison_20260103_024506.json",
        "ace/observability/health.py",
        "ace/retrieval_bandit.py",
        "ace/scaling.py"
      ],
      "ace_scores": [
        1.1433221,
        0.20997094,
        0.20995657,
        0.20663551999999996,
        0.20194446
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "      \"category\": \"keyword\",\n      \"description\": \"Security keyword matching\",\n      \"num_results\": 0,\n      \"relevant_at_1\": false,\n      \"relevant_at_5\": false,\n      \"top_result\": \"N/A\",\n      \"scores\": [],\n      \"latency_ms\": \"1344.1\"\n    },\n    {\n      \"query\": \"something is broken\",\n      \"category\": \"vague\",\n      \"description\": \"Ultra-vague debugging query\",\n      \"num_results\": 0,\n      \"relevant_at_1\": true,\n      \"relevant_at_5\": true,\n      \"top_result\": \"N/A\",\n      \"scores\": [],\n  ",
        "\"\"\"\nHealth Check System for ACE Framework (Phase 3D)\n\nThis module provides health checks for external dependencies (Qdrant, LM Studio).\nTracks component availability, latency, and error states.\n\"\"\"\n\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict, Optional\n\nimport httpx\n\n",
        "    def _is_cold_start(self) -> bool:\n        \"\"\"Check if bandit is in cold start state (all b vectors are zero).\n\n        Cold start means no arm has received any feedback yet.\n        \"\"\"\n        for arm in self.arms:\n            # If any b vector is non-zero, we have feedback data\n            if not np.allclose(self._b[arm], np.zeros(self.d)):\n                return False\n        return True\n\n    def get_state_snapshot(self) -> str:\n        \"\"\"Return hashable state for comparison.\n\n        Re",
        "class QdrantCluster:\n    \"\"\"Manage multiple Qdrant nodes with load balancing and failover.\n\n    Provides high-availability Qdrant access with:\n    - Load balancing strategies (round-robin, least-connections, weighted)\n    - Automatic failover on node failure\n    - Health monitoring\n    - Connection pooling\n\n    Example:\n        >>> cluster = QdrantCluster(\n        ...     nodes=[\"http://node1:6333\", \"http://node2:6333\"],\n        ...     strategy=LoadBalancingStrategy.ROUND_ROBIN\n        ...  )\n "
      ],
      "ace_line_counts": [
        348,
        128,
        15,
        70,
        108
      ],
      "auggie_files": [
        "ace/resilience.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Resilience patterns for robust LLM interactions.\n     2\t\n     3\tThis module provides circuit breaker, retry, and other resilience patterns\n     4\tfor handling transient failures in LLM API calls.\n     5\t\"\"\"\n     6\t\n     7\tfrom __future__ import annotations\n     8\t\n     9\timport functools\n    10\timport time\n    11\tfrom dataclasses import dataclass, field\n    12\tfrom enum import Enum\n    13\tfrom threading import Lock\n... (477 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.143)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "RetryPolicy execute method",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "ace/integrations/langchain.py",
        "ace/roles.py",
        "ace/audit.py",
        "ace/playbook.py"
      ],
      "ace_scores": [
        0.25129483999999996,
        0.2111812,
        0.19045482999999996,
        0.16612904,
        0.16477340000000001
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "    def invoke(self, input: Any, **kwargs) -> Any:\n        \"\"\"\n        Execute runnable with ACE learning (sync).\n\n        Args:\n            input: Input for the runnable (string, dict, etc.)\n            **kwargs: Additional arguments passed to runnable.invoke()\n\n        Returns:\n            Output from the runnable\n\n        Example:\n            # String input\n            result = ace_chain.invoke(\"What is ACE?\")\n\n            # Dict input\n            result = ace_chain.invoke({\"question\": \"What ",
        "    def _reflect_impl(\n        self,\n        *,\n        question: str,\n        generator_output: GeneratorOutput,\n        playbook: Playbook,\n        ground_truth: Optional[str],\n        feedback: Optional[str],\n        max_refinement_rounds: int = 1,\n        **kwargs: Any,\n    ) -> ReflectorOutput:\n        playbook_excerpt = _make_playbook_excerpt(playbook, generator_output.bullet_ids)\n\n        # Format playbook section based on citation presence\n        if playbook_excerpt:\n            playboo",
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "\"\"\"Playbook storage and mutation logic for ACE.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import asdict, dataclass, field\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Union, cast\n\nfrom .delta import DeltaBatch, DeltaOperation\n\n\n# Phase 1C: Asymmetric penalty weights for bullet tagging\n# Harmful tags penalized 2x to suppress bad strategies faster\nPENALTY_WEIGHTS = {\"helpful\": 1, \"harmf"
      ],
      "ace_line_counts": [
        348,
        110,
        86,
        37,
        108
      ],
      "auggie_files": [
        "docs/INTEGRATION_GUIDE.md"
      ],
      "auggie_contents": [
        "...\n   240\t\n   241\t    def save_playbook(self, path: str):\n   242\t        \"\"\"Save learned strategies.\"\"\"\n   243\t        self.playbook.save_to_file(path)\n   244\t\n   245\t    def load_playbook(self, path: str):\n   246\t        \"\"\"Load existing strategies.\"\"\"\n   247\t        self.playbook = Playbook.load_from_file(path)\n   248\t\n   249\t    def enable_learning(self):\n   250\t        \"\"\"Enable learning.\"\"\"\n   251\t        self.is_learning = True\n... (528 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "CacheManager get set methods",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/caching.py"
      ],
      "ace_files": [
        "ace/caching.py",
        "ace/unified_memory.py",
        "ace/retrieval.py",
        "ace/retrieval_caching.py",
        "ace/code_retrieval.py"
      ],
      "ace_scores": [
        0.49164446,
        0.48730967999999997,
        0.47756718,
        0.31772439999999996,
        0.238344
      ],
      "ace_contents": [
        "\"\"\"Response caching for efficient multi-epoch training.\n\nThis module provides caching mechanisms to avoid redundant LLM calls\nduring multi-epoch training, saving time and costs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom threading import Lock\nfrom typing import Any, Dict, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .llm import LLMClient\n\n\n@datacl",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non",
        "\"\"\"\nRetrieval-specific caching layer for ACE Framework (Phase 4B).\n\nThis module caches RETRIEVAL data (embeddings, query results), NOT LLM responses.\nFor LLM response caching, see ace/caching.py.\n\nCaching Strategy:\n- EmbeddingCache: Text -> embedding vector (768-dim floats)\n- QueryResultCache: Query -> List[QdrantScoredResult] with bullet-aware invalidation\n\nBoth caches use LRU eviction with optional TTL expiration.\n\"\"\"\n\nfrom threading import Lock\nfrom collections import OrderedDict\nfrom datacla",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca"
      ],
      "ace_line_counts": [
        346,
        471,
        393,
        27,
        24
      ],
      "auggie_files": [
        "ace/caching.py"
      ],
      "auggie_contents": [
        "...\n    30\t\n    31\t\n    32\tclass ResponseCache:\n    33\t    \"\"\"LRU cache for LLM responses with TTL support.\n    34\t\n    35\t    Provides efficient caching of LLM responses to avoid redundant calls\n    36\t    during multi-epoch training. Supports:\n    37\t    - TTL (time-to-live) for automatic expiration\n    38\t    - LRU eviction when max size is reached\n    39\t    - Context-aware caching (same prompt, different context = different entry)\n    40\t    - Persistence to disk\n    41\t    - Hit rate metrics\n... (511 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "@dataclass class Bullet playbook",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/playbook.py"
      ],
      "ace_files": [
        "ace/playbook.py",
        "ace/playbook.py",
        "scripts/generate_sample_playbook.py",
        "tenant_data/tenant-x/playbook_x.json",
        "tenant_data/tenant-create/new_playbook.json"
      ],
      "ace_scores": [
        1.2184921,
        0.7825373,
        0.5701469,
        0.51189146,
        0.5117335599999999
      ],
      "ace_contents": [
        "\"\"\"Playbook storage and mutation logic for ACE.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import asdict, dataclass, field\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Union, cast\n\nfrom .delta import DeltaBatch, DeltaOperation\n\n\n# Phase 1C: Asymmetric penalty weights for bullet tagging\n# Harmful tags penalized 2x to suppress bad strategies faster\nPENALTY_WEIGHTS = {\"helpful\": 1, \"harmf",
        "class EnrichedBullet(Bullet):\n    \"\"\"\n    Bullet with semantic scaffolding metadata for intelligent retrieval.\n\n    Extends Bullet with dimensional, structural, relational, and usage metadata\n    that enables purpose-aware retrieval and smarter bullet selection.\n\n    The semantic scaffolding approach follows the principle that the bottleneck\n    in retrieval is often not embeddings or rerankers, but lack of metadata\n    that reflects how the data is actually used.\n\n    Attributes:\n        # Effe",
        "\"\"\"Generate a sample playbook for retrieval benchmarking.\"\"\"\nimport json\nfrom ace.playbook import Playbook, Bullet\n\n# Create bullet content based on ID (realistic examples)\nBULLET_TEMPLATES = {\n    # Debugging\n    \"debug_timeout\": (\"debugging\", \"For timeout errors, check network latency, database query duration, and external API response times. Use distributed tracing to identify bottlenecks.\"),\n    \"check_logs\": (\"debugging\", \"Always start by checking application logs, error logs, and system lo",
        "{\n  \"bullets\": {\n    \"strategy-00001\": {\n      \"id\": \"strategy-00001\",\n      \"section\": \"strategy\",\n      \"content\": \"Tenant A strategy\",\n      \"helpful\": 7,\n      \"harmful\": 0,\n      \"neutral\": 0,\n      \"created_at\": \"2026-01-04T06:15:39.791847+00:00\",\n      \"updated_at\": \"2026-01-04T06:15:39.791851+00:00\",\n      \"last_validated\": null\n    }\n  },\n  \"sections\": {\n    \"strategy\": [\n      \"strategy-00001\"\n    ]\n  },\n  \"next_id\": 1\n}",
        "{\n  \"bullets\": {\n    \"strategy-00001\": {\n      \"id\": \"strategy-00001\",\n      \"section\": \"strategy\",\n      \"content\": \"Tenant A strategy\",\n      \"helpful\": 7,\n      \"harmful\": 0,\n      \"neutral\": 0,\n      \"created_at\": \"2026-01-04T06:15:39.786904+00:00\",\n      \"updated_at\": \"2026-01-04T06:15:39.786911+00:00\",\n      \"last_validated\": null\n    }\n  },\n  \"sections\": {\n    \"strategy\": [\n      \"strategy-00001\"\n    ]\n  },\n  \"next_id\": 1\n}"
      ],
      "ace_line_counts": [
        108,
        925,
        132,
        21,
        21
      ],
      "auggie_files": [
        "ace/playbook.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Playbook storage and mutation logic for ACE.\"\"\"\n     2\t\n     3\tfrom __future__ import annotations\n     4\t\n     5\timport json\n     6\tfrom dataclasses import asdict, dataclass, field\n     7\tfrom datetime import datetime, timezone\n     8\tfrom pathlib import Path\n     9\tfrom typing import Any, Callable, Dict, Iterable, List, Optional, Union, cast\n    10\t\n    11\tfrom .delta import DeltaBatch, DeltaOperation\n    12\t\n    13\t\n... (441 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (1.218)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "CodeChunk dataclass file_path start_line",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/code_chunker.py",
        "ace/code_indexer.py"
      ],
      "ace_files": [
        "ace/code_chunker.py",
        "ace/code_chunker.py",
        "ace/code_indexer.py",
        "ace/code_indexer.py",
        "ace/code_enrichment.py"
      ],
      "ace_scores": [
        1.2023568,
        0.5275936999999999,
        0.49491086000000006,
        0.49079623000000006,
        0.3727027
      ],
      "ace_contents": [
        "\"\"\"AST-based semantic code chunking module.\n\nThis module provides intelligent code chunking that respects language syntax\nboundaries (functions, classes, methods) rather than arbitrary line counts.\n\nSupports multiple languages via tree-sitter:\n- Python (via built-in ast module or tree-sitter)\n- JavaScript/TypeScript (via tree-sitter)\n- Go (via tree-sitter)\n\nConfiguration:\n    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    ACE_AST_MAX_LINES: Maximum lines per chunk (def",
        "def chunk_code(content: str, language: str = \"python\") -> List[CodeChunk]:\n    \"\"\"Chunk code using the default ASTChunker instance.\n    \n    Args:\n        content: Source code to chunk\n        language: Programming language\n    \n    Returns:\n        List of CodeChunk objects\n    \"\"\"\n    return ASTChunker().chunk(content, language)",
        "    def chunk_file(self, file_path: str) -> List[CodeChunkIndexed]:\n        \"\"\"\n        Parse and chunk a code file.\n        \n        Args:\n            file_path: Absolute path to code file\n            \n        Returns:\n            List of CodeChunkIndexed instances\n        \"\"\"\n        chunks = []\n        \n        # Check file exists\n        if not os.path.exists(file_path):\n            logger.warning(f\"File not found: {file_path}\")\n            return chunks\n        \n        # Read file content\n",
        "\"\"\"Code indexer module for workspace code indexing.\n\nThis module provides code indexing capabilities that scan a workspace,\nparse code files using ASTChunker, and store indexed chunks in Qdrant\nfor semantic code search.\n\nConfiguration:\n    ACE_CODE_COLLECTION: Qdrant collection name (default: ace_code_context)\n    ACE_CODE_EMBEDDING_DIM: Embedding dimension (default: from EmbeddingConfig)\n    QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n\nThe indexer supports:\n- Multi-language p",
        "\"\"\"Code-aware bullet enrichment for ACE Phase 2B.\n\nThis module provides CodeAwareEnricher which enriches bullets with code-specific\nmetadata extracted from code context:\n\n- Symbol extraction (functions, classes, methods)\n- Auto-generation of trigger patterns from code symbols\n- Programming language detection\n- Code snippet extraction from markdown-style content\n- Optimized embedding text for code search\n\nIntegration with code_analysis.py (when available) is done via lazy import\nto avoid circular"
      ],
      "ace_line_counts": [
        420,
        11,
        86,
        55,
        69
      ],
      "auggie_files": [
        "ace/code_indexer.py",
        "ace/code_chunker.py"
      ],
      "auggie_contents": [
        "...\n    38\t\n    39\t@dataclass\n    40\tclass CodeChunkIndexed:\n    41\t    \"\"\"A code chunk ready for indexing with all metadata.\"\"\"\n    42\t    \n    43\t    content: str\n    44\t    file_path: str  # Relative to workspace root\n    45\t    start_line: int\n    46\t    end_line: int\n    47\t    language: str\n    48\t    symbols: List[str] = field(default_factory=list)\n    49\t    chunk_hash: str = \"\"\n... (462 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"dataclass definitions in the codebase, particularly for code representation or code snippets\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    50\t    \n    51\t    try:\n    52\t        import tree_sitter_typescript\n    53\t        TREE_SITTER_LANGUAGES[\"typescript\"] = tree_sitter.Language(tree_sitter_typescript.language_typescript())\n    54\t        TREE_SITTER_LANGUAGES[\"ts\"] = TREE_SITTER_LANGUAGES[\"typescript\"]\n    55\t        TREE_SITTER_LANGUAGES[\"tsx\"] = tree_sitter.Language(tree_sitter_typescript.language_tsx())\n    56\t    except ImportError:\n    57\t        pass\n    58\t    \n    59\t    try:\n    60\t        import tree_sitter_go\n    61\t        TREE_SITTER_LANGUAGES[\"go\"] = tree_sitter.Language(tree_sitter_go.language())\n... (533 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (1 vs 0)",
        "High confidence top score (1.202)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "QueryResult dataclass score file_path",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "benchmarks/phase_effectiveness_benchmark.py",
        "ace/retrieval_optimized.py",
        "rag_training/run_hybrid_evaluation.py",
        "rag_training/optimizations/v3_bge_reranker.py",
        "ace_vs_auggie_headtohead.py"
      ],
      "ace_scores": [
        0.58136797,
        0.38609252999999993,
        0.34818810000000006,
        0.33476750000000005,
        0.3322356
      ],
      "ace_contents": [
        "class QueryResult:\n    \"\"\"Result for a single benchmark query.\"\"\"\n    query: str\n    query_type: str\n    difficulty: str\n    relevant_ids: List[str]\n    retrieved_ids: List[str]\n    scores: List[float]\n    top1_hit: bool\n    first_relevant_rank: Optional[int]\n    reciprocal_rank: float\n    ndcg_at_5: float\n    separation_score: float  # Gap between relevant and irrelevant scores\n\n\n@dataclass\nclass PhaseMetrics:\n    \"\"\"Aggregate metrics for a phase configuration.\"\"\"\n    phase_name: str\n    phase_",
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============",
        "    def evaluate_query(\n        self,\n        query: str,\n        query_category: str,\n        difficulty: str,\n        expected_memory_id: int\n    ) -> QueryResult:\n        \"\"\"Evaluate a single query.\"\"\"\n        results, latency = self.hybrid_search(query)\n\n        retrieved_ids = [r[\"id\"] for r in results]\n        retrieved_scores = [r.get(\"score\", 0) for r in results]\n\n        # Find rank of expected memory\n        rank = None\n        score = None\n        if expected_memory_id in retrieved_id",
        "    def evaluate_memory(self, test_case: Dict) -> Dict:\n        memory_id = test_case[\"memory_id\"]\n        content = test_case[\"content\"]\n        category = test_case[\"category\"]\n        queries = test_case.get(\"generated_queries\", [])\n\n        query_results = []\n        for q in queries:\n            qr = self.evaluate_query(\n                query=q[\"query\"],\n                query_category=q[\"category\"],\n                difficulty=q[\"difficulty\"],\n                expected_memory_id=memory_id\n   ",
        "#!/usr/bin/env python3\n\"\"\"\nACE vs Auggie MCP Head-to-Head Benchmark.\n\nThis script directly compares ACE CodeRetrieval against Auggie MCP\nusing real-world queries. For each query:\n1. Call ACE code retrieval\n2. Parse Auggie MCP results (provided manually or via MCP call)\n3. Compare files retrieved, rankings, content relevancy\n4. Determine winner based on code context quality\n\nKey metrics:\n- File coverage: Did both systems find the right file?\n- Ranking: Which system ranked the best file higher?\n- "
      ],
      "ace_line_counts": [
        64,
        87,
        102,
        60,
        66
      ],
      "auggie_files": [
        "rag_training/optimizations/v2_query_expansion.py"
      ],
      "auggie_contents": [
        "...\n   140\t\n   141\t\n   142\t# ============================================================================\n   143\t# DATA CLASSES\n   144\t# ============================================================================\n   145\t\n   146\t@dataclass\n   147\tclass QueryResult:\n   148\t    \"\"\"Result of a single query evaluation.\"\"\"\n   149\t    query: str\n   150\t    query_category: str\n   151\t    difficulty: str\n... (546 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "Better chunk size (76 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "@property score getter",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/playbook.py",
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/retrieval.py",
        "ace/retrieval.py",
        "ace/unified_memory.py",
        "ace/retrieval_optimized.py",
        "ace/semantic_scorer.py"
      ],
      "ace_scores": [
        0.5466755,
        0.53831375,
        0.50732446,
        0.4846779,
        0.48393443
      ],
      "ace_contents": [
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_"
      ],
      "ace_line_counts": [
        393,
        137,
        471,
        87,
        104
      ],
      "auggie_files": [
        "ace/retrieval_optimized.py"
      ],
      "auggie_contents": [
        "...\n   499\t    \n   500\t    def score(self, query: str) -> QuerySpecificityScore:\n   501\t        \"\"\"\n   502\t        Score query specificity and determine expansion strategy.\n   503\t        \n   504\t        Returns:\n   505\t            QuerySpecificityScore with all expansion parameters\n   506\t        \"\"\"\n   507\t        words = query.split()\n   508\t        word_count = len(words)\n   509\t        \n   510\t        # Calculate specificity factors (0.0 to 1.0 each)\n... (537 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "__init__ constructor pattern",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/pattern_detector.py",
        "ace/integrations/langchain.py",
        "rag_training/training_data/crossencoder_training_pairs.json",
        "rag_training/test_suite/enhanced_test_suite.json",
        "rag_training/training_data/crossencoder_training_pairs.json"
      ],
      "ace_scores": [
        0.4552456,
        0.4426541,
        0.35970674,
        0.35505173,
        0.32673467
      ],
      "ace_contents": [
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n",
        "    def __init__(\n        self,\n        runnable: Any,\n        ace_model: str = \"openai/glm-4.6\",\n        ace_api_key: Optional[str] = None,\n        ace_api_base: Optional[str] = None,\n        playbook_path: Optional[str] = None,\n        is_learning: bool = True,\n        output_parser: Optional[Callable[[Any], str]] = None,\n    ):\n        \"\"\"\n        Initialize ACELangChain wrapper.\n\n        IMPORTANT: LLM Configuration\n        =============================\n        The ACE learning components (R",
        "  },\n  {\n    \"query\": \"what is consolidate\",\n    \"memory\": \"Consolidate tool configs in a single file for clarity and maintainability.\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"what is consolidate\",\n    \"memory\": \"Consolidate microservices into single-app for simplicity.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"what is consolidate\",\n    \"memory\": \"Centralize imports in __init__.py for consistent module access.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"what is consolidate\",\n    \"memory\": \"Sanitize all",
        "          \"query\": \"how do I prevent\",\n          \"category\": \"question_how\",\n          \"difficulty\": \"medium\",\n          \"expected_rank\": 5,\n          \"min_similarity\": 0.3,\n          \"generation_method\": \"rule_based\"\n        },\n        {\n          \"query\": \"what is initialize\",\n          \"category\": \"question_what\",\n          \"difficulty\": \"hard\",\n          \"expected_rank\": 5,\n          \"min_similarity\": 0.3,\n          \"generation_method\": \"rule_based\"\n        },\n        {\n          \"query\": \"w",
        "  },\n  {\n    \"query\": \"what is adopt\",\n    \"memory\": \"Adopt a unified naming convention for all external integrations.\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"what is adopt\",\n    \"memory\": \"Atomically replace files, then verify writes.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"what is adopt\",\n    \"memory\": \"Adopt modern module APIs to bypass deprecated workarounds.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"what is adopt\",\n    \"memory\": \"Centralize imports in __init__.py for consistent module access."
      ],
      "ace_line_counts": [
        20,
        106,
        520,
        120,
        220
      ],
      "auggie_files": [
        "tests/test_conflict_detection.py"
      ],
      "auggie_contents": [
        "...\n    43\t\n    44\t        # Contradictory bullet\n    45\t        self.contradictory_bullet = UnifiedBullet(\n    46\t            id=\"test-bullet-2\",\n    47\t            content=\"Never use Python, prefer JavaScript for scripting\",\n    48\t            namespace=UnifiedNamespace.TASK_STRATEGIES,\n    49\t            source=UnifiedSource.TASK_EXECUTION,\n    50\t            section=\"task_guidance\",\n    51\t            helpful_count=3,\n    52\t            harmful_count=0,\n    53\t            category=\"language_preference\"\n    54\t        )\n... (537 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "context manager __enter__ __exit__",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/multitenancy.py",
        "ace/observability/metrics.py",
        "ace/async_retrieval.py",
        ".ace/.ace.json",
        "ace/observability/tracing.py"
      ],
      "ace_scores": [
        0.6530431999999999,
        0.5907382299999999,
        0.5879227,
        0.42934996,
        0.42878205
      ],
      "ace_contents": [
        "\"\"\"\nMulti-tenancy support for ACE framework.\n\nThis module provides tenant isolation for playbooks and Qdrant collections,\nensuring that different tenants cannot access each other's data.\n\nFeatures:\n- Thread-local tenant context tracking\n- Tenant-scoped playbook storage\n- Tenant-scoped Qdrant collections\n- Cross-tenant access prevention\n- Path traversal attack protection\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nimport threading\nfrom pathlib import Path\nfrom typing import List, Optional\n",
        "def track_latency(operation: str, tenant_id: str = \"default\") -> Generator[None, None, None]:\n    \"\"\"\n    Context manager for automatic latency tracking.\n\n    Records operation duration to retrieval_latency_histogram.\n\n    Args:\n        operation: Name of the operation being tracked\n        tenant_id: Tenant identifier for multi-tenancy support\n\n    Yields:\n        None\n\n    Example:\n        >>> with track_latency(operation=\"semantic_search\", tenant_id=\"tenant_123\"):\n        ...     perform_sear",
        "class AsyncQdrantBulletIndex:\n    \"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\n    Provides O(1) semantic retrieval using:\n    - Dense vectors from LM Studio (nomic-embed-text-v1.5)\n    - BM25 sparse vectors for keyword matching\n    - Hybrid search with RRF fusion\n    - Async operations for concurrent execution\n\n    Example:\n        >>> async with AsyncQdrantBulletIndex() as index:\n        ...     results = await index.retrieve(\"how do I debug this error?\")\n        ...    ",
        "{\n  \"workspace_name\": \"agentic-context-engine\",\n  \"workspace_path\": \"D:\\\\ApplicationDevelopment\\\\Tools\\\\agentic-context-engine\",\n  \"collection_name\": \"agentic-context-engine_code_context\",\n  \"onboarded_at\": \"2026-01-05T14:08:46.446329\"\n}",
        "\"\"\"\nDistributed Tracing for ACE Framework (Phase 3D)\n\nThis module provides OpenTelemetry integration for distributed tracing.\nWhen OpenTelemetry is not installed, importing TracingManager raises ImportError\nto allow tests to properly skip. The trace_operation decorator gracefully degrades.\n\"\"\"\n\nimport functools\nfrom typing import Optional, Any\n\n# Try to import OpenTelemetry\ntry:\n    from opentelemetry import trace\n    from opentelemetry.trace import Status, StatusCode\n\n    TRACING_AVAILABLE = Tr"
      ],
      "ace_line_counts": [
        128,
        55,
        99,
        6,
        24
      ],
      "auggie_files": [
        "ace/multitenancy.py"
      ],
      "auggie_contents": [
        "...\n    58\t\n    59\t\n    60\tclass TenantContext:\n    61\t    \"\"\"\n    62\t    Context manager for setting the active tenant in the current thread.\n    63\t\n    64\t    Provides thread-local tenant tracking with proper nesting support.\n    65\t\n    66\t    Example:\n    67\t        with TenantContext(tenant_id=\"tenant-001\"):\n    68\t            # All operations here are scoped to tenant-001\n    69\t            manager.save_playbook(playbook, \"my_playbook\")\n... (549 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "Better chunk size (62 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "@staticmethod factory pattern",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/reranker.py",
        "ace/retrieval.py",
        "ace/__init__.py",
        "ace/unified_memory.py",
        "ace/adaptation.py"
      ],
      "ace_scores": [
        0.41097265,
        0.4098016,
        0.40700918,
        0.3931672,
        0.39305496
      ],
      "ace_contents": [
        "\"\"\"Cross-encoder reranking for improved retrieval precision.\n\nThis module provides cross-encoder based reranking to improve the precision\nof bullet retrieval. Cross-encoders process query-document pairs together,\nenabling more accurate relevance scoring than bi-encoders.\n\nUsage:\n    from ace.reranker import get_reranker, CrossEncoderReranker\n    \n    # Using singleton (recommended)\n    reranker = get_reranker()\n    scores = reranker.predict(\"query\", [\"doc1\", \"doc2\"])\n    \n    # Custom model\n    ",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"Agentic Context Engineering (ACE) reproduction framework.\"\"\"\n\nfrom typing import Optional\nfrom .playbook import Bullet, EnrichedBullet, Playbook, enrich_bullet, migrate_bullet\nfrom .delta import DeltaOperation, DeltaBatch\nfrom .retrieval import SmartBulletIndex, ScoredBullet, IntentClassifier\nfrom .llm import LLMClient, DummyLLMClient, TransformersLLMClient\nfrom .roles import (\n    Generator,\n    ReplayGenerator,\n    Reflector,\n    Curator,\n    GeneratorOutput,\n    ReflectorOutput,\n    Curato",
        "            class _PointsResult:\n                def __init__(self, points_data):\n                    self.points = []\n                    for p in points_data:\n                        class _Point:\n                            pass",
        "\"\"\"Adaptation loops for offline and online ACE training.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Sequence\n\nif TYPE_CHECKING:\n    from .observability.opik_integration import OpikIntegration\n    from .session_tracking import SessionOutcomeTracker\n\nfrom .playbook import Playbook\nfrom .roles import (\n    Curator,\n    CuratorOut"
      ],
      "ace_line_counts": [
        100,
        137,
        135,
        6,
        91
      ],
      "auggie_files": [
        "examples/zai_glm_example.py"
      ],
      "auggie_contents": [
        "...\n    37\t\n    38\t\n    39\tdef create_zai_client(model_name=\"glm-4\"):\n    40\t    \"\"\"Create a LiteLLM client configured for Z.ai GLM.\"\"\"\n    41\t\n    42\t    # Get Z.ai API key from environment\n    43\t    api_key = os.getenv(\"ZAI_API_KEY\")\n    44\t    if not api_key:\n    45\t        raise ValueError(\"Please set ZAI_API_KEY in your .env file\")\n    46\t\n    47\t    # Configure LiteLLM for Z.ai GLM using zhipuai provider (recommended)\n    48\t    client = LiteLLMClient(\n... (591 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "Better chunk size (94 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "@classmethod from_config",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/retrieval_optimized.py",
        "ace/retrieval.py",
        "ace/llm_providers/litellm_client.py",
        "ace/unified_memory.py"
      ],
      "ace_scores": [
        0.501402,
        0.48725733,
        0.47825795,
        0.46860054,
        0.46745145
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\""
      ],
      "ace_line_counts": [
        611,
        91,
        61,
        300,
        222
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "...\n    58\t\n    59\t\n    60\t@dataclass\n    61\tclass CodeEmbeddingConfig:\n    62\t    \"\"\"DEPRECATED - Use VoyageCodeEmbeddingConfig instead.\n    63\t    \n    64\t    This configuration was for the old Jina-v2-base-code model (768d).\n    65\t    Code indexing now uses Voyage-code-3 (1024d) exclusively.\n    66\t    \n    67\t    Environment Variables:\n    68\t        ACE_CODE_EMBEDDING_URL: LM Studio server URL (DEPRECATED)\n    69\t        ACE_CODE_EMBEDDING_MODEL: Model name (DEPRECATED)\n... (497 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "custom exception class",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/security.py",
        "fibonacci.py",
        "ace/unified_memory.py",
        "ace/reranker.py",
        "examples/zai_custom_client.py"
      ],
      "ace_scores": [
        0.6254736000000001,
        0.5530997,
        0.4320449,
        0.41402513,
        0.40349334
      ],
      "ace_contents": [
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes\nclass AuthenticationError(Exception):\n    \"\"\"Raised when authenticat",
        "\"\"\"\nFibonacci Number Calculator\n\nA high-performance, production-ready implementation for calculating Fibonacci numbers\nwith comprehensive error handling, multiple algorithm options, and extensive documentation.\n\nAuthor: Elite Software Engineer\nVersion: 1.0.0\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Union, Optional, Callable, Any\nfrom enum import Enum\n\n\nclass FibonacciAlgorithm(Enum):\n    \"\"\"Enumeration of available Fibonacci calculation algorithms.\"\"\"\n    ITER",
        "            class _PointsResult:\n                def __init__(self, points_data):\n                    self.points = []\n                    for p in points_data:\n                        class _Point:\n                            pass",
        "\"\"\"Cross-encoder reranking for improved retrieval precision.\n\nThis module provides cross-encoder based reranking to improve the precision\nof bullet retrieval. Cross-encoders process query-document pairs together,\nenabling more accurate relevance scoring than bi-encoders.\n\nUsage:\n    from ace.reranker import get_reranker, CrossEncoderReranker\n    \n    # Using singleton (recommended)\n    reranker = get_reranker()\n    scores = reranker.predict(\"query\", [\"doc1\", \"doc2\"])\n    \n    # Custom model\n    ",
        "#!/usr/bin/env python3\n\"\"\"\nCustom Z.ai GLM client with automatic API key detection.\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\n\nfrom ace.llm_providers.litellm_client import LiteLLMClient, LiteLLMConfig\n\nload_dotenv()\n\n\nclass ZaiGLMClient(LiteLLMClient):\n    \"\"\"Custom LiteLLM client for Z.ai GLM models.\"\"\"\n\n    def __init__(self, model=\"glm-4\", **kwargs):\n        \"\"\"Initialize Z.ai GLM client with automatic configuration.\"\"\"\n\n        api_key = kwargs.get(\"api_key\") or os.getenv(\"ZAI_API_KEY\")\n"
      ],
      "ace_line_counts": [
        89,
        37,
        6,
        100,
        60
      ],
      "auggie_files": [
        "ace/security.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"\n     2\tSecurity Module - Enterprise Authentication & Authorization\n     3\tImplements API key validation, JWT authentication, RBAC, and security middleware.\n     4\t\"\"\"\n     5\t\n     6\timport secrets\n     7\tfrom datetime import datetime, timedelta\n     8\tfrom typing import Dict, List, Optional, Union, Any\n     9\t\n    10\ttry:\n    11\t    import jwt\n    12\texcept ImportError:\n    13\t    raise ImportError(\n... (491 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "exception hierarchy",
      "category": "ClassDefinitions",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/security.py",
        "ace/retrieval.py",
        "ace/resilience.py",
        "ace/structured_enhancer.py",
        "ace/gemini_embeddings.py"
      ],
      "ace_scores": [
        0.48687327,
        0.45965385,
        0.45543402,
        0.45487148,
        0.4473536
      ],
      "ace_contents": [
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes\nclass AuthenticationError(Exception):\n    \"\"\"Raised when authenticat",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "#!/usr/bin/env python\n\"\"\"\nStructured Query Enhancer based on .enhancedprompt.md methodology.\n\nEXHAUSTIVE IMPLEMENTATION - Covers ALL software engineering domains.\n\nTransforms vague user queries into structured, actionable prompts using:\n1. Intent Classification (ANALYTICAL/IMPLEMENTATION/TROUBLESHOOTING/EXPLORATORY/LEARNING/REFACTORING)\n2. Domain Detection (40+ domains covering all software engineering areas)\n3. Context Expansion with domain-specific terminology (1000+ expansion terms)\n4. Query ",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n"
      ],
      "ace_line_counts": [
        89,
        137,
        149,
        35,
        88
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 3,
      "auggie_expected_rank": -1
    },
    {
      "query": "voyage-code-3 embedding model",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/code_retrieval.py",
        "ace/code_indexer.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/code_retrieval.py",
        "ace/code_indexer.py",
        "ace/gemini_embeddings.py",
        "ace/openai_embeddings.py"
      ],
      "ace_scores": [
        1.2044871000000001,
        1.144164,
        0.7704573699999999,
        0.6645390999999999,
        0.6534318
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "class CodeIndexer:\n    \"\"\"\n    Index workspace code files for semantic search.\n    \n    Scans workspace directories, parses code using ASTChunker,\n    generates embeddings, and stores in Qdrant for retrieval.\n    \n    Features:\n    - Multi-language support via ASTChunker\n    - Incremental updates on file changes\n    - File watching for auto-updates\n    - Gitignore and exclude pattern support\n    - Relative path storage for portability\n    \n    Example:\n        indexer = CodeIndexer(workspace_pat",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration"
      ],
      "ace_line_counts": [
        117,
        147,
        101,
        157,
        113
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "voyage-3 embedding generation",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/hyde_retrieval.py",
        "ace/config.py",
        "ace/gemini_embeddings.py",
        "ace/openai_embeddings.py",
        "ace/code_indexer.py"
      ],
      "ace_scores": [
        0.50642915,
        0.48071094999999997,
        0.45485845,
        0.44264880000000006,
        0.43033850000000007
      ],
      "ace_contents": [
        "\"\"\"HyDE-enhanced retrieval pipeline for ACE memory system.\n\nIntegrates HyDE (Hypothetical Document Embeddings) with existing hybrid search\ninfrastructure for improved retrieval accuracy on ambiguous/implicit queries.\n\nPipeline:\n1. Query -> HyDE expansion -> Generate hypothetical documents\n2. Embed hypotheticals -> Average embeddings\n3. Search Qdrant with averaged embedding + BM25 sparse\n4. Return results with hybrid RRF fusion\n\nPerformance target: +5-10% for implicit/scenario/template queries\n\"\"",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "    def _embed_batch(self, texts: List[str], batch_size: int = 128, max_tokens_per_batch: int = 100000) -> List[List[float]]:\n        \"\"\"Batch embed texts using Voyage API with token-aware splitting.\n        \n        Voyage API has two limits:\n        1. Max 128 texts per request\n        2. Max ~120k tokens per request\n        \n        This function respects both limits by dynamically sizing batches.\n        \n        Args:\n            texts: List of texts to embed\n            batch_size: Max tex"
      ],
      "ace_line_counts": [
        473,
        117,
        280,
        211,
        103
      ],
      "auggie_files": [
        "ace/code_retrieval.py"
      ],
      "auggie_contents": [
        "...\n    55\t    \n    56\t    def __init__(\n    57\t        self,\n    58\t        qdrant_url: str = None,\n    59\t        collection_name: str = None,\n    60\t        embed_fn: Callable[[str], List[float]] = None,\n    61\t    ):\n    62\t        \"\"\"\n    63\t        Initialize code retrieval.\n    64\t        \n    65\t        Args:\n    66\t            qdrant_url: Qdrant server URL (default: from env or localhost:6333)\n... (484 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"voyage-3 embedding API documentation 2026\"\n   num_results: 5\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        39
      ],
      "winner": "AUGGIE",
      "reason": "Auggie found expected file at rank 1, ACE missed it",
      "ace_advantages": [],
      "auggie_advantages": [
        "Found expected file at rank 1"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": true,
      "ace_expected_rank": -1,
      "auggie_expected_rank": 1
    },
    {
      "query": "bge-m3 sparse vector",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/hyde_retrieval.py"
      ],
      "ace_files": [
        "ace/hyde_retrieval.py",
        "ace/unified_memory.py",
        "ace/qdrant_retrieval.py",
        "ace/qdrant_retrieval.py",
        "ace/async_retrieval.py"
      ],
      "ace_scores": [
        0.58109947,
        0.5408373,
        0.5137827,
        0.4950728,
        0.4920512
      ],
      "ace_contents": [
        "    def _compute_bm25_sparse(self, text: str) -> Dict[str, Any]:\n        \"\"\"Compute BM25-style sparse vector for Qdrant.\n\n        Args:\n            text: Text to vectorize\n\n        Returns:\n            Dict with 'indices' (term hashes) and 'values' (BM25 weights)\n        \"\"\"\n        tokens = self._tokenize_for_bm25(text)\n        if not tokens:\n            return {\"indices\": [], \"values\": []}\n\n        tf = Counter(tokens)\n        doc_length = len(tokens)\n\n        indices = []\n        values = []\n",
        "    class _MatchValue:\n        value: Any\n    MatchValue = _MatchValue\n\n    @dataclass\n    class _MatchAny:\n        any: List[Any]\n    MatchAny = _MatchAny\n\n    @dataclass\n    class _Filter:\n        must: Optional[List[Any]] = None\n        should: Optional[List[Any]] = None\n    Filter = _Filter\n\n\n# =============================================================================\n# NAMESPACE AND SOURCE ENUMS\n# =============================================================================\n\nclass Unifie",
        "\"\"\"Vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides QdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with hybrid search (dense + BM25 sparse).\n\nPhase 1: Vector Search Integration for ACE Fortune 100 Production Readiness.\n\nKey features:\n- Dense embeddings via LM Studio (nomic-embed-text-v1.5, 768-dim)\n- BM25 sparse vectors for keyword matching (technical terms)\n- Hybrid search with RRF fusion for best of both approaches\n",
        "    def _get_embedding(self, text: str) -> List[float]:\n        \"\"\"Get dense embedding from LM Studio with automatic EOS token handling.\n\n        Args:\n            text: Text to embed (truncated to 8000 chars)\n\n        Returns:\n            768-dimensional embedding vector.\n\n        Raises:\n            RuntimeError: If embedding request fails.\n        \"\"\"\n        try:\n            # Add EOS token for Qwen models to fix GGUF tokenizer warning\n            # This ensures proper sentence boundary dete",
        "    async def get_embedding(self, text: str) -> List[float]:\n        \"\"\"Get dense embedding from LM Studio asynchronously.\n\n        Args:\n            text: Text to embed (truncated to 8000 chars)\n\n        Returns:\n            768-dimensional embedding vector.\n\n        Raises:\n            Exception: If embedding request fails.\n        \"\"\"\n        # Import httpx here so mocking works (patch('httpx.AsyncClient'))\n        import httpx\n\n        # Create client if needed (allows mocking to work)\n     "
      ],
      "ace_line_counts": [
        85,
        101,
        70,
        115,
        113
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "text-embedding-3-small openai",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/openai_embeddings.py"
      ],
      "ace_files": [
        "ace/openai_embeddings.py",
        "ace/gemini_embeddings.py",
        "rag_training/scripts/reindex_with_openai.py",
        "ace/semantic_scorer.py",
        "ace/embedding_finetuning/finetune_embeddings.py"
      ],
      "ace_scores": [
        1.30774317,
        0.6941816599999999,
        0.60035014,
        0.4925062,
        0.48642565
      ],
      "ace_contents": [
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"Re-index Qdrant Collection with OpenAI text-embedding-3-large.\n\nThis script:\n1. Extracts all memory payloads from existing Qdrant collection\n2. Generates fresh embeddings using OpenAI text-embedding-3-large (768 dims)\n3. Recreates the collection with new vectors\n4. Verifies embedding consistency\n\nUsage:\n    python reindex_with_openai.py <openai_api_key> [--replace]\n\nEnvironment:\n    OPENAI_API_KEY: Can also be set as environment variable\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport logging\nim",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "\"\"\"Fine-tune embedding models for domain adaptation.\n\nUses sentence-transformers with contrastive learning (MultipleNegativesRankingLoss)\nto adapt embeddings to the ACE memory domain, improving query-memory similarity.\n\nBase model: sentence-transformers/all-MiniLM-L6-v2 (lightweight, 384 dims)\nCan be fine-tuned on CPU/GPU, outputs compatible with Qdrant.\n\"\"\"\n\nimport json\nimport logging\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Optional,"
      ],
      "ace_line_counts": [
        211,
        280,
        93,
        104,
        144
      ],
      "auggie_files": [
        "ace/openai_embeddings.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"OpenAI Embedding Client for ACE Framework.\n     2\t\n     3\tProvides embeddings using OpenAI's text-embedding-3-large model\n     4\twith configurable dimensions.\n     5\t\n     6\tUsage:\n     7\t    from ace.openai_embeddings import OpenAIEmbeddingClient\n     8\t\n     9\t    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n    10\t\n    11\t    # Get embedding\n    12\t    embedding = client.get_embedding(\"This is a document about...\")\n    13\t\"\"\"\n... (488 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.308)"
      ],
      "auggie_advantages": [
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "text-embedding-ada-002 openai",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/openai_embeddings.py"
      ],
      "ace_files": [
        "ace/openai_embeddings.py",
        "ace/gemini_embeddings.py",
        "rag_training/scripts/reindex_with_openai.py",
        "ace/semantic_scorer.py",
        "ace/embedding_finetuning/finetune_embeddings.py"
      ],
      "ace_scores": [
        1.0380768599999999,
        0.630076,
        0.5272913699999999,
        0.4989528,
        0.4837012
      ],
      "ace_contents": [
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"Re-index Qdrant Collection with OpenAI text-embedding-3-large.\n\nThis script:\n1. Extracts all memory payloads from existing Qdrant collection\n2. Generates fresh embeddings using OpenAI text-embedding-3-large (768 dims)\n3. Recreates the collection with new vectors\n4. Verifies embedding consistency\n\nUsage:\n    python reindex_with_openai.py <openai_api_key> [--replace]\n\nEnvironment:\n    OPENAI_API_KEY: Can also be set as environment variable\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport logging\nim",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "\"\"\"Fine-tune embedding models for domain adaptation.\n\nUses sentence-transformers with contrastive learning (MultipleNegativesRankingLoss)\nto adapt embeddings to the ACE memory domain, improving query-memory similarity.\n\nBase model: sentence-transformers/all-MiniLM-L6-v2 (lightweight, 384 dims)\nCan be fine-tuned on CPU/GPU, outputs compatible with Qdrant.\n\"\"\"\n\nimport json\nimport logging\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Optional,"
      ],
      "ace_line_counts": [
        211,
        280,
        93,
        104,
        144
      ],
      "auggie_files": [
        "ace/openai_embeddings.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"OpenAI Embedding Client for ACE Framework.\n     2\t\n     3\tProvides embeddings using OpenAI's text-embedding-3-large model\n     4\twith configurable dimensions.\n     5\t\n     6\tUsage:\n     7\t    from ace.openai_embeddings import OpenAIEmbeddingClient\n     8\t\n     9\t    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n    10\t\n    11\t    # Get embedding\n    12\t    embedding = client.get_embedding(\"This is a document about...\")\n    13\t\"\"\"\n... (472 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.038)"
      ],
      "auggie_advantages": [
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "jina-embeddings-v2-base-code",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/gemini_embeddings.py",
        "ace/openai_embeddings.py",
        "ace/embedding_finetuning/finetune_embeddings.py",
        "ace/config.py",
        "ace/semantic_scorer.py"
      ],
      "ace_scores": [
        0.7235229,
        0.7012546399999999,
        0.5257894000000001,
        0.4807867,
        0.4717100000000001
      ],
      "ace_contents": [
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Fine-tune embedding models for domain adaptation.\n\nUses sentence-transformers with contrastive learning (MultipleNegativesRankingLoss)\nto adapt embeddings to the ACE memory domain, improving query-memory similarity.\n\nBase model: sentence-transformers/all-MiniLM-L6-v2 (lightweight, 384 dims)\nCan be fine-tuned on CPU/GPU, outputs compatible with Qdrant.\n\"\"\"\n\nimport json\nimport logging\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Optional,",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_"
      ],
      "ace_line_counts": [
        88,
        27,
        42,
        117,
        22
      ],
      "auggie_files": [
        "rag_training/optimizations/v6_hyde.py"
      ],
      "auggie_contents": [
        "...\n    47\t\n    48\tfrom ace.hyde import HyDEGenerator, HyDEConfig\n    49\tfrom ace.hyde_retrieval import HyDEEnhancedRetriever\n    50\tfrom ace.llm_providers.litellm_client import LiteLLMClient\n    51\t\n    52\t# ============================================================================\n    53\t# CONFIGURATION\n    54\t# ============================================================================\n    55\t\n    56\tQDRANT_URL = \"http://localhost:6333\"\n    57\tEMBEDDING_URL = \"http://192.168.10.64:1234\"\n    58\tCOLLECTION_NAME = \"ace_memories_hybrid\"\n... (483 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"jina-embeddings-v2-base-code model specifications 2026\"\n   num_results: 5\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        39
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 4, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 4"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 4,
      "auggie_expected_rank": -1
    },
    {
      "query": "gemini-1.5-flash model",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/gemini_embeddings.py"
      ],
      "ace_files": [
        "ace/gemini_embeddings.py",
        "ace/unified_memory.py",
        "fix_gguf_eos.py",
        "ace/retrieval.py",
        "rag_training/scripts/reindex_with_gemini.py"
      ],
      "ace_scores": [
        0.53105114,
        0.22659854999999998,
        0.21757003,
        0.21520664999999997,
        0.1463449
      ],
      "ace_contents": [
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\"",
        "#!/usr/bin/env python3\n\"\"\"\nFix GGUF model metadata to enable automatic EOS token addition.\n\nThis patches the tokenizer.ggml.add_eos_token metadata field to 'true'\nto ensure proper sentence boundary detection in embeddings.\n\nWARNING: Creates a backup before modifying. This operation modifies the GGUF file.\n\"\"\"\n\nimport sys\nimport shutil\nfrom pathlib import Path\n\ntry:\n    from gguf import GGUFReader, GGUFWriter\nexcept ImportError:\n    print(\"ERROR: gguf library not installed. Run: uv pip install gg",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"Re-index Qdrant Collection with Gemini Embeddings.\n\nThis script:\n1. Extracts all memory payloads from existing Qdrant collection\n2. Generates fresh embeddings using Gemini gemini-embedding-001\n3. Recreates the collection with new vectors\n4. Verifies embedding consistency\n\nUsage:\n    python reindex_with_gemini.py <gemini_api_key>\n\nEnvironment:\n    GEMINI_API_KEY: Can also be set as environment variable\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime "
      ],
      "ace_line_counts": [
        88,
        118,
        21,
        41,
        85
      ],
      "auggie_files": [
        "ace/integrations/litellm.py"
      ],
      "auggie_contents": [
        "...\n    91\t\n    92\t    def __init__(\n    93\t        self,\n    94\t        model: str = \"openai/glm-4.6\",\n    95\t        max_tokens: int = 512,\n    96\t        temperature: float = 0.0,\n    97\t        playbook_path: Optional[str] = None,\n    98\t        is_learning: bool = True,\n    99\t        api_key: Optional[str] = None,\n   100\t        api_base: Optional[str] = None,\n   101\t    ):\n   102\t        \"\"\"\n... (519 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace/llm_providers\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        59
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "claude-3-5-sonnet llm",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/llm.py"
      ],
      "ace_files": [
        "ace/llm_providers/litellm_client.py",
        "rag_training/training_data/crossencoder_training_pairs.json",
        "rag_training/training_data/crossencoder_training_pairs.json",
        "ace/llm.py",
        "ace/hyde.py"
      ],
      "ace_scores": [
        0.6193035,
        0.48311013,
        0.4754332,
        0.43470030000000004,
        0.42877283
      ],
      "ace_contents": [
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L",
        "  },\n  {\n    \"query\": \"how to structure kilocode\",\n    \"memory\": \"kilocode for GLM integration patterns\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"how to structure kilocode\",\n    \"memory\": \"[>] [DIRECTIVE] Always reference efficiency improvement docs explicitly when asked about optimizations\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"how to structure kilocode\",\n    \"memory\": \"kilocode at D:\\\\ApplicationDevelopment\\\\Tools\\\\kilocode\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"how to structure kilocode\",\n   ",
        "  },\n  {\n    \"query\": \"api endpoint\",\n    \"memory\": \"Match every API endpoint to an available model capability before implementation\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"api endpoint\",\n    \"memory\": \"Group tests in sub-tables for logical clarity and easier maintenance.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"api endpoint\",\n    \"memory\": \"Z.ai endpoint routing fix: forms/settings.ts defaults were incorrect - zai_model_name defaulted to claude-3-5-sonnet instead of glm-4.6, and zai_base_url was",
        "\"\"\"LLM client abstractions used by ACE components.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nimport json\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Deque, Dict, Optional, Union\n\nif TYPE_CHECKING:\n    import torch\n\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Container for LLM outputs.\"\"\"\n\n    text: str\n    raw: Optional[Dict[str, Any]] = None\n\n\nclass LLMClient(ABC):\n    \"\"\"Abstract interface so ACE can plug in",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp"
      ],
      "ace_line_counts": [
        186,
        220,
        420,
        146,
        172
      ],
      "auggie_files": [
        "ace/llm_providers/litellm_client.py"
      ],
      "auggie_contents": [
        "...\n    66\t\n    67\t\n    68\tclass LiteLLMClient(LLMClient):\n    69\t    \"\"\"\n    70\t    Production LLM client using LiteLLM for unified access to multiple providers.\n    71\t\n    72\t    Supports:\n    73\t    - OpenAI (GPT-3.5, GPT-4, etc.)\n    74\t    - Anthropic (Claude-2, Claude-3, etc.)\n    75\t    - Google (Gemini, PaLM)\n    76\t    - Cohere\n    77\t    - Azure OpenAI\n... (512 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace/llm_providers\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        37
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 4, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 4"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 4,
      "auggie_expected_rank": -1
    },
    {
      "query": "gpt-4o model config",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/llm.py"
      ],
      "ace_files": [
        "ace/integrations/litellm.py",
        "ace/config.py",
        "ace/integrations/browser_use.py",
        "scripts/run_benchmark.py",
        "examples/langchain/agent_with_tools_example.py"
      ],
      "ace_scores": [
        0.7912835,
        0.76190546,
        0.69518492,
        0.3952563,
        0.39266062
      ],
      "ace_contents": [
        "\"\"\"\nACE + LiteLLM integration for quick-start learning agents.\n\nThis module provides ACELiteLLM, a high-level wrapper bundling ACE learning\nwith LiteLLM for easy prototyping and simple tasks.\n\nWhen to Use ACELiteLLM:\n- Quick start: Want to try ACE with minimal setup\n- Simple tasks: Q&A, classification, reasoning\n- Prototyping: Experimenting with ACE learning\n- No framework needed: Direct LLM usage with learning\n\nWhen NOT to Use ACELiteLLM:\n- Browser automation \u2192 Use ACEAgent (browser-use)\n- Lang",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "    def __init__(\n        self,\n        task: Optional[str] = None,\n        llm: Any = None,\n        browser: Optional[Any] = None,\n        ace_model: str = \"openai/glm-4.6\",\n        ace_llm: Optional[LiteLLMClient] = None,\n        ace_max_tokens: int = 2048,\n        ace_api_key: Optional[str] = None,\n        ace_api_base: Optional[str] = None,\n        playbook: Optional[Playbook] = None,\n        playbook_path: Optional[str] = None,\n        is_learning: bool = True,\n        **agent_kwargs,\n    )",
        "#!/usr/bin/env python3\n\"\"\"\nRun ACE benchmarks with comprehensive evaluation and reporting.\n\nThis script provides a command-line interface for running benchmarks\nwith the ACE framework, supporting multiple benchmark types and\nconfiguration options.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom statistics import mean\nfrom typing import Dict, List, Any\n\n# Add project root to path\nROOT =",
        "def example_multi_step_reasoning():\n    \"\"\"Example 2: Agent learning multi-step reasoning patterns.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Example 2: Multi-Step Reasoning\")\n    print(\"=\" * 60)\n\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        print(\"Please set OPENAI_API_KEY in your .env file\")\n        return\n\n    # Create a chain that encourages step-by-step thinking\n    prompt = ChatPromptTemplate.from_template(\n        \"\"\"Break down the problem and solve step by step:\n\nProblem: {input}\n\nThin"
      ],
      "ace_line_counts": [
        357,
        611,
        100,
        147,
        96
      ],
      "auggie_files": [
        "ace/llm_providers/litellm_client.py"
      ],
      "auggie_contents": [
        "...\n    33\t\n    34\t\n    35\t@dataclass\n    36\tclass LiteLLMConfig:\n    37\t    \"\"\"Configuration for LiteLLM client.\"\"\"\n    38\t\n    39\t    model: str\n    40\t    api_key: Optional[str] = None\n    41\t    api_base: Optional[str] = None\n    42\t    api_version: Optional[str] = None\n    43\t    temperature: float = 0.0\n    44\t    max_tokens: int = 512\n... (577 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace/llm_providers\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        58
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "High confidence top score (0.791)"
      ],
      "auggie_advantages": [
        "Better chunk size (58 lines avg)"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "httpx-async client",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_retrieval.py",
        "ace/llm_providers/litellm_client.py",
        "ace/llm_providers/langchain_client.py",
        "ace/async_retrieval.py",
        "ace/hyde.py"
      ],
      "ace_scores": [
        0.67416856,
        0.6605637999999999,
        0.6131245,
        0.53690592,
        0.4639088
      ],
      "ace_contents": [
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L",
        "\"\"\"LangChain integration for ACE using langchain-litellm.\"\"\"\n\nfrom typing import Optional, Dict, Any, AsyncIterator, Iterator\nimport logging\n\nRouter: Optional[type]\n\ntry:\n    from langchain_litellm import ChatLiteLLM, ChatLiteLLMRouter\n    from litellm import Router as LiteLLMRouter\n\n    LANGCHAIN_AVAILABLE = True\n    Router = LiteLLMRouter  # type: ignore[assignment]\nexcept ImportError:\n    LANGCHAIN_AVAILABLE = False\n    ChatLiteLLM = None  # type: ignore\n    ChatLiteLLMRouter = None  # type: ",
        "    async def retrieve(\n        self,\n        query: str,\n        limit: int = 10,\n        query_type: Optional[str] = None,\n        min_score: float = 0.0,\n    ) -> List[QdrantScoredResult]:\n        \"\"\"Retrieve bullets using hybrid search asynchronously.\n\n        Combines dense semantic search with BM25 keyword matching\n        using Reciprocal Rank Fusion (RRF).\n\n        Args:\n            query: Natural language query\n            limit: Maximum number of results\n            query_type: Optiona",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp"
      ],
      "ace_line_counts": [
        269,
        186,
        102,
        106,
        172
      ],
      "auggie_files": [
        "ace/async_retrieval.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n     2\t\n     3\tThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\n     4\tbullets using Qdrant vector database with async operations.\n     5\t\n     6\tPhase 4A: Async Operations for ACE Framework.\n     7\t\n     8\tKey features:\n     9\t- Async embedding retrieval via httpx.AsyncClient\n    10\t- Parallel batch processing with asyncio.gather\n    11\t- Concurrent query handling\n    12\t- Non-blocking Qdrant operations\n    13\t\"\"\"\n... (512 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"httpx AsyncClient python usage examples 2026\"\n   num_results: 5\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        39
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (3 vs 0)"
      ],
      "auggie_advantages": [
        "Better chunk size (39 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "qdrant-client models",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/unified_memory.py",
        "ace/code_indexer.py"
      ],
      "ace_files": [
        "ace/deduplication.py",
        "ace/unified_memory.py",
        "ace/code_indexer.py",
        "ace/code_retrieval.py",
        "ace/qdrant_retrieval.py"
      ],
      "ace_scores": [
        0.84402533,
        0.8406205600000001,
        0.8256553600000001,
        0.8209683,
        0.7321709
      ],
      "ace_contents": [
        "class DeduplicationEngine:\n    \"\"\"\n    Main engine for clustering-based memory deduplication.\n\n    Example:\n        >>> engine = DeduplicationEngine(\n        ...     collection_name=\"ace_memories_hybrid\",\n        ...     qdrant_url=\"http://localhost:6333\"\n        ... )\n        >>> result = engine.run_deduplication(dry_run=True)\n        >>> print(f\"Found {result.duplicate_groups} duplicate groups\")\n    \"\"\"\n\n    def __init__(\n        self,\n        collection_name: Optional[str] = None,\n        qdr",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "    def _init_qdrant(self) -> None:\n        \"\"\"Initialize Qdrant client and collection with hybrid search support.\"\"\"\n        try:\n            from qdrant_client import QdrantClient\n            from qdrant_client.models import Distance, VectorParams, SparseVectorParams\n            \n            self._client = QdrantClient(url=self.qdrant_url)\n            \n            # Create collection if not exists - with hybrid vectors (dense + sparse)\n            if not self._client.collection_exists(self.col",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "\"\"\"Vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides QdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with hybrid search (dense + BM25 sparse).\n\nPhase 1: Vector Search Integration for ACE Fortune 100 Production Readiness.\n\nKey features:\n- Dense embeddings via LM Studio (nomic-embed-text-v1.5, 768-dim)\n- BM25 sparse vectors for keyword matching (technical terms)\n- Hybrid search with RRF fusion for best of both approaches\n"
      ],
      "ace_line_counts": [
        105,
        471,
        84,
        147,
        281
      ],
      "auggie_files": [
        "docs/API_REFERENCE.md"
      ],
      "auggie_contents": [
        "...\n    33\t```\n    34\t\n    35\t**Environment Variables:**\n    36\t- `ACE_EMBEDDING_URL` - Embedding server URL (default: `http://192.168.10.64:1234`)\n    37\t- `ACE_EMBEDDING_MODEL` - Model name (default: `qwen/qwen3-embedding-8b`)\n    38\t- `ACE_EMBEDDING_DIM` - Embedding dimension (default: `4096`)\n    39\t\n    40\t#### QdrantConfig\n    41\t\n    42\t```python\n    43\tfrom ace.config import QdrantConfig\n    44\t\n... (521 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"qdrant-client python models API 2024\"\n   num_results: 5\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        39
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "loguru-logger setup",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/audit.py"
      ],
      "ace_files": [
        "ace/observability/opik_integration.py",
        "ace/audit.py",
        "ace/typo_correction.py",
        "ace/roles.py",
        "ace/code_retrieval.py"
      ],
      "ace_scores": [
        0.36866449999999995,
        0.36138512999999994,
        0.3295616,
        0.32943153000000003,
        0.31969556
      ],
      "ace_contents": [
        "\"\"\"\nOpik Integration for ACE Framework\n\nProvides enterprise-grade observability and tracing for ACE components.\nReplaces custom explainability with production-ready Opik platform.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import asdict\n\nOpikLogger: Optional[type]\n\ntry:\n    import opik\n    from opik import track, opik_context\n\n    OPIK_AVAILABLE = True\n\n    # Try to import LiteLLM Opik",
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im",
        "\"\"\"Generator, Reflector, and Curator components.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom .delta import DeltaBatch\nfrom .llm import LLMClient\nfrom .playbook import Playbook\nfrom .prompts import CURATOR_PROMPT, GENERATOR_PROMPT, REFLECTOR_PROMPT\n\n# Import Opik tracing with graceful degradation\ntry:\n    from .observability.tracers import maybe_track\nexcept ImportError:",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca"
      ],
      "ace_line_counts": [
        348,
        130,
        652,
        115,
        37
      ],
      "auggie_files": [
        "ace/observability/opik_integration.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"\n     2\tOpik Integration for ACE Framework\n     3\t\n     4\tProvides enterprise-grade observability and tracing for ACE components.\n     5\tReplaces custom explainability with production-ready Opik platform.\n     6\t\"\"\"\n     7\t\n     8\tfrom __future__ import annotations\n     9\t\n    10\timport logging\n    11\tfrom datetime import datetime\n    12\tfrom typing import Any, Dict, List, Optional, Union\n    13\tfrom dataclasses import asdict\n... (603 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        62
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "tenacity-retry decorator",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "ace/retrieval_optimized.py",
        "ace/observability/metrics.py",
        "ace/multitenancy.py",
        "ace/observability/tracers.py"
      ],
      "ace_scores": [
        0.52470996,
        0.44454460000000007,
        0.44445960000000007,
        0.4439868,
        0.4360136
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"\nPrometheus Metrics Collection for ACE Framework (Phase 3D)\n\nThis module provides production-grade metrics collection using Prometheus client library.\nTracks retrieval latency, operation counts, and bullet counts across tenants.\n\nDESIGN NOTE: To support both labeled and unlabeled usage (for test compatibility),\nwe create wrapper classes that provide default label values when called without labels.\n\"\"\"\n\nimport time\nfrom contextlib import contextmanager\nfrom typing import Generator, Optional\n\nf",
        "\"\"\"\nMulti-tenancy support for ACE framework.\n\nThis module provides tenant isolation for playbooks and Qdrant collections,\nensuring that different tenants cannot access each other's data.\n\nFeatures:\n- Thread-local tenant context tracking\n- Tenant-scoped playbook storage\n- Tenant-scoped Qdrant collections\n- Cross-tenant access prevention\n- Path traversal attack protection\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nimport threading\nfrom pathlib import Path\nfrom typing import List, Optional\n",
        "\"\"\"\nACE-specific tracing utilities for Opik integration.\n\nProvides utilities for conditionally applying Opik tracing to ACE framework components.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Any, Callable, Optional, TypeVar\n\nlogger = logging.getLogger(__name__)\n\n# Type variable for decorated functions\nF = TypeVar(\"F\", bound=Callable[..., Any])\n\n# Flag to check if Opik is available\n_OPIK_AVAILABLE = False\ntry:\n    import opik\n    from opik import track\n\n    _OPIK_AVA"
      ],
      "ace_line_counts": [
        348,
        91,
        178,
        57,
        62
      ],
      "auggie_files": [
        "ace/resilience.py"
      ],
      "auggie_contents": [
        "...\n   171\t\n   172\t    def __call__(self, func: Callable[..., T]) -> Callable[..., T]:\n   173\t        \"\"\"Use circuit breaker as a decorator.\n   174\t\n   175\t        Args:\n   176\t            func: Function to wrap with circuit breaker\n   177\t\n   178\t        Returns:\n   179\t            Wrapped function\n   180\t        \"\"\"\n   181\t\n   182\t        @functools.wraps(func)\n... (539 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"tenacity python retry decorator 2026 best practices\"\n   num_results: 5\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        39
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Better chunk size (39 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "pydantic-v2 validation",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/chain_of_verification.py",
        "ace/hyde.py",
        "ace/security.py",
        "ace/enrichment.py",
        "ace/prompts_v2.py"
      ],
      "ace_scores": [
        0.4452728,
        0.42276576,
        0.4177269,
        0.41365130000000006,
        0.40667720000000007
      ],
      "ace_contents": [
        "\"\"\"Chain-of-Verification (CoVe) for improved reflection accuracy.\n\nThis module implements Chain-of-Verification, which generates verification\nquestions about the initial reflection, answers them independently, and\nuses the answers to refine the final output. This technique improves\naccuracy through self-verification.\n\nReference: Dhuliawala et al., \"Chain-of-Verification Reduces Hallucination\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom typing impor",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes",
        "\"\"\"LLM-based bullet enrichment for production-grade semantic scaffolding.\n\nThis module provides LLM-powered enrichment of bullets with semantic\nmetadata for intelligent retrieval. Unlike heuristic-based enrichment,\nthis uses the CURATOR_ENRICHMENT_PROMPT to extract accurate metadata.\n\nFor production use, always prefer LLMBulletEnricher over the heuristic\nenrich_bullet() function in playbook.py.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom typing import Any, Dict, List, Optional, TYP",
        "\"\"\"\nState-of-the-art prompt templates for ACE roles - Version 2.0\n\nDEPRECATION WARNING: This module (prompts_v2) is superseded by prompts_v2_1.\nPlease use prompts_v2_1.py for new projects, which includes:\n- MCP (Model Context Protocol) enhancements\n- Improved error handling and validation\n- Better structured reasoning templates\n- Enhanced meta-cognitive instructions\n\nFor migration guide, see docs/PROMPTS.md\n\nThese prompts incorporate best practices from production AI systems including:\n- Identit"
      ],
      "ace_line_counts": [
        118,
        58,
        18,
        39,
        894
      ],
      "auggie_files": [
        ".claude/project.json"
      ],
      "auggie_contents": [
        "     1\t{\n     2\t    \"name\": \"ACE Framework\",\n     3\t    \"description\": \"Agentic Context Engineering Framework\",\n     4\t    \"version\": \"0.5.0\",\n     5\t    \"python_version\": \"3.11+\",\n     6\t    \"entry_points\": [\n     7\t        \"ace/\",\n     8\t        \"examples/\",\n     9\t        \"tests/\"\n    10\t    ],\n    11\t    \"dependencies\": {\n    12\t        \"required\": [\n    13\t            \"litellm>=1.78.0\",\n... (573 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (5 vs 1)"
      ],
      "auggie_advantages": [
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "pytest-fixture setup",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "tests/"
      ],
      "ace_files": [
        "pyproject.toml",
        "tests/integrations/__init__.py",
        "ace/__init__.py",
        "verify_setup.py",
        "benchmark_ace_vs_auggie.py"
      ],
      "ace_scores": [
        0.38625301999999995,
        0.37524484999999996,
        0.35871615,
        0.35277345000000004,
        0.34469225000000003
      ],
      "ace_contents": [
        "Documentation = \"https://github.com/Kayba-ai/agentic-context-engine#readme\"\nRepository = \"https://github.com/Kayba-ai/agentic-context-engine\"\nIssues = \"https://github.com/Kayba-ai/agentic-context-engine/issues\"\n\n[dependency-groups]\ndev = [\n    \"pytest>=7.0.0\",\n    \"pytest-asyncio>=0.21.0\",\n    \"pytest-cov>=4.0.0\",\n    \"black>=23.0.0\",\n    \"mypy>=1.0.0\",\n    \"pre-commit>=3.0.0\",\n    \"git-changelog>=2.5.0\",\n]\ndemos = [\n    \"browser-use>=0.9.0\",\n    \"rich>=13.0.0\",\n    \"datasets>=2.0.0\",\n    \"pyyam",
        "\"\"\"Tests for ACE integrations with external agentic frameworks.\"\"\"\n",
        "    \"IntentClassifier\",\n    \"LLMClient\",\n    \"DummyLLMClient\",\n    \"TransformersLLMClient\",\n    \"LiteLLMClient\",\n    \"Generator\",\n    \"ReplayGenerator\",\n    \"SelfConsistencyGenerator\",\n    \"Reflector\",\n    \"CoVeReflector\",\n    \"Curator\",\n    \"GeneratorOutput\",\n    \"ReflectorOutput\",\n    \"CuratorOutput\",\n    \"OfflineAdapter\",\n    \"OnlineAdapter\",\n    \"Sample\",\n    \"TaskEnvironment\",\n    \"SimpleEnvironment\",\n    \"EnvironmentResult\",\n    \"AdapterStepResult\",\n    # Out-of-box integrations\n    \"ACELi",
        "def main():\n    print(\"\ud83d\ude80 ACE Framework Development Environment Verification\")\n    print(\"=\" * 60)\n\n    # Check Python version\n    python_version = sys.version_info\n    print(f\"\\n\ud83d\udccb Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n    if python_version < (3, 11):\n        print(\"\u274c Python 3.11+ required\")\n        return False\n    else:\n        print(\"\u2705 Python version OK\")\n\n    # Check if UV is available\n    has_uv = run_command(\"uv --version\", \"Checking UV insta",
        "\"\"\"Expanded ACE vs Auggie benchmark comparison.\n\nTests diverse query types across 5 categories:\n1. Code queries (functions, classes, patterns)\n2. Doc queries (guides, references, tutorials)\n3. Architecture queries (design, components, patterns)\n4. Config queries (settings, environment, credentials)\n5. Edge cases (specific symbols, error messages, imports)\n\nRuns ACE CodeRetrieval and optionally compares against Auggie MCP.\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nimport argparse\nfrom typing im"
      ],
      "ace_line_counts": [
        77,
        2,
        35,
        102,
        27
      ],
      "auggie_files": [
        "tests/conftest.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Shared pytest fixtures for ACE test suite.\n     2\t\n     3\tThis module provides reusable fixtures to eliminate code duplication\n     4\tacross test files and ensure consistent test data.\n     5\t\n     6\tNOTE: All tests use REAL implementations. Tests requiring LLM will be\n     7\tskipped if no API key is available. NO MOCKING/FAKING/STUBBING.\n     8\t\"\"\"\n     9\t\n    10\timport json\n    11\timport os\n    12\tfrom typing import Any, Dict\n    13\timport pytest\n... (566 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 2",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "Better chunk size (49 lines avg)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "numpy-array operations",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/async_retrieval.py",
        "ace/retrieval_bandit.py",
        "ace/query_features.py",
        "ace/delta.py",
        "ace/deduplication.py"
      ],
      "ace_scores": [
        0.33246567000000005,
        0.32982835,
        0.32798543,
        0.31480162999999994,
        0.31385381999999995
      ],
      "ace_contents": [
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"\nLinUCB Contextual Bandit for Adaptive Retrieval Strategy Selection\n\nPart of P7 ARIA (Adaptive Retrieval Intelligence Architecture).\n\nImplements P7.3 LinUCB algorithm with 4-arm retrieval strategy:\n- FAST: Low latency, minimal context\n- BALANCED: Moderate speed/quality tradeoff (cold start default)\n- DEEP: Maximum semantic depth\n- DIVERSE: Multi-perspective retrieval\n\nFormula: UCB(arm) = theta^T * x + alpha * sqrt(x^T * A^-1 * x)\nwhere:\n- theta = A^-1 * b (parameter estimate)\n- A = sum of x*x",
        "\"\"\"\nQuery Feature Extractor for LinUCB Bandit.\n\nPart of P7 ARIA (Adaptive Retrieval Intelligence Architecture).\n\nExtracts 10-dimension feature vector from queries for contextual bandit routing decisions.\nOptimized for <5ms extraction latency with >90% detection accuracy.\n\nThis module is an original contribution for adapting contextual bandits to RAG retrieval.\nThe feature set was designed empirically for query complexity classification.\n\"\"\"\n\nfrom typing import List\nimport re\n\n\nclass QueryFeature",
        "\"\"\"Delta operations produced by the ACE Curator.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Iterable, List, Literal, Optional, cast\n\n\nOperationType = Literal[\"ADD\", \"UPDATE\", \"TAG\", \"REMOVE\"]\n\n\n@dataclass\nclass DeltaOperation:\n    \"\"\"Single mutation to apply to the playbook.\n\n    Attributes:\n        type: Operation type (ADD, UPDATE, TAG, REMOVE)\n        section: Section name for the bullet\n        content: Bullet content text (",
        "\"\"\"\nAdvanced Memory Deduplication System for RAG.\n\nThis module provides clustering-based deduplication for Qdrant collections:\n- HDBSCAN/DBSCAN clustering for efficient duplicate detection (O(n log n) vs O(n^2))\n- Multi-collection support (ace_memories_hybrid, ace_unified)\n- Multiple merge strategies (keep_best, merge_content, canonical_form)\n- Cluster quality metrics (silhouette score, Davies-Bouldin index)\n- Dry-run mode for safe preview\n\nArchitecture:\n    1. Load memories with embeddings from"
      ],
      "ace_line_counts": [
        155,
        120,
        263,
        107,
        104
      ],
      "auggie_files": [
        "README.md"
      ],
      "auggie_contents": [
        "...\n   778\t\n   779\t### Horizontal Scaling\n   780\t\n   781\t```python\n   782\tfrom ace.scaling import ShardedBulletIndex, QdrantCluster, LoadBalancingStrategy\n   783\t\n   784\t# Sharded collections by tenant/domain\n   785\tsharded = ShardedBulletIndex(shard_strategy=ShardStrategy.TENANT)\n   786\tsharded.index_bullet(bullet, tenant_id=\"acme_corp\")\n   787\t\n   788\t# Clustered Qdrant with load balancing\n   789\tcluster = QdrantCluster(\n... (570 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (5 vs 1)"
      ],
      "auggie_advantages": [
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "json-schema validation",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "glm_test.json",
        "glm_test2.json",
        "ace/delta.py",
        "ace/chain_of_verification.py",
        "ace/prompts.py"
      ],
      "ace_scores": [
        0.40595284,
        0.36709877,
        0.36683007,
        0.36239465000000004,
        0.3515844
      ],
      "ace_contents": [
        "{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"\",\n        \"reasoning_content\": \"\\n1.  **\u62c6\u89e3\u539f\u59cb\u67e5\u8be2\uff1a**\\n    *   **\u6838\u5fc3\u52a8\u4f5c\uff1a** `validate`\uff08\u9a8c\u8bc1\uff09\\n    *   **\u6838\u5fc3\u5bf9\u8c61\uff1a** `user input`\uff08\u7528\u6237\u8f93\u5165\uff09\\n\\n2.  **\u4e3a\u6bcf\u4e2a\u90e8\u5206\u8fdb\u884c\u6784\u601d\uff0c\u5bfb\u627e\u540c\u4e49\u8bcd\u548c\u76f8\u5173\u6982\u5ff5\uff1a**\\n\\n    *   **`validate`\uff08\u9a8c\u8bc1\uff09\uff1a**\\n        *   \u8fd8\u6709\u54ea\u4e9b\u5176\u4ed6\u65b9\u5f0f\u53ef\u4ee5\u8868\u8fbe\u201c\u9a8c\u8bc1\u201d\uff1f\\n            *   check\uff08\u68c0\u67e5\uff09\\n            *   verify\uff08\u6838\u5b9e\uff09\\n            *   sanitize\uff08\u6e05\u7406\uff09\\n            *   clean\uff08\u6e05\u9664\uff09\\n            *   filter\uff08\u8fc7\u6ee4\uff09\\n            *   secure\uff08\u4fdd\u62a4\uff09\\n",
        "{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"\",\n        \"reasoning_content\": \"\\n1.  **\u5206\u6790\u6838\u5fc3\u67e5\u8be2\uff1a**\\n    *   **\u6838\u5fc3\u540d\u8bcd\uff1a**\u201cuser input\u201d\uff08\u7528\u6237\u8f93\u5165\uff09\\n    *   **\u6838\u5fc3\u52a8\u8bcd\uff1a**\u201cvalidate\u201d\uff08\u9a8c\u8bc1\uff09\\n    *   **\u9690\u542b\u76ee\u6807\uff1a**\u7528\u6237\u5e0c\u671b\u5b66\u4e60\u5982\u4f55\u786e\u4fdd\u6765\u81ea\u7528\u6237\u7684\u6570\u636e\u662f\u6b63\u786e\u7684\u3001\u5b89\u5168\u7684\uff0c\u5e76\u7b26\u5408\u9884\u671f\u683c\u5f0f\u3002\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5e38\u89c1\u7684\u7f16\u7a0b\u4efb\u52a1\u3002\\n\\n2.  **\u6784\u601d\u5173\u952e\u8bcd\u548c\u540c\u4e49\u8bcd\uff1a**\\n    *   **\u201cValidate\u201d\uff08\u9a8c\u8bc1\uff09\uff1a** check\uff08\u68c0\u67e5\uff09\u3001verify\uff08\u6838\u5b9e\uff09\u3001sanitize\uff08\u6e05\u7406\uff09\u3001clean\uff08\u6e05\u9664\uff09\u3001filter\uff08\u8fc7\u6ee4\uff09\u3001ensure\uff08\u786e\u4fdd\uff09\u3001confirm\uff08\u786e\u8ba4\uff09\u3001test\uff08\u6d4b\u8bd5\uff09\u3002\\n    *   **\u201cUser Input\u201d\uff08\u7528\u6237\u8f93\u5165\uff09\uff1a** form data\uff08\u8868\u5355\u6570\u636e\uff09\u3001data entry\uff08",
        "\"\"\"Delta operations produced by the ACE Curator.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Iterable, List, Literal, Optional, cast\n\n\nOperationType = Literal[\"ADD\", \"UPDATE\", \"TAG\", \"REMOVE\"]\n\n\n@dataclass\nclass DeltaOperation:\n    \"\"\"Single mutation to apply to the playbook.\n\n    Attributes:\n        type: Operation type (ADD, UPDATE, TAG, REMOVE)\n        section: Section name for the bullet\n        content: Bullet content text (",
        "\"\"\"Chain-of-Verification (CoVe) for improved reflection accuracy.\n\nThis module implements Chain-of-Verification, which generates verification\nquestions about the initial reflection, answers them independently, and\nuses the answers to refine the final output. This technique improves\naccuracy through self-verification.\n\nReference: Dhuliawala et al., \"Chain-of-Verification Reduces Hallucination\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom typing impor",
        "{playbook_excerpt}\n\nReturn JSON:\n{{\n  \"reasoning\": \"<analysis>\",\n  \"error_identification\": \"<what went wrong>\",\n  \"root_cause_analysis\": \"<why it happened>\",\n  \"correct_approach\": \"<what should be done>\",\n  \"key_insight\": \"<reusable takeaway>\",\n  \"bullet_tags\": [\n    {{\"id\": \"<bullet-id>\", \"tag\": \"helpful|harmful|neutral\"}}\n  ]\n}}\n\"\"\"\n\n\n# Default Curator prompt - updates playbook based on reflections\nCURATOR_PROMPT = \"\"\"\\\nYou are the curator of the ACE playbook. Merge the latest reflection into "
      ],
      "ace_line_counts": [
        26,
        26,
        107,
        118,
        50
      ],
      "auggie_files": [
        ".claude/project.json"
      ],
      "auggie_contents": [
        "     1\t{\n     2\t    \"name\": \"ACE Framework\",\n     3\t    \"description\": \"Agentic Context Engineering Framework\",\n     4\t    \"version\": \"0.5.0\",\n     5\t    \"python_version\": \"3.11+\",\n     6\t    \"entry_points\": [\n     7\t        \"ace/\",\n     8\t        \"examples/\",\n     9\t        \"tests/\"\n    10\t    ],\n    11\t    \"dependencies\": {\n    12\t        \"required\": [\n    13\t            \"litellm>=1.78.0\",\n... (577 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "re-regex pattern matching",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/retrieval.py",
        "ace/pattern_detector.py",
        "tenant_data/learned_typos.json",
        "ace/query_features.py",
        "debug_doc_ranking.py"
      ],
      "ace_scores": [
        0.43513493999999997,
        0.3656003,
        0.35063502,
        0.33520903999999996,
        0.33441940000000003
      ],
      "ace_contents": [
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non",
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef lo",
        "    \"session\": \"scession\",\n    \"wired\": \"weird\",\n    \"requests\": \"request\",\n    \"pipeline\": \"pipelines\",\n    \"there\": \"their\",\n    \"feature\": \"future\",\n    \"works\": \"work\",\n    \"anything\": \"anythink\",\n    \"degredation\": \"degradation\",\n    \"com\": \"come\",\n    \"aria\": \"area\",\n    \"eith\": \"either\",\n    \"elf\": \"self\",\n    \"ensbled\": \"enabled\",\n    \"mocking\": \"moking\",\n    \"expectations\": \"expectation\",\n    \"message\": \"massage\",\n    \"command\": \"commande\",\n    \"phase\": \"phrase\",\n    \"tools\": \"tool\",\n  ",
        "\"\"\"\nQuery Feature Extractor for LinUCB Bandit.\n\nPart of P7 ARIA (Adaptive Retrieval Intelligence Architecture).\n\nExtracts 10-dimension feature vector from queries for contextual bandit routing decisions.\nOptimized for <5ms extraction latency with >90% detection accuracy.\n\nThis module is an original contribution for adapting contextual bandits to RAG retrieval.\nThe feature set was designed empirically for query complexity classification.\n\"\"\"\n\nfrom typing import List\nimport re\n\n",
        "\"\"\"Debug doc ranking for pattern queries.\"\"\"\n\nfrom ace.code_retrieval import CodeRetrieval\n\nr = CodeRetrieval()\n\n# Check if docs even get embedded/indexed\nquery = \"error handling patterns in Python with try except\"\nprint(f\"Query: {query}\")\nprint(\"=\" * 60)\n\n# Run search with more results to see docs\nresults = r.search(query, limit=50)\n\n# Separate code and docs\ncode_files = [res for res in results if not res['file_path'].endswith('.md')]\ndoc_files = [res for res in results if res['file_path'].ends"
      ],
      "ace_line_counts": [
        393,
        385,
        133,
        16,
        34
      ],
      "auggie_files": [
        "ace/pattern_detector.py"
      ],
      "auggie_contents": [
        "...\n    11\t\n    12\timport os\n    13\timport re\n    14\timport json\n    15\tfrom dataclasses import dataclass, field\n    16\tfrom typing import List, Dict, Any, Optional\n    17\tfrom datetime import datetime\n    18\tfrom pathlib import Path\n    19\t\n    20\t\n    21\tdef load_config() -> Dict[str, Any]:\n    22\t    \"\"\"Load configuration from environment variables.\"\"\"\n... (456 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "asyncio-gather parallel",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "compare_ace_auggie_headtohead.py",
        "ace/hyde.py",
        "benchmark_ace_vs_auggie.py"
      ],
      "ace_scores": [
        0.47626820000000003,
        0.44969305000000004,
        0.41352142999999997,
        0.3829196,
        0.37475802999999996
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Head-to-head ACE vs Auggie comparison test.\n\nFor each query, we call both systems and compare:\n1. Top file match\n2. Top 3 files overlap\n3. Content relevance\n\"\"\"\nimport subprocess\nimport sys\nimport json\nfrom typing import Dict, List, Any\n\n# Test queries - comprehensive coverage\nTEST_QUERIES = [\n    # Core ACE classes\n    \"EmbeddingConfig class definition dataclass\",\n    \"UnifiedMemoryIndex class Qdrant namespace hybrid search\",\n    \"CodeRetrieval class search method dense vector\",\n    \"ASTChun",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"Expanded ACE vs Auggie benchmark comparison.\n\nTests diverse query types across 5 categories:\n1. Code queries (functions, classes, patterns)\n2. Doc queries (guides, references, tutorials)\n3. Architecture queries (design, components, patterns)\n4. Config queries (settings, environment, credentials)\n5. Edge cases (specific symbols, error messages, imports)\n\nRuns ACE CodeRetrieval and optionally compares against Auggie MCP.\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nimport argparse\nfrom typing im"
      ],
      "ace_line_counts": [
        364,
        155,
        94,
        58,
        120
      ],
      "auggie_files": [
        "ace/async_retrieval.py"
      ],
      "auggie_contents": [
        "...\n    55\t\n    56\t\n    57\tclass AsyncQdrantBulletIndex:\n    58\t    \"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n    59\t\n    60\t    Provides O(1) semantic retrieval using:\n    61\t    - Dense vectors from LM Studio (nomic-embed-text-v1.5)\n    62\t    - BM25 sparse vectors for keyword matching\n    63\t    - Hybrid search with RRF fusion\n    64\t    - Async operations for concurrent execution\n    65\t\n    66\t    Example:\n... (492 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "functools-lru-cache decorator",
      "category": "TechnicalIdentifiers",
      "expected_files": [
        "ace/caching.py",
        "ace/gemini_embeddings.py"
      ],
      "ace_files": [
        "ace/caching.py",
        "ace/retrieval_caching.py",
        "ace/resilience.py",
        "ace/hyde.py",
        "ace/pattern_detector.py"
      ],
      "ace_scores": [
        0.5704653000000001,
        0.5182247400000001,
        0.47245760000000003,
        0.4617669,
        0.44204667
      ],
      "ace_contents": [
        "\"\"\"Response caching for efficient multi-epoch training.\n\nThis module provides caching mechanisms to avoid redundant LLM calls\nduring multi-epoch training, saving time and costs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom threading import Lock\nfrom typing import Any, Dict, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .llm import LLMClient\n\n\n@datacl",
        "\"\"\"\nRetrieval-specific caching layer for ACE Framework (Phase 4B).\n\nThis module caches RETRIEVAL data (embeddings, query results), NOT LLM responses.\nFor LLM response caching, see ace/caching.py.\n\nCaching Strategy:\n- EmbeddingCache: Text -> embedding vector (768-dim floats)\n- QueryResultCache: Query -> List[QdrantScoredResult] with bullet-aware invalidation\n\nBoth caches use LRU eviction with optional TTL expiration.\n\"\"\"\n\nfrom threading import Lock\nfrom collections import OrderedDict\nfrom datacla",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef lo"
      ],
      "ace_line_counts": [
        346,
        250,
        149,
        172,
        56
      ],
      "auggie_files": [
        "ace/caching.py"
      ],
      "auggie_contents": [
        "...\n    30\t\n    31\t\n    32\tclass ResponseCache:\n    33\t    \"\"\"LRU cache for LLM responses with TTL support.\n    34\t\n    35\t    Provides efficient caching of LLM responses to avoid redundant calls\n    36\t    during multi-epoch training. Supports:\n    37\t    - TTL (time-to-live) for automatic expiration\n    38\t    - LRU eviction when max size is reached\n    39\t    - Context-aware caching (same prompt, different context = different entry)\n    40\t    - Persistence to disk\n    41\t    - Hit rate metrics\n... (531 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "def search function in retrieval",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_retrieval.py",
        "ace/retrieval.py"
      ],
      "ace_files": [
        "ace/retrieval_optimized.py",
        "ace/code_retrieval.py",
        "ace/retrieval_optimized.py",
        "ace/code_retrieval.py",
        "ace/retrieval.py"
      ],
      "ace_scores": [
        0.9217693,
        0.87711424,
        0.7782106,
        0.771541,
        0.75181247
      ],
      "ace_contents": [
        "    def search(\n        self,\n        query: str,\n        limit: int = None,\n        return_metrics: bool = False\n    ) -> List[RetrievalResult] | Tuple[List[RetrievalResult], SearchMetrics]:\n        \"\"\"\n        Search for relevant memories.\n\n        Args:\n            query: Search query\n            limit: Maximum results (default from config)\n            return_metrics: Whether to return search metrics\n\n        Returns:\n            List of RetrievalResult, optionally with SearchMetrics\n        ",
        "    def _apply_filename_boost(self, query: str, file_path: str, score: float, content: str = \"\") -> float:\n        \"\"\"\n        Apply filename and content boost when query terms match file path or definitions.\n        \n        This mimics Auggie MCP's behavior where files with names matching\n        query terms OR containing class/function definitions get prioritized.\n        \n        Args:\n            query: Original search query\n            file_path: File path being scored\n            score: O",
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p"
      ],
      "ace_line_counts": [
        116,
        773,
        188,
        147,
        137
      ],
      "auggie_files": [
        "ace/retrieval.py"
      ],
      "auggie_contents": [
        "...\n   207\t\n   208\t    def retrieve(\n   209\t        self,\n   210\t        query: Optional[str] = None,\n   211\t        task_type: Optional[str] = None,\n   212\t        domain: Optional[str] = None,\n   213\t        complexity: Optional[str] = None,\n   214\t        intent: Optional[IntentType] = None,\n   215\t        limit: Optional[int] = None,\n   216\t        rank_by_effectiveness: bool = False,\n   217\t        min_effectiveness: Optional[float] = None,\n   218\t        query_type: Optional[str] = None,\n... (471 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 4 advantages vs 2",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.922)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)",
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "async def embed method",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/async_retrieval.py",
        "ace/gemini_embeddings.py"
      ],
      "ace_files": [
        "ace/async_retrieval.py",
        "ace/gemini_embeddings.py",
        "ace/openai_embeddings.py",
        "ace/semantic_scorer.py",
        "ace/qdrant_retrieval.py"
      ],
      "ace_scores": [
        0.7737361,
        0.544801,
        0.5193787,
        0.5176296,
        0.5061246
      ],
      "ace_contents": [
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "    def _get_embedding(self, text: str) -> List[float]:\n        \"\"\"Get dense embedding from LM Studio with automatic EOS token handling.\n\n        Args:\n            text: Text to embed (truncated to 8000 chars)\n\n        Returns:\n            768-dimensional embedding vector.\n\n        Raises:\n            RuntimeError: If embedding request fails.\n        \"\"\"\n        try:\n            # Add EOS token for Qwen models to fix GGUF tokenizer warning\n            # This ensures proper sentence boundary dete"
      ],
      "ace_line_counts": [
        363,
        280,
        211,
        104,
        115
      ],
      "auggie_files": [
        "ace/async_retrieval.py"
      ],
      "auggie_contents": [
        "...\n   165\t\n   166\t        Raises:\n   167\t            Exception: If embedding request fails.\n   168\t        \"\"\"\n   169\t        # Import httpx here so mocking works (patch('httpx.AsyncClient'))\n   170\t        import httpx\n   171\t\n   172\t        # Create client if needed (allows mocking to work)\n   173\t        async with httpx.AsyncClient(timeout=30.0) as client:\n   174\t            resp = await client.post(\n   175\t                f\"{self._embedding_url}/v1/embeddings\",\n   176\t                json={\n... (495 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.774)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "def _apply_filename_boost",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/code_retrieval.py",
        "ace/retrieval.py",
        "debug_doc_ranking.py",
        "debug_docs.py",
        "ace/code_retrieval.py"
      ],
      "ace_scores": [
        1.2169392,
        0.41922212,
        0.41512567,
        0.39980078,
        0.3882861
      ],
      "ace_contents": [
        "    def _apply_filename_boost(self, query: str, file_path: str, score: float, content: str = \"\") -> float:\n        \"\"\"\n        Apply filename and content boost when query terms match file path or definitions.\n        \n        This mimics Auggie MCP's behavior where files with names matching\n        query terms OR containing class/function definitions get prioritized.\n        \n        Args:\n            query: Original search query\n            file_path: File path being scored\n            score: O",
        "    def _apply_elf_scoring(\n        self,\n        bullet: Any,\n        base_score: float,\n        match_reasons: List[str],\n    ) -> Tuple[float, List[str]]:\n        \"\"\"Apply ELF-inspired scoring adjustments based on config flags.\n\n        Args:\n            bullet: The bullet being scored (UnifiedBullet or EnrichedBullet)\n            base_score: The initial score before ELF adjustments\n            match_reasons: List of match reasons to append to\n\n        Returns:\n            Tuple of (adjusted_",
        "\"\"\"Debug doc ranking for pattern queries.\"\"\"\n\nfrom ace.code_retrieval import CodeRetrieval\n\nr = CodeRetrieval()\n\n# Check if docs even get embedded/indexed\nquery = \"error handling patterns in Python with try except\"\nprint(f\"Query: {query}\")\nprint(\"=\" * 60)\n\n# Run search with more results to see docs\nresults = r.search(query, limit=50)\n\n# Separate code and docs\ncode_files = [res for res in results if not res['file_path'].endswith('.md')]\ndoc_files = [res for res in results if res['file_path'].ends",
        "import os\nimport re\n\n# Simulate the actual function\nfile_path = \"docs/INTEGRATION_GUIDE.md\"\nquery = \"try except error handling pattern\"\n\nstop_words = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'how', 'what',\n              'where', 'when', 'why', 'can', 'will', 'method', 'function', 'class',\n              'code', 'file', 'def', 'implementation', 'search', 'find', 'get', 'set',\n              'pattern', 'error', 'handling', 'import', 'logging', 'logger', 'setup',\n              'exception",
        "    def _is_test_file(self, file_path: str) -> bool:\n        \"\"\"Check if a file path is a test file.\n        \n        Args:\n            file_path: Relative file path\n            \n        Returns:\n            True if file is a test file\n        \"\"\"\n        path_lower = file_path.lower()\n        \n        # Test directory patterns\n        if '/tests/' in path_lower or '\\\\tests\\\\' in path_lower:\n            return True\n        if '/test/' in path_lower or '\\\\test\\\\' in path_lower:\n            return"
      ],
      "ace_line_counts": [
        530,
        68,
        34,
        49,
        72
      ],
      "auggie_files": [
        "ace/code_retrieval.py"
      ],
      "auggie_contents": [
        "...\n   227\t    \n   228\t    def _apply_filename_boost(self, query: str, file_path: str, score: float, content: str = \"\") -> float:\n   229\t        \"\"\"\n   230\t        Apply filename and content boost when query terms match file path or definitions.\n   231\t        \n   232\t        This mimics Auggie MCP's behavior where files with names matching\n   233\t        query terms OR containing class/function definitions get prioritized.\n   234\t        \n   235\t        Args:\n   236\t            query: Original search query\n   237\t            file_path: File path being scored\n   238\t            score: Original embedding similarity score\n... (388 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (1.217)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "create_sparse_vector BM25 function",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/unified_memory.py"
      ],
      "ace_files": [
        "ace/unified_memory.py",
        "ace/hyde_retrieval.py",
        "ace/retrieval_optimized.py",
        "ace/retrieval_presets.py",
        "ace/qdrant_retrieval.py"
      ],
      "ace_scores": [
        0.71364614,
        0.60162824,
        0.5255808,
        0.51826346,
        0.51576316
      ],
      "ace_contents": [
        "    class _MatchValue:\n        value: Any\n    MatchValue = _MatchValue\n\n    @dataclass\n    class _MatchAny:\n        any: List[Any]\n    MatchAny = _MatchAny\n\n    @dataclass\n    class _Filter:\n        must: Optional[List[Any]] = None\n        should: Optional[List[Any]] = None\n    Filter = _Filter\n\n\n# =============================================================================\n# NAMESPACE AND SOURCE ENUMS\n# =============================================================================\n\nclass Unifie",
        "    def _compute_bm25_sparse(self, text: str) -> Dict[str, Any]:\n        \"\"\"Compute BM25-style sparse vector for Qdrant.\n\n        Args:\n            text: Text to vectorize\n\n        Returns:\n            Dict with 'indices' (term hashes) and 'values' (BM25 weights)\n        \"\"\"\n        tokens = self._tokenize_for_bm25(text)\n        if not tokens:\n            return {\"indices\": [], \"values\": []}\n\n        tf = Counter(tokens)\n        doc_length = len(tokens)\n\n        indices = []\n        values = []\n",
        "def tokenize_bm25(text: str) -> List[str]:\n    \"\"\"\n    Tokenize text for BM25, preserving technical terms.\n    \"\"\"\n    # Split CamelCase\n    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n    # Split snake_case\n    text = text.replace('_', ' ')\n    # Extract tokens\n    tokens = re.findall(r'[a-zA-Z0-9]+', text.lower())\n    # Filter\n    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n    return tokens\n\n\ndef compute_bm25_sparse(\n    text: str,\n    k1: float = 1.5,\n    b: float ",
        "\"\"\"\nRetrieval Presets - Optimized configurations for different query types.\n\nBased on empirical testing:\n- Baseline precision: 75.6%\n- Architecture queries: 33.3% (worst)\n- Target: 95%+ precision across all categories\n\nWinning optimizations:\n1. BM25-heavy weighting (dense=0.3, sparse=0.7) -> +50% P@3\n2. Post-retrieval deduplication (0.90 threshold) -> +2.7%\n3. Query expansion with domain synonyms -> +3% (conditional)\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing",
        "\"\"\"Vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides QdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with hybrid search (dense + BM25 sparse).\n\nPhase 1: Vector Search Integration for ACE Fortune 100 Production Readiness.\n\nKey features:\n- Dense embeddings via LM Studio (nomic-embed-text-v1.5, 768-dim)\n- BM25 sparse vectors for keyword matching (technical terms)\n- Hybrid search with RRF fusion for best of both approaches\n"
      ],
      "ace_line_counts": [
        101,
        85,
        81,
        20,
        70
      ],
      "auggie_files": [
        "rag_training/optimizations/v8_bm25_hybrid.py",
        "rag_training/scripts/reindex_with_lmstudio.py"
      ],
      "auggie_contents": [
        "...\n   119\t\n   120\t\n   121\t# ============================================================================\n   122\t# BM25 SPARSE VECTORS\n   123\t# ============================================================================\n   124\t\n   125\tdef tokenize_bm25(text: str) -> List[str]:\n   126\t    \"\"\"Tokenize text for BM25, preserving technical terms.\"\"\"\n   127\t    # Split CamelCase\n   128\t    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n   129\t    # Split snake_case\n   130\t    text = text.replace('_', ' ')\n... (509 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"create_sparse_vector function, sparse embeddings, BM25 scoring, text retrieval\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   148\t\n   149\t\n   150\tdef get_lmstudio_embeddings_batch(client: httpx.Client, texts: List[str], model: str) -> List[List[float]]:\n   151\t    \"\"\"Get embeddings for multiple texts from LM Studio.\"\"\"\n   152\t    embeddings = []\n   153\t    for text in texts:\n   154\t        emb = get_lmstudio_embedding(client, text, model)\n   155\t        embeddings.append(emb)\n   156\t    return embeddings\n   157\t\n   158\t\n   159\tdef compute_bm25_sparse(text: str) -> Dict[str, Any]:\n... (567 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "def format_auggie_style",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/code_retrieval.py",
        "ace_vs_auggie_headtohead.py",
        "tests/compare_ace_auggie_dynamic.py",
        "ace/code_retrieval.py",
        "benchmark_ace_vs_auggie.py"
      ],
      "ace_scores": [
        1.17661355,
        0.3993804,
        0.39126575,
        0.38816182,
        0.3870425
      ],
      "ace_contents": [
        "    def format_auggie_style(\n        self,\n        results: List[Dict[str, Any]],\n        max_lines_per_file: int = 200,\n        expand_context: bool = True,\n        context_lines_before: int = 20,\n        context_lines_after: int = 20,\n    ) -> str:\n        \"\"\"\n        Format results in Auggie MCP-compatible style.\n        \n        Auggie format:\n            The following code sections were retrieved:\n            Path: file/path.py\n                 1\tline content\n                 2\tline content",
        "def normalize_path(path: str) -> str:\n    \"\"\"Normalize path for comparison.\"\"\"\n    if not path:\n        return \"\"\n    # Convert to lowercase, replace backslashes\n    path = path.replace(\"\\\\\", \"/\").lower()\n    # Extract just the filename for comparison\n    return os.path.basename(path)\n\n\ndef paths_match(path1: str, path2: str) -> bool:\n    \"\"\"Check if two paths refer to the same file.\"\"\"\n    if not path1 or not path2:\n        return False\n    n1 = normalize_path(path1)\n    n2 = normalize_path(pat",
        "#!/usr/bin/env python3\n\"\"\"\nDYNAMIC ACE vs Auggie MCP Comparison Test\n\nThis test queries Auggie MCP to get GROUND TRUTH, then verifies ACE matches.\nNO hardcoded expectations - Auggie's results ARE the expected results.\n\nThe test flow:\n1. For each query: Call Auggie MCP via subprocess/script to get actual results\n2. Record the TOP file Auggie returns (the ground truth)\n3. Query ACE with the same query\n4. Verify ACE's top result matches Auggie's top result\n\nThis is the ONLY valid way to test - Augg",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "\"\"\"Expanded ACE vs Auggie benchmark comparison.\n\nTests diverse query types across 5 categories:\n1. Code queries (functions, classes, patterns)\n2. Doc queries (guides, references, tutorials)\n3. Architecture queries (design, components, patterns)\n4. Config queries (settings, environment, credentials)\n5. Edge cases (specific symbols, error messages, imports)\n\nRuns ACE CodeRetrieval and optionally compares against Auggie MCP.\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nimport argparse\nfrom typing im"
      ],
      "ace_line_counts": [
        211,
        104,
        30,
        147,
        120
      ],
      "auggie_files": [
        "final_test.py",
        "ace/code_retrieval.py",
        "ace/code_retrieval.py"
      ],
      "auggie_contents": [
        "...\n    19\t\n    20\t# Also test the Auggie-style output\n    21\tprint(\"\\n\" + \"=\"*60)\n    22\tprint(\"=== Auggie-style formatted output ===\")\n    23\tprint(\"=\"*60)\n    24\t# FIX: format_auggie_style expects List[Dict], not Dict with \"results\" key\n    25\toutput = retriever.format_auggie_style(results[:5])\n    26\tprint(output[:2000] if len(output) > 2000 else output)\n...\n",
        "...\n... (509 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"formatting functions, style utilities, or text formatting helpers in the codebase\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n  1338\t    \n  1339\t    def format_auggie_style(\n  1340\t        self,\n  1341\t        results: List[Dict[str, Any]],\n  1342\t        max_lines_per_file: int = 200,\n  1343\t        expand_context: bool = True,\n  1344\t        context_lines_before: int = 20,\n  1345\t        context_lines_after: int = 20,\n  1346\t    ) -> str:\n  1347\t        \"\"\"\n  1348\t        Format results in Auggie MCP-compatible style.\n  1349\t        \n... (514 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        11,
        8,
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 5 advantages vs 0",
      "ace_advantages": [
        "Higher rank for expected file (1 vs 2)",
        "More unique files (3 vs 1)",
        "High confidence top score (1.177)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 2
    },
    {
      "query": "def get_embedding batch",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_retrieval.py",
        "ace/openai_embeddings.py"
      ],
      "ace_files": [
        "ace/retrieval_optimized.py",
        "ace/openai_embeddings.py",
        "ace/async_retrieval.py",
        "ace/gemini_embeddings.py",
        "ace/semantic_scorer.py"
      ],
      "ace_scores": [
        0.8946719,
        0.8936214,
        0.8883245999999999,
        0.63329834,
        0.62683564
      ],
      "ace_contents": [
        "class OptimizedRetriever:\n    \"\"\"\n    State-of-the-art retriever with hybrid search, query expansion,\n    multi-query RRF fusion, and cross-encoder re-ranking.\n    \"\"\"\n\n    def __init__(self, config: Dict[str, Any] = None):\n        \"\"\"\n        Initialize retriever with configuration.\n\n        Args:\n            config: Override default configuration values\n        \"\"\"\n        self.config = {**_get_default_config(), **(config or {})}\n\n        if not HTTPX_AVAILABLE:\n            raise ImportError(\"",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_"
      ],
      "ace_line_counts": [
        80,
        211,
        269,
        88,
        104
      ],
      "auggie_files": [
        "ace/gemini_embeddings.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Gemini Embedding Client for ACE Framework.\n     2\t\n     3\tProvides embeddings using Google's gemini-embedding-001 model\n     4\twith proper task type optimization for retrieval (document vs query).\n     5\t\n     6\tUsage:\n     7\t    from ace.gemini_embeddings import GeminiEmbeddingClient\n     8\t\n     9\t    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n    10\t\n    11\t    # For indexing documents\n    12\t    doc_embedding = client.embed_document(\"This is a document about...\")\n    13\t\n... (542 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "async def retrieve_async",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/unified_memory.py",
        "ace/retrieval.py",
        "ace/async_retrieval.py",
        "ace/retrieval_optimized.py",
        "ace/retrieval.py"
      ],
      "ace_scores": [
        0.5532152,
        0.53650284,
        0.5312494,
        0.5266208,
        0.5192983
      ],
      "ace_contents": [
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non"
      ],
      "ace_line_counts": [
        693,
        137,
        269,
        91,
        393
      ],
      "auggie_files": [
        "ace/async_retrieval.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n     2\t\n     3\tThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\n     4\tbullets using Qdrant vector database with async operations.\n     5\t\n     6\tPhase 4A: Async Operations for ACE Framework.\n     7\t\n     8\tKey features:\n     9\t- Async embedding retrieval via httpx.AsyncClient\n    10\t- Parallel batch processing with asyncio.gather\n    11\t- Concurrent query handling\n    12\t- Non-blocking Qdrant operations\n    13\t\"\"\"\n... (500 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 3)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 3,
      "auggie_expected_rank": 1
    },
    {
      "query": "def index_file method",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_indexer.py"
      ],
      "ace_files": [
        "ace/code_indexer.py",
        "check_chunks.py",
        "ace/dependency_graph.py",
        "ace/code_retrieval.py",
        "ace/code_retrieval.py"
      ],
      "ace_scores": [
        0.7712958,
        0.484246,
        0.39106578,
        0.38063157,
        0.36860427
      ],
      "ace_contents": [
        "    def chunk_file(self, file_path: str) -> List[CodeChunkIndexed]:\n        \"\"\"\n        Parse and chunk a code file.\n        \n        Args:\n            file_path: Absolute path to code file\n            \n        Returns:\n            List of CodeChunkIndexed instances\n        \"\"\"\n        chunks = []\n        \n        # Check file exists\n        if not os.path.exists(file_path):\n            logger.warning(f\"File not found: {file_path}\")\n            return chunks\n        \n        # Read file content\n",
        "\"\"\"Check indexed chunks for a specific file.\"\"\"\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Filter, FieldCondition, MatchValue\n\nc = QdrantClient('http://localhost:6333')\nr = c.scroll(\n    'ace_code_context', \n    limit=50,\n    scroll_filter=Filter(\n        must=[FieldCondition(key='file_path', match=MatchValue(value='ace/unified_memory.py'))]\n    ),\n    with_payload=True\n)\n\nprint(f\"Found {len(r[0])} chunks for ace/unified_memory.py\")\nprint()\n\nfor p in sorted(r[0], key",
        "    def analyze_file(self, filepath: str) -> Dict:\n        \"\"\"Analyze a complete file for imports, symbols, and call graph.\n\n        Args:\n            filepath: Path to source file\n\n        Returns:\n            Dict containing:\n                - imports: List of Import objects\n                - symbols: List of defined symbols (functions, classes)\n                - call_graph: List of CallEdge objects\n\n        Raises:\n            FileNotFoundError: If file doesn't exist\n        \"\"\"\n        path ",
        "    def _apply_filename_boost(self, query: str, file_path: str, score: float, content: str = \"\") -> float:\n        \"\"\"\n        Apply filename and content boost when query terms match file path or definitions.\n        \n        This mimics Auggie MCP's behavior where files with names matching\n        query terms OR containing class/function definitions get prioritized.\n        \n        Args:\n            query: Original search query\n            file_path: File path being scored\n            score: O",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca"
      ],
      "ace_line_counts": [
        392,
        20,
        98,
        530,
        37
      ],
      "auggie_files": [
        "ace/code_indexer.py"
      ],
      "auggie_contents": [
        "...\n   193\t\n   194\tclass CodeIndexer:\n   195\t    \"\"\"\n   196\t    Index workspace code files for semantic search.\n   197\t    \n   198\t    Scans workspace directories, parses code using ASTChunker,\n   199\t    generates embeddings, and stores in Qdrant for retrieval.\n   200\t    \n   201\t    Features:\n   202\t    - Multi-language support via ASTChunker\n   203\t    - Incremental updates on file changes\n   204\t    - File watching for auto-updates\n... (437 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.771)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "def parse_ast method",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_chunker.py",
        "ace/code_analysis.py"
      ],
      "ace_files": [
        "ace/code_analysis.py",
        "ace/code_chunker.py",
        "ace/code_analysis.py",
        "ace/dependency_graph.py",
        "rag_training/training_data/crossencoder_training_pairs.json"
      ],
      "ace_scores": [
        0.49335623,
        0.4603936,
        0.43795794,
        0.41469902,
        0.3727705
      ],
      "ace_contents": [
        "        def visit_node(node, parent_class=None):\n            \"\"\"Recursively visit AST nodes.\"\"\"\n            # Extract interface definitions\n            if node.type == \"interface_declaration\":\n                name_node = node.child_by_field_name(\"name\")\n                if name_node:\n                    name = code[name_node.start_byte : name_node.end_byte]\n                    symbol = CodeSymbol(\n                        name=name,\n                        kind=\"interface\",\n                       ",
        "\"\"\"AST-based semantic code chunking module.\n\nThis module provides intelligent code chunking that respects language syntax\nboundaries (functions, classes, methods) rather than arbitrary line counts.\n\nSupports multiple languages via tree-sitter:\n- Python (via built-in ast module or tree-sitter)\n- JavaScript/TypeScript (via tree-sitter)\n- Go (via tree-sitter)\n\nConfiguration:\n    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    ACE_AST_MAX_LINES: Maximum lines per chunk (def",
        "\"\"\"ACE Code Analysis module (Phase 2A: Tree-sitter Integration).\n\nThis module provides AST-based code understanding for code-specific queries\nusing tree-sitter parsing. Supports Python, TypeScript, JavaScript, and Go.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport tree_sitter_go as tsgo\nimport tree_sitter_javascript as tsjs\nimport tree_sitter_python as tspython\nimport tree_sitter_typescript as tstype\nfrom tree_sitter import Langua",
        "\"\"\"Dependency graph analysis for code understanding.\n\nExtracts imports, function calls, and dependency relationships from source code\nusing tree-sitter for multiple programming languages.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nimport re\n\n\n@dataclass\nclass Import:\n    \"\"\"Represents an import statement in source code.\"\"\"\n\n    module: str\n    names: List[str] = field(default_factory=list)\n    alias: Optional[str] = None\n    lin",
        "  },\n  {\n    \"query\": \"verify ast parsers against language specs before static\",\n    \"memory\": \"Validate AST parsers against language specs before static analysis\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"verify ast parsers against language specs before static\",\n    \"memory\": \"Verify external dependencies before storing their output\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"verify ast parsers against language specs before static\",\n    \"memory\": \"\ud83d\udd34 [FRUSTRATION] Never hardcode configuration values; a"
      ],
      "ace_line_counts": [
        95,
        553,
        156,
        29,
        320
      ],
      "auggie_files": [
        "ace/code_analysis.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"ACE Code Analysis module (Phase 2A: Tree-sitter Integration).\n     2\t\n     3\tThis module provides AST-based code understanding for code-specific queries\n     4\tusing tree-sitter parsing. Supports Python, TypeScript, JavaScript, and Go.\n     5\t\"\"\"\n     6\t\n     7\tfrom dataclasses import dataclass, field\n     8\tfrom pathlib import Path\n     9\tfrom typing import List, Optional\n    10\t\n    11\timport tree_sitter_go as tsgo\n    12\timport tree_sitter_javascript as tsjs\n    13\timport tree_sitter_python as tspython\n... (453 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "def store memory method",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/unified_memory.py"
      ],
      "ace_files": [
        "ace/unified_memory.py",
        "ace/qdrant_retrieval.py",
        "ace/unified_memory.py",
        "ace/unified_memory.py",
        "ace_mcp_server.py"
      ],
      "ace_scores": [
        0.9275331600000001,
        0.66783953,
        0.64892317,
        0.64759992,
        0.54684865
      ],
      "ace_contents": [
        "    def index_bullet(\n        self,\n        bullet: UnifiedBullet,\n        dedup_threshold: float = 0.92,\n        enable_dedup: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Index a single bullet with deduplication and conflict detection support.\n\n        Checks for semantically similar existing memories before inserting.\n        If a match is found above threshold, reinforces the existing memory\n        instead of creating a duplicate. Also detects potential conflicts.\n\n        Args:",
        "    def index_bullet(self, bullet: \"Bullet\") -> None:\n        \"\"\"Index a single bullet to Qdrant.\n\n        Creates both dense and sparse vectors for hybrid search.\n\n        Args:\n            bullet: Bullet to index (Bullet or EnrichedBullet)\n        \"\"\"\n        from .playbook import EnrichedBullet\n\n        self._ensure_collection()\n\n        # Generate embedding text\n        embedding_text = self._bullet_to_embedding_text(bullet)\n\n        # Get dense embedding\n        dense_vector = self._get_emb",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "class UnifiedBullet:\n    \"\"\"\n    Unified bullet combining ACE Playbook and Personal Memory schemas.\n\n    This schema is a superset of both systems, supporting:\n    - ACE scoring (helpful_count, harmful_count)\n    - Personal memory scoring (severity, reinforcement_count)\n    - Semantic scaffolding (trigger_patterns, task_types, etc.)\n    - Namespace separation for organization\n\n    Attributes:\n        # Identity\n        id: Unique identifier\n        namespace: \"user_prefs\" | \"task_strategies\" | \"",
        "async def ace_store(\n    content: str,\n    namespace: str = \"user_prefs\",\n    section: str = \"general\",\n    severity: int = 5,\n    category: str = \"PREFERENCE\",\n) -> str:\n    \"\"\"Store a new memory/lesson in ACE unified memory.\n    \n    Use this tool when the user:\n    - Expresses a preference or directive (\"I prefer...\", \"Always...\", \"Never...\")\n    - Provides a correction or teaches you something\n    - Shares workflow patterns or coding standards\n    - Gives feedback that should be remembered f"
      ],
      "ace_line_counts": [
        286,
        62,
        471,
        112,
        158
      ],
      "auggie_files": [
        "ace_mcp_server.py"
      ],
      "auggie_contents": [
        "...\n   725\t\n   726\t\n   727\t@server.tool()\n   728\tasync def ace_store(\n   729\t    content: str,\n   730\t    namespace: str = \"user_prefs\",\n   731\t    section: str = \"general\",\n   732\t    severity: int = 5,\n   733\t    category: str = \"PREFERENCE\",\n   734\t) -> str:\n   735\t    \"\"\"Store a new memory/lesson in ACE unified memory.\n   736\t\n... (467 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "def retry_with_backoff",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "ace/scaling.py",
        "ace/retrieval_optimized.py",
        "ace/unified_memory.py",
        "ace/roles.py"
      ],
      "ace_scores": [
        0.4995288,
        0.41808802,
        0.40771693,
        0.39496768,
        0.388654
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "    def retrieve(\n        self,\n        query: str,\n        collection_name: str = \"ace_bullets\",\n        limit: int = 10,\n        enable_failover: bool = True,\n    ) -> List[\"QdrantScoredResult\"]:\n        \"\"\"Retrieve with automatic failover.\n\n        Args:\n            query: Natural language query\n            collection_name: Qdrant collection name\n            limit: Maximum number of results\n            enable_failover: Enable automatic failover on failure\n\n        Returns:\n            List of",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "    def _reflect_impl(\n        self,\n        *,\n        question: str,\n        generator_output: GeneratorOutput,\n        playbook: Playbook,\n        ground_truth: Optional[str],\n        feedback: Optional[str],\n        max_refinement_rounds: int = 1,\n        **kwargs: Any,\n    ) -> ReflectorOutput:\n        playbook_excerpt = _make_playbook_excerpt(playbook, generator_output.bullet_ids)\n\n        # Format playbook section based on citation presence\n        if playbook_excerpt:\n            playboo"
      ],
      "ace_line_counts": [
        348,
        101,
        91,
        471,
        86
      ],
      "auggie_files": [
        "ace/resilience.py"
      ],
      "auggie_contents": [
        "...\n    48\t\n    49\t    Example:\n    50\t        >>> breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n    51\t        >>>\n    52\t        >>> @breaker\n    53\t        >>> def call_llm(prompt):\n    54\t        ...     return llm_client.complete(prompt)\n    55\t        >>>\n    56\t        >>> try:\n    57\t        ...     result = call_llm(\"Hello\")\n    58\t        >>> except CircuitOpenError:\n    59\t        ...     # Circuit is open, use fallback\n... (538 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "def handle_exception",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/security.py",
        "debug_docs.py",
        "ace/typo_correction.py",
        "ace/resilience.py",
        "ace/audit.py"
      ],
      "ace_scores": [
        0.4364276,
        0.41709277,
        0.41685337,
        0.4100349,
        0.3952339
      ],
      "ace_contents": [
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes\nclass AuthenticationError(Exception):\n    \"\"\"Raised when authenticat",
        "import os\nimport re\n\n# Simulate the actual function\nfile_path = \"docs/INTEGRATION_GUIDE.md\"\nquery = \"try except error handling pattern\"\n\nstop_words = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'how', 'what',\n              'where', 'when', 'why', 'can', 'will', 'method', 'function', 'class',\n              'code', 'file', 'def', 'implementation', 'search', 'find', 'get', 'set',\n              'pattern', 'error', 'handling', 'import', 'logging', 'logger', 'setup',\n              'exception",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt"
      ],
      "ace_line_counts": [
        89,
        49,
        652,
        149,
        37
      ],
      "auggie_files": [
        "ace/resilience.py"
      ],
      "auggie_contents": [
        "...\n    48\t\n    49\t    Example:\n    50\t        >>> breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n    51\t        >>>\n    52\t        >>> @breaker\n    53\t        >>> def call_llm(prompt):\n    54\t        ...     return llm_client.complete(prompt)\n    55\t        >>>\n    56\t        >>> try:\n    57\t        ...     result = call_llm(\"Hello\")\n    58\t        >>> except CircuitOpenError:\n    59\t        ...     # Circuit is open, use fallback\n... (564 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 4)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 4,
      "auggie_expected_rank": 1
    },
    {
      "query": "def log_error method",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/audit.py"
      ],
      "ace_files": [
        "ace/audit.py",
        "ace/observability/opik_integration.py",
        "ace/pattern_detector.py",
        "ace/integrations/langchain.py",
        "auggie_query.json"
      ],
      "ace_scores": [
        0.4187629,
        0.39314526,
        0.36884445,
        0.35825032,
        0.35712442
      ],
      "ace_contents": [
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "    def log_role_performance(\n        self,\n        role_name: str,\n        execution_time: float,\n        success: bool,\n        input_data: Optional[Dict[str, Any]] = None,\n        output_data: Optional[Dict[str, Any]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Log ACE role performance metrics.\"\"\"\n        if not self.enabled:\n            return\n\n        try:\n            opik_context.update_current_trace(\n                feedback_scores=[\n              ",
        "    def register_pattern(\n        self,\n        pattern_id: str,\n        regex: str,\n        fix_template: str,\n        severity: str = \"medium\"\n    ) -> None:\n        \"\"\"\n        Register a new pattern for detection.\n        \n        Args:\n            pattern_id: Unique identifier for the pattern\n            regex: Regular expression to match errors\n            fix_template: Suggested fix template\n            severity: Pattern severity (low, medium, high)\n        \"\"\"\n        self._patterns[patt",
        "    def _learn(self, original_input: Any, result: Any):\n        \"\"\"\n        Learn from successful execution.\n\n        Args:\n            original_input: Original input to runnable\n            result: Output from runnable\n        \"\"\"\n        try:\n            # Parse output to string\n            output_str = self.output_parser(result)\n\n            # Build task description\n            if isinstance(original_input, str):\n                task = original_input\n            elif isinstance(original_input",
        "{\"jsonrpc\":\"2.0\",\"method\":\"tools/call\",\"params\":{\"name\":\"codebase-retrieval\",\"arguments\":{\"information_request\":\"CodeRetrieval class search method\"}},\"id\":2}\n"
      ],
      "ace_line_counts": [
        235,
        114,
        101,
        115,
        2
      ],
      "auggie_files": [
        "rag_training/optimizations/v8_bm25_hybrid.py"
      ],
      "auggie_contents": [
        "...\n    90\tlogger = logging.getLogger(__name__)\n    91\t\n    92\t\n    93\t# ============================================================================\n    94\t# METRICS\n    95\t# ============================================================================\n    96\t\n    97\tdef calculate_recall_at_k(retrieved_ids: List[str], ground_truth_id: str, k: int) -> float:\n    98\t    \"\"\"Calculate Recall@K metric.\"\"\"\n    99\t    return 1.0 if ground_truth_id in retrieved_ids[:k] else 0.0\n   100\t\n   101\t\n... (528 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "def normalize_path",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace_vs_auggie_headtohead.py",
        "tests/compare_ace_auggie_dynamic.py",
        "benchmark_ace_vs_auggie.py",
        "ace/query_preprocessor.py",
        "ace/code_retrieval.py"
      ],
      "ace_scores": [
        1.19439903,
        0.9965578399999999,
        0.9791107,
        0.43073732,
        0.39883113
      ],
      "ace_contents": [
        "def normalize_path(path: str) -> str:\n    \"\"\"Normalize path for comparison.\"\"\"\n    if not path:\n        return \"\"\n    # Convert to lowercase, replace backslashes\n    path = path.replace(\"\\\\\", \"/\").lower()\n    # Extract just the filename for comparison\n    return os.path.basename(path)\n\n\ndef paths_match(path1: str, path2: str) -> bool:\n    \"\"\"Check if two paths refer to the same file.\"\"\"\n    if not path1 or not path2:\n        return False\n    n1 = normalize_path(path1)\n    n2 = normalize_path(pat",
        "#!/usr/bin/env python3\n\"\"\"\nDYNAMIC ACE vs Auggie MCP Comparison Test\n\nThis test queries Auggie MCP to get GROUND TRUTH, then verifies ACE matches.\nNO hardcoded expectations - Auggie's results ARE the expected results.\n\nThe test flow:\n1. For each query: Call Auggie MCP via subprocess/script to get actual results\n2. Record the TOP file Auggie returns (the ground truth)\n3. Query ACE with the same query\n4. Verify ACE's top result matches Auggie's top result\n\nThis is the ONLY valid way to test - Augg",
        "\"\"\"Expanded ACE vs Auggie benchmark comparison.\n\nTests diverse query types across 5 categories:\n1. Code queries (functions, classes, patterns)\n2. Doc queries (guides, references, tutorials)\n3. Architecture queries (design, components, patterns)\n4. Config queries (settings, environment, credentials)\n5. Edge cases (specific symbols, error messages, imports)\n\nRuns ACE CodeRetrieval and optionally compares against Auggie MCP.\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nimport argparse\nfrom typing im",
        "\"\"\"\nQuery preprocessing module for ACE framework.\n\nProvides text normalization, non-query detection, conversational wrapper removal,\nand typo correction.\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import List\n\nfrom .typo_correction import TypoCorrector\n\n\n@dataclass\nclass PreprocessResult:\n    \"\"\"Result of query preprocessing.\"\"\"\n    cleaned_query: str\n    is_valid_query: bool\n    original_query: str\n    transformations_applied: List[str] = field(default_factory=list)\n...",
        "    def _apply_filename_boost(self, query: str, file_path: str, score: float, content: str = \"\") -> float:\n        \"\"\"\n        Apply filename and content boost when query terms match file path or definitions.\n        \n        This mimics Auggie MCP's behavior where files with names matching\n        query terms OR containing class/function definitions get prioritized.\n        \n        Args:\n            query: Original search query\n            file_path: File path being scored\n            score: O"
      ],
      "ace_line_counts": [
        104,
        180,
        132,
        131,
        530
      ],
      "auggie_files": [
        "benchmark_ace_vs_auggie.py"
      ],
      "auggie_contents": [
        "...\n   125\t\n   126\t\n   127\tdef normalize_path(path: str) -> str:\n   128\t    \"\"\"Normalize path for comparison.\"\"\"\n   129\t    path = path.replace(\"\\\\\", \"/\").lower()\n   130\t    # Remove leading ./ or absolute paths\n   131\t    if \"/\" in path:\n   132\t        # Keep only the relative part\n   133\t        parts = path.split(\"/\")\n   134\t        # Find first significant directory (ace, docs, tests, etc.)\n   135\t        for i, part in enumerate(parts):\n   136\t            if part in (\"ace\", \"docs\", \"tests\", \"examples\", \"scripts\"):\n... (502 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 5, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 5"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 5,
      "auggie_expected_rank": -1
    },
    {
      "query": "def extract_symbols",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_analysis.py"
      ],
      "ace_files": [
        "ace/code_analysis.py",
        "ace/code_enrichment.py",
        "ace/code_chunker.py",
        "ace/code_enrichment.py",
        "debug_docs.py"
      ],
      "ace_scores": [
        1.05698404,
        0.5250234,
        0.47086397,
        0.4593444,
        0.44205633
      ],
      "ace_contents": [
        "\"\"\"ACE Code Analysis module (Phase 2A: Tree-sitter Integration).\n\nThis module provides AST-based code understanding for code-specific queries\nusing tree-sitter parsing. Supports Python, TypeScript, JavaScript, and Go.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport tree_sitter_go as tsgo\nimport tree_sitter_javascript as tsjs\nimport tree_sitter_python as tspython\nimport tree_sitter_typescript as tstype\nfrom tree_sitter import Langua",
        "    def generate_code_triggers(self, code: str, language: str) -> List[str]:\n        \"\"\"Generate trigger patterns from code symbols.\n\n        Extracts:\n        - Function/method names\n        - Class names\n        - Interface names\n        - Key programming terms\n\n        Args:\n            code: Source code text\n            language: Programming language\n\n        Returns:\n            List of trigger pattern strings (deduplicated).\n        \"\"\"\n        triggers = []\n\n        # Language-specific pa",
        "    def _extract_definitions_treesitter(self, root_node: Any, language: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract top-level definitions from a tree-sitter parse tree.\n        \n        Args:\n            root_node: The root node of the tree-sitter parse tree\n            language: The programming language\n        \n        Returns:\n            List of definition dictionaries with name, start, end, type keys\n        \"\"\"\n        definitions = []\n        \n        # Node types to extract per lang",
        "\"\"\"Code-aware bullet enrichment for ACE Phase 2B.\n\nThis module provides CodeAwareEnricher which enriches bullets with code-specific\nmetadata extracted from code context:\n\n- Symbol extraction (functions, classes, methods)\n- Auto-generation of trigger patterns from code symbols\n- Programming language detection\n- Code snippet extraction from markdown-style content\n- Optimized embedding text for code search\n\nIntegration with code_analysis.py (when available) is done via lazy import\nto avoid circular",
        "import os\nimport re\n\n# Simulate the actual function\nfile_path = \"docs/INTEGRATION_GUIDE.md\"\nquery = \"try except error handling pattern\"\n\nstop_words = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'how', 'what',\n              'where', 'when', 'why', 'can', 'will', 'method', 'function', 'class',\n              'code', 'file', 'def', 'implementation', 'search', 'find', 'get', 'set',\n              'pattern', 'error', 'handling', 'import', 'logging', 'logger', 'setup',\n              'exception"
      ],
      "ace_line_counts": [
        554,
        99,
        118,
        69,
        49
      ],
      "auggie_files": [
        "ace/code_analysis.py"
      ],
      "auggie_contents": [
        "...\n    16\t\n    17\t\n    18\t@dataclass\n    19\tclass CodeSymbol:\n    20\t    \"\"\"Represents a code symbol (function, class, method, etc.).\"\"\"\n    21\t\n    22\t    name: str\n    23\t    kind: str  # function, class, method, interface, struct\n    24\t    start_line: int\n    25\t    end_line: int\n    26\t    docstring: Optional[str] = None\n    27\t    parameters: Optional[List[str]] = field(default_factory=list)\n... (443 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (1.057)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "def chunk_code",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_chunker.py"
      ],
      "ace_files": [
        "ace/code_chunker.py",
        "ace/code_chunker.py",
        "ace/code_indexer.py",
        "ace/code_indexer.py",
        "ace/code_retrieval.py"
      ],
      "ace_scores": [
        0.9115401,
        0.55184364,
        0.49437246,
        0.4779946,
        0.47259676
      ],
      "ace_contents": [
        "def chunk_code(content: str, language: str = \"python\") -> List[CodeChunk]:\n    \"\"\"Chunk code using the default ASTChunker instance.\n    \n    Args:\n        content: Source code to chunk\n        language: Programming language\n    \n    Returns:\n        List of CodeChunk objects\n    \"\"\"\n    return ASTChunker().chunk(content, language)",
        "\"\"\"AST-based semantic code chunking module.\n\nThis module provides intelligent code chunking that respects language syntax\nboundaries (functions, classes, methods) rather than arbitrary line counts.\n\nSupports multiple languages via tree-sitter:\n- Python (via built-in ast module or tree-sitter)\n- JavaScript/TypeScript (via tree-sitter)\n- Go (via tree-sitter)\n\nConfiguration:\n    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    ACE_AST_MAX_LINES: Maximum lines per chunk (def",
        "    def chunk_file(self, file_path: str) -> List[CodeChunkIndexed]:\n        \"\"\"\n        Parse and chunk a code file.\n        \n        Args:\n            file_path: Absolute path to code file\n            \n        Returns:\n            List of CodeChunkIndexed instances\n        \"\"\"\n        chunks = []\n        \n        # Check file exists\n        if not os.path.exists(file_path):\n            logger.warning(f\"File not found: {file_path}\")\n            return chunks\n        \n        # Read file content\n",
        "\"\"\"Code indexer module for workspace code indexing.\n\nThis module provides code indexing capabilities that scan a workspace,\nparse code files using ASTChunker, and store indexed chunks in Qdrant\nfor semantic code search.\n\nConfiguration:\n    ACE_CODE_COLLECTION: Qdrant collection name (default: ace_code_context)\n    ACE_CODE_EMBEDDING_DIM: Embedding dimension (default: from EmbeddingConfig)\n    QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n\nThe indexer supports:\n- Multi-language p",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca"
      ],
      "ace_line_counts": [
        11,
        420,
        86,
        55,
        147
      ],
      "auggie_files": [
        "ace/code_chunker.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"AST-based semantic code chunking module.\n     2\t\n     3\tThis module provides intelligent code chunking that respects language syntax\n     4\tboundaries (functions, classes, methods) rather than arbitrary line counts.\n     5\t\n     6\tSupports multiple languages via tree-sitter:\n     7\t- Python (via built-in ast module or tree-sitter)\n     8\t- JavaScript/TypeScript (via tree-sitter)\n     9\t- Go (via tree-sitter)\n    10\t\n    11\tConfiguration:\n    12\t    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    13\t    ACE_AST_MAX_LINES: Maximum lines per chunk (default: 120)\n... (418 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (0.912)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "def expand_query",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/retrieval_presets.py",
        "ace/structured_enhancer.py",
        "ace/retrieval_presets.py",
        "rag_training/optimizations/v3_bge_reranker.py",
        "ace/retrieval_optimized.py"
      ],
      "ace_scores": [
        1.0824422,
        1.0419679,
        0.8105764,
        0.60114205,
        0.57255936
      ],
      "ace_contents": [
        "def filter_by_adaptive_threshold(\n    results_with_scores: List[Tuple[any, float]],\n    percentile: int = 70,\n    use_gap_detection: bool = True,\n    min_keep: int = 3,\n) -> List[Tuple[any, float]]:\n    \"\"\"\n    Filter results using adaptive threshold computed from score distribution.\n\n    Args:\n        results_with_scores: List of (result, score) tuples\n        percentile: Percentile threshold for filtering\n        use_gap_detection: Enable score gap detection\n        min_keep: Minimum results t",
        "class StructuredQueryEnhancer:\n    \"\"\"Enhance queries using .enhancedprompt.md methodology.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the enhancer.\"\"\"\n        pass\n    \n    def classify_intent(self, query: str) -> QueryIntent:\n        \"\"\"Classify the query intent.\"\"\"\n        query_lower = query.lower()\n        \n        # Check each intent pattern\n        intent_scores = {}\n        for intent, patterns in INTENT_PATTERNS.items():\n            score = sum(1 for p in patterns if re.searc",
        "def expand_query_with_llm(\n    query: str,\n    llm_url: Optional[str] = None,\n    model: Optional[str] = None,\n    timeout: Optional[float] = None,\n) -> List[str]:\n    \"\"\"\n    Use LLM (GLM 4.6) to semantically expand query for better retrieval.\n\n    This generates related terms and rephrased queries that capture\n    the semantic intent beyond simple synonym matching.\n\n    All settings default to values from ace.config.LLMConfig.\n\n    Args:\n        query: Original query string\n        llm_url: Z.",
        "class QueryExpander:\n    \"\"\"Rule-based query expansion.\"\"\"\n\n    def __init__(self):\n        self.synonyms = SYNONYMS\n\n    def expand_query(self, query: str, num_expansions: int = 3) -> List[str]:\n        \"\"\"Expand a query into multiple variations.\"\"\"\n        expansions = [query]\n\n        synonym_version = self._expand_synonyms(query)\n        if synonym_version != query:\n            expansions.append(synonym_version)\n\n        question_version = self._reformulate_question(query)\n        if questio",
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============"
      ],
      "ace_line_counts": [
        104,
        106,
        116,
        107,
        424
      ],
      "auggie_files": [
        "ace/query_enhancer.py"
      ],
      "auggie_contents": [
        "...\n    96\t\n    97\t\n    98\tdef expand_vague_terms(query: str) -> Tuple[str, List[str]]:\n    99\t    \"\"\"Expand vague terms into domain-specific keywords.\n   100\t    \n   101\t    Args:\n   102\t        query: The user's query string.\n   103\t        \n   104\t    Returns:\n   105\t        Tuple of (expanded query, list of added terms).\n   106\t    \"\"\"\n   107\t    query_lower = query.lower()\n... (490 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "High confidence top score (1.082)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "async def batch_embed",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/gemini_embeddings.py",
        "ace/async_retrieval.py",
        "ace/openai_embeddings.py",
        "ace/code_indexer.py",
        "ace/hyde.py"
      ],
      "ace_scores": [
        0.62710917,
        0.6261844,
        0.62120205,
        0.5899819,
        0.5768598
      ],
      "ace_contents": [
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "    def _embed_batch(self, texts: List[str], batch_size: int = 128, max_tokens_per_batch: int = 100000) -> List[List[float]]:\n        \"\"\"Batch embed texts using Voyage API with token-aware splitting.\n        \n        Voyage API has two limits:\n        1. Max 128 texts per request\n        2. Max ~120k tokens per request\n        \n        This function respects both limits by dynamically sizing batches.\n        \n        Args:\n            texts: List of texts to embed\n            batch_size: Max tex",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp"
      ],
      "ace_line_counts": [
        280,
        565,
        211,
        103,
        58
      ],
      "auggie_files": [
        "ace/gemini_embeddings.py"
      ],
      "auggie_contents": [
        "...\n    83\t\n    84\t        # Endpoints\n    85\t        self.embed_url = f\"{GEMINI_API_BASE}/models/{model}:embedContent\"\n    86\t        self.batch_url = f\"{GEMINI_API_BASE}/models/{model}:batchEmbedContents\"\n    87\t\n    88\t        logger.info(f\"Gemini embedding client initialized: {model}, dim={dimension}\")\n    89\t\n    90\t    def _embed_single(\n    91\t        self,\n    92\t        text: str,\n    93\t        task_type: TaskType = \"SEMANTIC_SIMILARITY\",\n    94\t    ) -> List[float]:\n... (499 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "async def stream_results",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/llm_providers/langchain_client.py",
        "ace/async_retrieval.py",
        "ace/code_retrieval.py",
        "ace/retrieval_optimized.py"
      ],
      "ace_scores": [
        0.5395371,
        0.5290283,
        0.5020777,
        0.48804787,
        0.48096818
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "    async def acomplete_with_stream(self, prompt: str, **kwargs) -> AsyncIterator[str]:\n        \"\"\"\n        Asynchronously complete a prompt with streaming using the LangChain LiteLLM client.\n\n        Args:\n            prompt: The prompt to complete\n            **kwargs: Additional parameters for the completion\n\n        Yields:\n            Tokens as they are generated\n        \"\"\"\n        filtered_kwargs = self._filter_kwargs(kwargs)\n\n        try:\n            async for chunk in self.llm.astream(p",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============"
      ],
      "ace_line_counts": [
        364,
        20,
        269,
        37,
        87
      ],
      "auggie_files": [
        "ace/llm_providers/langchain_client.py"
      ],
      "auggie_contents": [
        "...\n   183\t\n   184\t            return LLMResponse(text=response.content, raw=metadata)\n   185\t\n   186\t        except Exception as e:\n   187\t            logger.error(f\"Error in async LangChain completion: {e}\")\n   188\t            raise\n   189\t\n   190\t    def complete_with_stream(self, prompt: str, **kwargs) -> Iterator[str]:\n   191\t        \"\"\"\n   192\t        Complete a prompt with streaming using the LangChain LiteLLM client.\n   193\t\n   194\t        Args:\n... (527 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 3, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 3"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 3,
      "auggie_expected_rank": -1
    },
    {
      "query": "asyncio gather parallel execution",
      "category": "FunctionPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "compare_ace_auggie_headtohead.py",
        "rag_training/optimizations/v6_hyde.py",
        "ace/hyde.py"
      ],
      "ace_scores": [
        0.56103903,
        0.5371449,
        0.49309164,
        0.46867335,
        0.45836085
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Head-to-head ACE vs Auggie comparison test.\n\nFor each query, we call both systems and compare:\n1. Top file match\n2. Top 3 files overlap\n3. Content relevance\n\"\"\"\nimport subprocess\nimport sys\nimport json\nfrom typing import Dict, List, Any\n\n# Test queries - comprehensive coverage\nTEST_QUERIES = [\n    # Core ACE classes\n    \"EmbeddingConfig class definition dataclass\",\n    \"UnifiedMemoryIndex class Qdrant namespace hybrid search\",\n    \"CodeRetrieval class search method dense vector\",\n    \"ASTChun",
        "\"\"\"\nV6: HyDE (Hypothetical Document Embeddings) Optimization\n=========================================================\n\n**Technique**: HyDE - Generate hypothetical documents that would answer the query,\nthen use averaged embeddings for semantic search.\n\n**Pipeline**:\n1. Query -> LLM generates 3-5 hypothetical answer documents\n2. Embed each hypothetical document\n3. Average embeddings into single vector\n4. Search Qdrant with averaged embedding + BM25 sparse (RRF fusion)\n\n**Expected Improvement**: ",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp"
      ],
      "ace_line_counts": [
        364,
        155,
        94,
        107,
        58
      ],
      "auggie_files": [
        "ace/async_retrieval.py",
        "ace/hyde.py"
      ],
      "auggie_contents": [
        "...\n   188\t\n   189\t    async def batch_get_embeddings(self, texts: List[str]) -> List[List[float]]:\n   190\t        \"\"\"Retrieve embeddings for multiple texts in parallel.\n   191\t\n   192\t        Uses asyncio.gather for concurrent execution.\n   193\t\n   194\t        Args:\n   195\t            texts: List of texts to embed\n   196\t\n   197\t        Returns:\n   198\t            List of embedding vectors (same order as input).\n   199\t\n... (491 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"asyncio.gather usage, parallel async operations, concurrent LLM calls\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   221\t\n   222\t        # Generate hypotheticals\n   223\t        logger.info(f\"Generating {num_docs} hypotheticals for query: {query[:50]}...\")\n   224\t        hypotheticals = []\n   225\t\n   226\t        for i in range(num_docs):\n   227\t            try:\n   228\t                hypothetical = self._generate_single_hypothetical(query)\n   229\t                hypotheticals.append(hypothetical)\n   230\t                logger.debug(f\"Generated hypothetical {i+1}/{num_docs}\")\n   231\t            except Exception as e:\n   232\t                logger.warning(f\"Failed to generate hypothetical {i+1}/{num_docs}: {e}\")\n... (473 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 1",
      "ace_advantages": [
        "More unique files (3 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "environment variables dotenv loading",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/config.py",
        "verify_setup.py",
        "ace/context_injector.py",
        "rag_training/training_data/crossencoder_training_pairs.json"
      ],
      "ace_scores": [
        0.7613620699999999,
        0.6064366299999999,
        0.59966206,
        0.57813906,
        0.5113424
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "class MultiStageConfig:\n    \"\"\"\n    Multi-stage retrieval configuration (coarse-to-fine optimization).\n\n    Implements a 4-stage retrieval pipeline:\n    1. Stage 1 (Coarse): High-recall candidate retrieval (10x limit)\n    2. Stage 2 (Filter): Score-based filtering (DISABLED by default - RRF scores unreliable)\n    3. Stage 3 (Rerank): Cross-encoder reranking on all Stage 1 candidates\n    4. Stage 4 (Final): Deduplication and final selection\n\n    Benefits:\n    - Higher recall by fetching more cand",
        "def main():\n    print(\"\ud83d\ude80 ACE Framework Development Environment Verification\")\n    print(\"=\" * 60)\n\n    # Check Python version\n    python_version = sys.version_info\n    print(f\"\\n\ud83d\udccb Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n    if python_version < (3, 11):\n        print(\"\u274c Python 3.11+ required\")\n        return False\n    else:\n        print(\"\u2705 Python version OK\")\n\n    # Check if UV is available\n    has_uv = run_command(\"uv --version\", \"Checking UV insta",
        "\"\"\"Context injection module for enriching prompts with relevant memories.\n\nThis module provides automatic context injection that retrieves relevant\nmemories/knowledge from ACE and prepends them to prompts.\n\nConfiguration:\n    ACE_ENABLE_CONTEXT_INJECTION: Enable/disable context injection (default: false)\n    ACE_CONTEXT_MAX_ITEMS: Maximum context items to inject (default: 5)\n    ACE_CONTEXT_FORMAT: Output format - \"plain\" or \"markdown\" (default: plain)\n\nWhen disabled, returns the original prompt",
        "  },\n  {\n    \"query\": \"how to structure env\",\n    \"memory\": \"Extract env checks and standardize health endpoints.\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"how to structure env\",\n    \"memory\": \"Define model structure upfront to accelerate development.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"how to structure env\",\n    \"memory\": \"Centralize imports in __init__.py for consistent module access.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"how to structure env\",\n    \"memory\": \"\ud83d\udfe1 [DIRECTIVE] Always document "
      ],
      "ace_line_counts": [
        221,
        113,
        102,
        110,
        520
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized ACE configuration.\n     2\t\n     3\tAll embedding and retrieval settings in one place.\n     4\tOverride via environment variables or .env file.\n     5\t\"\"\"\n     6\t\n     7\timport os\n     8\tfrom dataclasses import dataclass, field\n     9\tfrom typing import Optional\n    10\tfrom pathlib import Path\n    11\t\n    12\t# Load .env if python-dotenv is available\n    13\ttry:\n... (595 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".env\"\n   type: \"file\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        63
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (0.761)"
      ],
      "auggie_advantages": [
        "Better chunk size (63 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "API_KEY environment variable",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/integrations/browser_use.py",
        "ace/integrations/langchain.py",
        "ace/config.py",
        "verify_setup.py",
        "ace/llm_providers/litellm_client.py"
      ],
      "ace_scores": [
        0.83,
        0.83,
        0.65229823,
        0.53635502,
        0.52857215
      ],
      "ace_contents": [
        "\"\"\"\nBrowser-use integration for ACE framework.\n\nThis module provides ACEAgent, a drop-in replacement for browser-use Agent\nthat automatically learns from execution feedback.\n\nThis is the reference implementation for ACE integrations with external agentic\nframeworks. It demonstrates the pattern:\n1. External framework (browser-use) executes task\n2. ACE injects playbook context beforehand\n3. ACE learns from execution afterward (Reflector + Curator)\n\nExample:\n    from ace.integrations import ACEAgen",
        "\"\"\"\nACE + LangChain integration for learning from chain/agent execution.\n\nThis module provides ACELangChain, a wrapper that adds ACE learning capabilities\nto any LangChain Runnable (chains, agents, custom runnables).\n\nWhen to Use ACELangChain:\n- Complex workflows: Multi-step LangChain chains\n- Tool-using agents: LangChain agents with tools\n- Custom runnables: Your own LangChain components\n- Production workflows: LangChain orchestration with learning\n\nWhen NOT to Use ACELangChain:\n- Simple Q&A \u2192 ",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "def main():\n    print(\"\ud83d\ude80 ACE Framework Development Environment Verification\")\n    print(\"=\" * 60)\n\n    # Check Python version\n    python_version = sys.version_info\n    print(f\"\\n\ud83d\udccb Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n    if python_version < (3, 11):\n        print(\"\u274c Python 3.11+ required\")\n        return False\n    else:\n        print(\"\u2705 Python version OK\")\n\n    # Check if UV is available\n    has_uv = run_command(\"uv --version\", \"Checking UV insta",
        "    def _normalize_model_for_custom_endpoint(\n        self, model: str, api_base: Optional[str]\n    ) -> str:\n        \"\"\"\n        Normalize model name for custom OpenAI-compatible endpoints.\n\n        LiteLLM requires provider prefix for model routing. When using custom\n        endpoints (api_base is set) with models that don't have a recognized\n        provider prefix, we assume OpenAI-compatible API and add 'openai/' prefix.\n\n        This allows config to use simple model names like 'glm-4.6' w"
      ],
      "ace_line_counts": [
        564,
        501,
        117,
        102,
        113
      ],
      "auggie_files": [
        ".env.example"
      ],
      "auggie_contents": [
        "     1\t# API Keys for LLM Providers\n     2\t# Copy this file to .env and add your actual API keys\n     3\t\n     4\t# Z.ai GLM (DEFAULT - ACE uses this by default)\n     5\tZAI_API_KEY=your-zai-api-key-here\n     6\t\n     7\t# OpenAI\n     8\tOPENAI_API_KEY=your-openai-api-key-here\n     9\t\n    10\t# Anthropic (Claude)\n    11\tANTHROPIC_API_KEY=your-anthropic-api-key-here\n    12\t\n    13\t# Google (Gemini)\n... (609 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".env\"\n   type: \"file\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        63
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 3, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 3"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 3,
      "auggie_expected_rank": -1
    },
    {
      "query": "QDRANT_URL connection string",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/scaling.py",
        "ace/hyde_retrieval.py",
        "ace/deduplication.py",
        "ace/scaling.py",
        "ace/observability/health.py"
      ],
      "ace_scores": [
        0.6403705,
        0.6223959,
        0.6196048,
        0.5983949,
        0.5859909
      ],
      "ace_contents": [
        "class QdrantCluster:\n    \"\"\"Manage multiple Qdrant nodes with load balancing and failover.\n\n    Provides high-availability Qdrant access with:\n    - Load balancing strategies (round-robin, least-connections, weighted)\n    - Automatic failover on node failure\n    - Health monitoring\n    - Connection pooling\n\n    Example:\n        >>> cluster = QdrantCluster(\n        ...     nodes=[\"http://node1:6333\", \"http://node2:6333\"],\n        ...     strategy=LoadBalancingStrategy.ROUND_ROBIN\n        ...  )\n ",
        "\"\"\"HyDE-enhanced retrieval pipeline for ACE memory system.\n\nIntegrates HyDE (Hypothetical Document Embeddings) with existing hybrid search\ninfrastructure for improved retrieval accuracy on ambiguous/implicit queries.\n\nPipeline:\n1. Query -> HyDE expansion -> Generate hypothetical documents\n2. Embed hypotheticals -> Average embeddings\n3. Search Qdrant with averaged embedding + BM25 sparse\n4. Return results with hybrid RRF fusion\n\nPerformance target: +5-10% for implicit/scenario/template queries\n\"\"",
        "\"\"\"\nAdvanced Memory Deduplication System for RAG.\n\nThis module provides clustering-based deduplication for Qdrant collections:\n- HDBSCAN/DBSCAN clustering for efficient duplicate detection (O(n log n) vs O(n^2))\n- Multi-collection support (ace_memories_hybrid, ace_unified)\n- Multiple merge strategies (keep_best, merge_content, canonical_form)\n- Cluster quality metrics (silhouette score, Davies-Bouldin index)\n- Dry-run mode for safe preview\n\nArchitecture:\n    1. Load memories with embeddings from",
        "\"\"\"Horizontal scaling for ACE Framework with sharded collections and clustering.\n\nPhase 4C: Horizontal Scaling Implementation\n\nThis module provides:\n- ShardedBulletIndex: Multi-tenant bullet storage with tenant/domain/hybrid sharding\n- QdrantCluster: Load-balanced Qdrant cluster with automatic failover\n- ClusterHealthCheck: Cluster health monitoring and metrics\n\nKey features:\n- Tenant isolation via sharded collections\n- Domain-specific bullet stores\n- Load balancing: round-robin, least-connectio",
        "\"\"\"\nHealth Check System for ACE Framework (Phase 3D)\n\nThis module provides health checks for external dependencies (Qdrant, LM Studio).\nTracks component availability, latency, and error states.\n\"\"\"\n\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict, Optional\n\nimport httpx\n\n"
      ],
      "ace_line_counts": [
        108,
        473,
        104,
        153,
        15
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized ACE configuration.\n     2\t\n     3\tAll embedding and retrieval settings in one place.\n     4\tOverride via environment variables or .env file.\n     5\t\"\"\"\n     6\t\n     7\timport os\n     8\tfrom dataclasses import dataclass, field\n     9\tfrom typing import Optional\n    10\tfrom pathlib import Path\n    11\t\n    12\t# Load .env if python-dotenv is available\n    13\ttry:\n... (525 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie found expected file at rank 1, ACE missed it",
      "ace_advantages": [],
      "auggie_advantages": [
        "Found expected file at rank 1"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": true,
      "ace_expected_rank": -1,
      "auggie_expected_rank": 1
    },
    {
      "query": "VOYAGE_API_KEY config",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/security.py",
        "ace/llm_providers/litellm_client.py",
        "ace/code_indexer.py",
        "ace/gemini_embeddings.py"
      ],
      "ace_scores": [
        0.5769848000000001,
        0.41735116,
        0.41164923,
        0.39146662,
        0.38777652
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes\nclass AuthenticationError(Exception):\n    \"\"\"Raised when authenticat",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L",
        "    def _embed_batch(self, texts: List[str], batch_size: int = 128, max_tokens_per_batch: int = 100000) -> List[List[float]]:\n        \"\"\"Batch embed texts using Voyage API with token-aware splitting.\n        \n        Voyage API has two limits:\n        1. Max 128 texts per request\n        2. Max ~120k tokens per request\n        \n        This function respects both limits by dynamically sizing batches.\n        \n        Args:\n            texts: List of texts to embed\n            batch_size: Max tex",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n"
      ],
      "ace_line_counts": [
        221,
        89,
        300,
        103,
        88
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized ACE configuration.\n     2\t\n     3\tAll embedding and retrieval settings in one place.\n     4\tOverride via environment variables or .env file.\n     5\t\"\"\"\n     6\t\n     7\timport os\n     8\tfrom dataclasses import dataclass, field\n     9\tfrom typing import Optional\n    10\tfrom pathlib import Path\n    11\t\n    12\t# Load .env if python-dotenv is available\n    13\ttry:\n... (518 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".env\"\n   type: \"file\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        63
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Better chunk size (63 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "OPENAI_API_KEY config",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/openai_embeddings.py",
        "ace/config.py",
        "ace/llm_providers/litellm_client.py",
        "ace/retrieval_optimized.py",
        "ace/security.py"
      ],
      "ace_scores": [
        0.5952436,
        0.5704454,
        0.53244674,
        0.52483237,
        0.51502484
      ],
      "ace_contents": [
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes\nclass AuthenticationError(Exception):\n    \"\"\"Raised when authenticat"
      ],
      "ace_line_counts": [
        113,
        221,
        300,
        91,
        89
      ],
      "auggie_files": [
        ".env.example"
      ],
      "auggie_contents": [
        "     1\t# API Keys for LLM Providers\n     2\t# Copy this file to .env and add your actual API keys\n     3\t\n     4\t# Z.ai GLM (DEFAULT - ACE uses this by default)\n     5\tZAI_API_KEY=your-zai-api-key-here\n     6\t\n     7\t# OpenAI\n     8\tOPENAI_API_KEY=your-openai-api-key-here\n     9\t\n    10\t# Anthropic (Claude)\n    11\tANTHROPIC_API_KEY=your-anthropic-api-key-here\n    12\t\n    13\t# Google (Gemini)\n... (627 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "MODEL_NAME configuration",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/retrieval_presets.py",
        "ace/unified_memory.py",
        "ace/retrieval_optimized.py",
        "ace/gemini_embeddings.py"
      ],
      "ace_scores": [
        0.44225731,
        0.43036047,
        0.42279303,
        0.41687298,
        0.41189098
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"\nRetrieval Presets - Optimized configurations for different query types.\n\nBased on empirical testing:\n- Baseline precision: 75.6%\n- Architecture queries: 33.3% (worst)\n- Target: 95%+ precision across all categories\n\nWinning optimizations:\n1. BM25-heavy weighting (dense=0.3, sparse=0.7) -> +50% P@3\n2. Post-retrieval deduplication (0.90 threshold) -> +2.7%\n3. Query expansion with domain synonyms -> +3% (conditional)\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\"",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n"
      ],
      "ace_line_counts": [
        547,
        58,
        222,
        91,
        88
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "...\n    41\t\n    42\t\n    43\t@dataclass\n    44\tclass EmbeddingConfig:\n    45\t    \"\"\"Embedding model configuration for memory/lessons (general-purpose).\"\"\"\n    46\t\n    47\t    # LM Studio server\n    48\t    url: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_URL\", \"http://192.168.10.64:1234\"))\n    49\t\n    50\t    # Model name (Qwen3-Embedding-8B - proper embedding model, 4096 dims)\n    51\t    model: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_MODEL\", \"text-embedding-qwen3-embedding-8b\"))\n    52\t\n... (454 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".env\"\n   type: \"file\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        63
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Better chunk size (63 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "EMBEDDING_DIMENSION size",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/openai_embeddings.py",
        "ace/gemini_embeddings.py",
        "ace/semantic_scorer.py",
        "ace/qdrant_retrieval.py",
        "ace/async_retrieval.py"
      ],
      "ace_scores": [
        0.5946801,
        0.58597505,
        0.5457678,
        0.5326811,
        0.52714205
      ],
      "ace_contents": [
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "    def _get_embedding(self, text: str) -> List[float]:\n        \"\"\"Get dense embedding from LM Studio with automatic EOS token handling.\n\n        Args:\n            text: Text to embed (truncated to 8000 chars)\n\n        Returns:\n            768-dimensional embedding vector.\n\n        Raises:\n            RuntimeError: If embedding request fails.\n        \"\"\"\n        try:\n            # Add EOS token for Qwen models to fix GGUF tokenizer warning\n            # This ensures proper sentence boundary dete",
        "    async def get_embedding(self, text: str) -> List[float]:\n        \"\"\"Get dense embedding from LM Studio asynchronously.\n\n        Args:\n            text: Text to embed (truncated to 8000 chars)\n\n        Returns:\n            768-dimensional embedding vector.\n\n        Raises:\n            Exception: If embedding request fails.\n        \"\"\"\n        # Import httpx here so mocking works (patch('httpx.AsyncClient'))\n        import httpx\n\n        # Create client if needed (allows mocking to work)\n     "
      ],
      "ace_line_counts": [
        211,
        280,
        104,
        115,
        113
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "...\n    41\t\n    42\t\n    43\t@dataclass\n    44\tclass EmbeddingConfig:\n    45\t    \"\"\"Embedding model configuration for memory/lessons (general-purpose).\"\"\"\n    46\t\n    47\t    # LM Studio server\n    48\t    url: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_URL\", \"http://192.168.10.64:1234\"))\n    49\t\n    50\t    # Model name (Qwen3-Embedding-8B - proper embedding model, 4096 dims)\n    51\t    model: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_MODEL\", \"text-embedding-qwen3-embedding-8b\"))\n    52\t\n... (498 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie found expected file at rank 1, ACE missed it",
      "ace_advantages": [],
      "auggie_advantages": [
        "Found expected file at rank 1"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": true,
      "ace_expected_rank": -1,
      "auggie_expected_rank": 1
    },
    {
      "query": "MAX_TOKENS limit",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/unified_memory.py",
        "ace/self_consistency.py",
        "ace/retrieval_optimized.py",
        "ace/gemini_embeddings.py",
        "ace/typo_correction.py"
      ],
      "ace_scores": [
        0.4823206,
        0.480762,
        0.4779244,
        0.47644007,
        0.47355908
      ],
      "ace_contents": [
        "    class _MatchValue:\n        value: Any\n    MatchValue = _MatchValue\n\n    @dataclass\n    class _MatchAny:\n        any: List[Any]\n    MatchAny = _MatchAny\n\n    @dataclass\n    class _Filter:\n        must: Optional[List[Any]] = None\n        should: Optional[List[Any]] = None\n    Filter = _Filter\n\n\n# =============================================================================\n# NAMESPACE AND SOURCE ENUMS\n# =============================================================================\n\nclass Unifie",
        "\"\"\"Self-consistency sampling for improved generation accuracy.\n\nThis module implements self-consistency decoding, which generates multiple\nresponses for the same prompt and selects the most consistent answer via\nmajority voting. This technique improves accuracy for tasks where reasoning\npaths can vary but the final answer should converge.\n\nReference: Wang et al., \"Self-Consistency Improves Chain of Thought Reasoning\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom collections import Co",
        "def tokenize_bm25(text: str) -> List[str]:\n    \"\"\"\n    Tokenize text for BM25, preserving technical terms.\n    \"\"\"\n    # Split CamelCase\n    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n    # Split snake_case\n    text = text.replace('_', ' ')\n    # Extract tokens\n    tokens = re.findall(r'[a-zA-Z0-9]+', text.lower())\n    # Filter\n    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n    return tokens\n\n\ndef compute_bm25_sparse(\n    text: str,\n    k1: float = 1.5,\n    b: float ",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im"
      ],
      "ace_line_counts": [
        101,
        59,
        81,
        88,
        652
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "BATCH_SIZE setting",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py",
        "ace/code_indexer.py"
      ],
      "ace_files": [
        "ace/retrieval_presets.py",
        "ace/config.py",
        "ace/gemini_embeddings.py",
        "ace/async_retrieval.py",
        "ace/delta.py"
      ],
      "ace_scores": [
        0.44310108,
        0.44041827,
        0.43709162,
        0.4362055,
        0.4359655
      ],
      "ace_contents": [
        "\"\"\"\nRetrieval Presets - Optimized configurations for different query types.\n\nBased on empirical testing:\n- Baseline precision: 75.6%\n- Architecture queries: 33.3% (worst)\n- Target: 95%+ precision across all categories\n\nWinning optimizations:\n1. BM25-heavy weighting (dense=0.3, sparse=0.7) -> +50% P@3\n2. Post-retrieval deduplication (0.90 threshold) -> +2.7%\n3. Query expansion with domain synonyms -> +3% (conditional)\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Delta operations produced by the ACE Curator.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Iterable, List, Literal, Optional, cast\n\n\nOperationType = Literal[\"ADD\", \"UPDATE\", \"TAG\", \"REMOVE\"]\n\n\n@dataclass\nclass DeltaOperation:\n    \"\"\"Single mutation to apply to the playbook.\n\n    Attributes:\n        type: Operation type (ADD, UPDATE, TAG, REMOVE)\n        section: Section name for the bullet\n        content: Bullet content text ("
      ],
      "ace_line_counts": [
        58,
        221,
        88,
        54,
        107
      ],
      "auggie_files": [
        ".env.example"
      ],
      "auggie_contents": [
        "...\n    31\t\n    32\t# Hugging Face (optional)\n    33\tHUGGINGFACE_API_KEY=your-huggingface-api-key-here\n    34\t\n    35\t# Replicate (optional)\n    36\tREPLICATE_API_KEY=your-replicate-api-key-here\n    37\t\n    38\t# Together AI (optional)\n    39\tTOGETHER_API_KEY=your-together-api-key-here\n    40\t\n    41\t# Model Configuration (optional)\n    42\tDEFAULT_MODEL=gpt-4o-mini\n... (573 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "TIMEOUT_SECONDS value",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "ace/gemini_embeddings.py",
        "ace/self_consistency.py",
        "ace/session_tracking.py",
        "ace/llm_providers/litellm_client.py"
      ],
      "ace_scores": [
        0.4117083,
        0.40666497,
        0.3997575,
        0.39259616,
        0.39255118
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"Self-consistency sampling for improved generation accuracy.\n\nThis module implements self-consistency decoding, which generates multiple\nresponses for the same prompt and selects the most consistent answer via\nmajority voting. This technique improves accuracy for tasks where reasoning\npaths can vary but the final answer should converge.\n\nReference: Wang et al., \"Self-Consistency Improves Chain of Thought Reasoning\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom collections import Co",
        "\"\"\"\nSession Outcome Tracking Infrastructure.\n\nTracks bullet effectiveness per session type with TTL-based cleanup.\nPart of Phase 2A: ACE RAG Optimization - Session Outcome Tracker.\n\"\"\"\n\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import Dict\nfrom datetime import datetime, timedelta\n\n\n@dataclass\nclass SessionOutcome:\n    \"\"\"Records outcome statistics for a specific bullet in a session context.\n\n    Attributes:\n        uses: Total number of times this bullet was used\n        w",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L"
      ],
      "ace_line_counts": [
        149,
        88,
        119,
        28,
        65
      ],
      "auggie_files": [
        "examples/browser-use/domain-checker/ace_domain_checker.py"
      ],
      "auggie_contents": [
        "...\n    29\t\n    30\t# Import ACE framework with new integration\n    31\tfrom ace import ACEAgent\n    32\tfrom ace.observability import configure_opik\n    33\tfrom browser_use import ChatBrowserUse\n    34\t\n    35\t# Utility function for timeout calculation\n    36\tdef calculate_timeout_steps(timeout_seconds: float) -> int:\n    37\t    \"\"\"Calculate steps for timeout based on 1 step per 12 seconds.\"\"\"\n    38\t    return int(timeout_seconds // 12)\n...\n   334\t\n... (469 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "Better chunk size (90 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "LOG_LEVEL setting",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py",
        "ace/audit.py"
      ],
      "ace_files": [
        "ace/audit.py",
        "ace/config.py",
        "ace/observability/opik_integration.py",
        "ace/retrieval.py",
        "ace/context_injector.py"
      ],
      "ace_scores": [
        0.4191479,
        0.3974266,
        0.37323803,
        0.3702752,
        0.36887112
      ],
      "ace_contents": [
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"\nOpik Integration for ACE Framework\n\nProvides enterprise-grade observability and tracing for ACE components.\nReplaces custom explainability with production-ready Opik platform.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import asdict\n\nOpikLogger: Optional[type]\n\ntry:\n    import opik\n    from opik import track, opik_context\n\n    OPIK_AVAILABLE = True\n\n    # Try to import LiteLLM Opik",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"Context injection module for enriching prompts with relevant memories.\n\nThis module provides automatic context injection that retrieves relevant\nmemories/knowledge from ACE and prepends them to prompts.\n\nConfiguration:\n    ACE_ENABLE_CONTEXT_INJECTION: Enable/disable context injection (default: false)\n    ACE_CONTEXT_MAX_ITEMS: Maximum context items to inject (default: 5)\n    ACE_CONTEXT_FORMAT: Output format - \"plain\" or \"markdown\" (default: plain)\n\nWhen disabled, returns the original prompt"
      ],
      "ace_line_counts": [
        37,
        117,
        41,
        61,
        25
      ],
      "auggie_files": [
        "docs/API_REFERENCE.md"
      ],
      "auggie_contents": [
        "...\n   896\t\n   897\tmanager = TenantManager(base_path=\"./tenant_data\")\n   898\t\n   899\t# Tenant-scoped operations\n   900\twith TenantContext(tenant_id=\"tenant-a\"):\n   901\t    manager.save_playbook(playbook, \"my_playbook\")\n   902\t    loaded = manager.load_playbook(\"my_playbook\")  # Isolated to tenant-a\n   903\t\n   904\t# Cross-tenant access prevented\n   905\twith TenantContext(tenant_id=\"tenant-b\"):\n   906\t    loaded = manager.load_playbook(\"my_playbook\")  # Different playbook\n   907\t```\n... (509 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".env\"\n   type: \"file\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        63
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "DEBUG_MODE flag",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/context_injector.py",
        "ace/structured_enhancer.py",
        "ace/code_retrieval.py",
        "debug_blended.py",
        "debug_doc_ranking.py"
      ],
      "ace_scores": [
        0.40035713,
        0.3928624,
        0.39135444,
        0.38615343,
        0.38424635
      ],
      "ace_contents": [
        "\"\"\"Context injection module for enriching prompts with relevant memories.\n\nThis module provides automatic context injection that retrieves relevant\nmemories/knowledge from ACE and prepends them to prompts.\n\nConfiguration:\n    ACE_ENABLE_CONTEXT_INJECTION: Enable/disable context injection (default: false)\n    ACE_CONTEXT_MAX_ITEMS: Maximum context items to inject (default: 5)\n    ACE_CONTEXT_FORMAT: Output format - \"plain\" or \"markdown\" (default: plain)\n\nWhen disabled, returns the original prompt",
        "#!/usr/bin/env python\n\"\"\"\nStructured Query Enhancer based on .enhancedprompt.md methodology.\n\nEXHAUSTIVE IMPLEMENTATION - Covers ALL software engineering domains.\n\nTransforms vague user queries into structured, actionable prompts using:\n1. Intent Classification (ANALYTICAL/IMPLEMENTATION/TROUBLESHOOTING/EXPLORATORY/LEARNING/REFACTORING)\n2. Domain Detection (40+ domains covering all software engineering areas)\n3. Context Expansion with domain-specific terminology (1000+ expansion terms)\n4. Query ",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "#!/usr/bin/env python3\n\"\"\"Debug why code retrieval isn't working in MCP server.\"\"\"\n\nimport os\n\nprint(\"=== ENV CHECK ===\")\nprint(f\"ACE_WORKSPACE_PATH: {os.environ.get('ACE_WORKSPACE_PATH', 'NOT SET')}\")\nprint(f\"QDRANT_URL: {os.environ.get('QDRANT_URL', 'NOT SET')}\")\nprint(f\"ACE_CODE_COLLECTION: {os.environ.get('ACE_CODE_COLLECTION', 'NOT SET')}\")\n\nprint(\"\\n=== QDRANT COLLECTIONS ===\")\nfrom qdrant_client import QdrantClient\nclient = QdrantClient(url=\"http://localhost:6333\")\ncollections = [c.name f",
        "\"\"\"Debug doc ranking for pattern queries.\"\"\"\n\nfrom ace.code_retrieval import CodeRetrieval\n\nr = CodeRetrieval()\n\n# Check if docs even get embedded/indexed\nquery = \"error handling patterns in Python with try except\"\nprint(f\"Query: {query}\")\nprint(\"=\" * 60)\n\n# Run search with more results to see docs\nresults = r.search(query, limit=50)\n\n# Separate code and docs\ncode_files = [res for res in results if not res['file_path'].endswith('.md')]\ndoc_files = [res for res in results if res['file_path'].ends"
      ],
      "ace_line_counts": [
        110,
        35,
        37,
        55,
        34
      ],
      "auggie_files": [
        "QUICKSTART_CLAUDE_CODE.md"
      ],
      "auggie_contents": [
        "...\n    79\t\n    80\tShould show:\n    81\t- SessionStart: ace_session_start.py\n    82\t- UserPromptSubmit: ace_inject_context.py\n    83\t- PostToolUse: ace_learn_from_edit.py\n    84\t- Stop: ace_session_end.py\n    85\t\n    86\t### Test hooks manually:\n    87\t\n    88\t```powershell\n    89\t# Test session start\n    90\tpython .claude/hooks/ace_session_start.py\n... (501 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "Better chunk size (54 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "config validation pydantic",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/config.py",
        "ace/security.py",
        "ace/retrieval_optimized.py",
        "ace/typo_correction.py"
      ],
      "ace_scores": [
        0.69106002,
        0.53010285,
        0.5267504,
        0.5099229,
        0.49761847
      ],
      "ace_contents": [
        "class MultiStageConfig:\n    \"\"\"\n    Multi-stage retrieval configuration (coarse-to-fine optimization).\n\n    Implements a 4-stage retrieval pipeline:\n    1. Stage 1 (Coarse): High-recall candidate retrieval (10x limit)\n    2. Stage 2 (Filter): Score-based filtering (DISABLED by default - RRF scores unreliable)\n    3. Stage 3 (Rerank): Cross-encoder reranking on all Stage 1 candidates\n    4. Stage 4 (Final): Deduplication and final selection\n\n    Benefits:\n    - Higher recall by fetching more cand",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im"
      ],
      "ace_line_counts": [
        113,
        330,
        18,
        91,
        652
      ],
      "auggie_files": [
        "ace/config.py",
        "benchmarks/manager.py"
      ],
      "auggie_contents": [
        "...\n    41\t\n    42\t\n    43\t@dataclass\n    44\tclass EmbeddingConfig:\n    45\t    \"\"\"Embedding model configuration for memory/lessons (general-purpose).\"\"\"\n    46\t\n    47\t    # LM Studio server\n    48\t    url: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_URL\", \"http://192.168.10.64:1234\"))\n    49\t\n    50\t    # Model name (Qwen3-Embedding-8B - proper embedding model, 4096 dims)\n    51\t    model: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_MODEL\", \"text-embedding-qwen3-embedding-8b\"))\n    52\t\n... (528 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"pydantic BaseModel configuration classes validation\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   199\t\n   200\t    def validate_config(self, task_name: str) -> List[str]:\n   201\t        \"\"\"\n   202\t        Validate a benchmark configuration and return any issues found.\n   203\t\n   204\t        Returns:\n   205\t            List of validation error messages, empty if valid.\n   206\t        \"\"\"\n   207\t        errors = []\n   208\t\n   209\t        try:\n   210\t            config = self.get_config(task_name)\n... (537 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        20,
        41
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (3 vs 1)"
      ],
      "auggie_advantages": [
        "Better chunk size (30 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "defaults dict configuration",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/retrieval_presets.py",
        "ace/config.py",
        "ace/retrieval_presets.py",
        "tenant_data/learned_typos.json",
        "ace/retrieval_optimized.py"
      ],
      "ace_scores": [
        0.46251523,
        0.4555466,
        0.43999103,
        0.43487227,
        0.42152348
      ],
      "ace_contents": [
        "def get_preset_config(preset: RetrievalPreset) -> RetrievalConfig:\n    \"\"\"Get configuration for a preset.\"\"\"\n    return PRESET_CONFIGS.get(preset, PRESET_CONFIGS[RetrievalPreset.BALANCED])\n\n\ndef cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0\n\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    norm1 = math.sqrt(sum(a * a for a in vec1))\n",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "\"\"\"\nRetrieval Presets - Optimized configurations for different query types.\n\nBased on empirical testing:\n- Baseline precision: 75.6%\n- Architecture queries: 33.3% (worst)\n- Target: 95%+ precision across all categories\n\nWinning optimizations:\n1. BM25-heavy weighting (dense=0.3, sparse=0.7) -> +50% P@3\n2. Post-retrieval deduplication (0.90 threshold) -> +2.7%\n3. Query expansion with domain synonyms -> +3% (conditional)\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing",
        "{\n  \"typos\": {\n    \"aceconfig\": \"config\",\n    \"content\": \"context\",\n    \"thresholds\": \"threshold\",\n    \"inline\": \"online\",\n    \"configure\": \"config\",\n    \"except\": \"exception\",\n    \"monitor\": \"monitoring\",\n    \"configs\": \"config\",\n    \"simple\": \"sample\",\n    \"contexts\": \"context\",\n    \"tracking\": \"ranking\",\n    \"examples\": \"samples\",\n    \"optimizations\": \"optimization\",\n    \"expected\": \"execute\",\n    \"variations\": \"validation\",\n    \"configura\": \"configuration\",\n    \"match\": \"batch\",\n    \"real\": ",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa"
      ],
      "ace_line_counts": [
        87,
        330,
        58,
        233,
        91
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized ACE configuration.\n     2\t\n     3\tAll embedding and retrieval settings in one place.\n     4\tOverride via environment variables or .env file.\n     5\t\"\"\"\n     6\t\n     7\timport os\n     8\tfrom dataclasses import dataclass, field\n     9\tfrom typing import Optional\n    10\tfrom pathlib import Path\n    11\t\n    12\t# Load .env if python-dotenv is available\n    13\ttry:\n... (464 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "nested config structure",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/config.py",
        "check_playbook.py",
        "ace/retrieval_optimized.py",
        "ace/retrieval_presets.py"
      ],
      "ace_scores": [
        0.64664544,
        0.45865712,
        0.4427564,
        0.43237332,
        0.4269529
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "class MultiStageConfig:\n    \"\"\"\n    Multi-stage retrieval configuration (coarse-to-fine optimization).\n\n    Implements a 4-stage retrieval pipeline:\n    1. Stage 1 (Coarse): High-recall candidate retrieval (10x limit)\n    2. Stage 2 (Filter): Score-based filtering (DISABLED by default - RRF scores unreliable)\n    3. Stage 3 (Rerank): Cross-encoder reranking on all Stage 1 candidates\n    4. Stage 4 (Final): Deduplication and final selection\n\n    Benefits:\n    - Higher recall by fetching more cand",
        "\"\"\"Check config.py chunks.\"\"\"\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Filter, FieldCondition, MatchValue\n\nc = QdrantClient('http://localhost:6333')\nr = c.scroll(\n    'ace_code_context', \n    limit=50,\n    scroll_filter=Filter(must=[FieldCondition(key='file_path', match=MatchValue(value='ace/config.py'))]),\n    with_payload=True\n)\n\nsorted_chunks = sorted(r[0], key=lambda x: x.payload['start_line'])\nprint(f\"Total chunks for ace/config.py: {len(sorted_chunks)}\")\nprin",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "def get_preset_config(preset: RetrievalPreset) -> RetrievalConfig:\n    \"\"\"Get configuration for a preset.\"\"\"\n    return PRESET_CONFIGS.get(preset, PRESET_CONFIGS[RetrievalPreset.BALANCED])\n\n\ndef cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0\n\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    norm1 = math.sqrt(sum(a * a for a in vec1))\n"
      ],
      "ace_line_counts": [
        330,
        177,
        20,
        91,
        87
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized ACE configuration.\n     2\t\n     3\tAll embedding and retrieval settings in one place.\n     4\tOverride via environment variables or .env file.\n     5\t\"\"\"\n     6\t\n     7\timport os\n     8\tfrom dataclasses import dataclass, field\n     9\tfrom typing import Optional\n    10\tfrom pathlib import Path\n    11\t\n    12\t# Load .env if python-dotenv is available\n    13\ttry:\n... (490 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (3 vs 0)"
      ],
      "auggie_advantages": [
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "config from dict",
      "category": "Configuration",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/unified_memory.py",
        "tenant_data/learned_typos.json",
        "benchmarks/manager.py",
        "check_playbook.py",
        "ace/config.py"
      ],
      "ace_scores": [
        0.52641934,
        0.40568024,
        0.40033627,
        0.3835234,
        0.37742567
      ],
      "ace_contents": [
        "    def from_dict(cls, data: Dict[str, Any]) -> \"UnifiedBullet\":\n        \"\"\"Deserialize from dictionary.\"\"\"\n        # Handle namespace enum\n        namespace = data.get(\"namespace\", \"user_prefs\")\n        if isinstance(namespace, str):\n            try:\n                namespace = UnifiedNamespace(namespace)\n            except ValueError:\n                namespace = UnifiedNamespace.USER_PREFS\n\n        # Handle source enum\n        source = data.get(\"source\", \"migration\")\n        if isinstance(sour",
        "{\n  \"typos\": {\n    \"aceconfig\": \"config\",\n    \"content\": \"context\",\n    \"thresholds\": \"threshold\",\n    \"inline\": \"online\",\n    \"configure\": \"config\",\n    \"except\": \"exception\",\n    \"monitor\": \"monitoring\",\n    \"configs\": \"config\",\n    \"simple\": \"sample\",\n    \"contexts\": \"context\",\n    \"tracking\": \"ranking\",\n    \"examples\": \"samples\",\n    \"optimizations\": \"optimization\",\n    \"expected\": \"execute\",\n    \"variations\": \"validation\",\n    \"configura\": \"configuration\",\n    \"match\": \"batch\",\n    \"real\": ",
        "\"\"\"\nBenchmark task manager for configuration-driven evaluation.\n\nThis module implements the BenchmarkTaskManager which handles YAML config loading,\ntask discovery, and benchmark instantiation following lm-evaluation-harness patterns.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport sys\nimport yaml\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Type\n\n# Try to import tomllib (Python 3.11+) or tomli for TOML support\ntry:\n    if sys.version_info >= (3, 11):\n        import",
        "\"\"\"Check config.py chunks.\"\"\"\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Filter, FieldCondition, MatchValue\n\nc = QdrantClient('http://localhost:6333')\nr = c.scroll(\n    'ace_code_context', \n    limit=50,\n    scroll_filter=Filter(must=[FieldCondition(key='file_path', match=MatchValue(value='ace/config.py'))]),\n    with_payload=True\n)\n\nsorted_chunks = sorted(r[0], key=lambda x: x.payload['start_line'])\nprint(f\"Total chunks for ace/config.py: {len(sorted_chunks)}\")\nprin",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s"
      ],
      "ace_line_counts": [
        104,
        233,
        112,
        20,
        221
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized ACE configuration.\n     2\t\n     3\tAll embedding and retrieval settings in one place.\n     4\tOverride via environment variables or .env file.\n     5\t\"\"\"\n     6\t\n     7\timport os\n     8\tfrom dataclasses import dataclass, field\n     9\tfrom typing import Optional\n    10\tfrom pathlib import Path\n    11\t\n    12\t# Load .env if python-dotenv is available\n    13\ttry:\n... (518 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 4 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 5)",
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 5,
      "auggie_expected_rank": 1
    },
    {
      "query": "try except error handling pattern",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "debug_docs.py",
        "docs/INTEGRATION_GUIDE.md",
        "debug_doc_ranking.py",
        "ace/pattern_detector.py"
      ],
      "ace_scores": [
        0.4990272,
        0.48460218,
        0.47873296000000004,
        0.46781826,
        0.45865613
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "import os\nimport re\n\n# Simulate the actual function\nfile_path = \"docs/INTEGRATION_GUIDE.md\"\nquery = \"try except error handling pattern\"\n\nstop_words = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'how', 'what',\n              'where', 'when', 'why', 'can', 'will', 'method', 'function', 'class',\n              'code', 'file', 'def', 'implementation', 'search', 'find', 'get', 'set',\n              'pattern', 'error', 'handling', 'import', 'logging', 'logger', 'setup',\n              'exception",
        "\nfor chunk in ace_agent.stream(\"Generate report\"):\n    print(chunk, end=\"\", flush=True)\n```\n\n#### Key Considerations\n- Collect full response before learning\n- Don't block streaming (learn after completion)\n- Maintain streaming interface for caller\n\n---\n\n### Error-Prone Agents\n\n#### When to Use\n- Agent frequently fails or throws exceptions\n- Want to learn from failures\n- Need robust error handling\n\n#### Pattern\n\n```python\nclass ACERobustAgent:\n    \"\"\"Wraps agent with error handling and failure le",
        "\"\"\"Debug doc ranking for pattern queries.\"\"\"\n\nfrom ace.code_retrieval import CodeRetrieval\n\nr = CodeRetrieval()\n\n# Check if docs even get embedded/indexed\nquery = \"error handling patterns in Python with try except\"\nprint(f\"Query: {query}\")\nprint(\"=\" * 60)\n\n# Run search with more results to see docs\nresults = r.search(query, limit=50)\n\n# Separate code and docs\ncode_files = [res for res in results if not res['file_path'].endswith('.md')]\ndoc_files = [res for res in results if res['file_path'].ends",
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n"
      ],
      "ace_line_counts": [
        149,
        49,
        320,
        34,
        20
      ],
      "auggie_files": [
        "docs/INTEGRATION_GUIDE.md"
      ],
      "auggie_contents": [
        "...\n  1085\t\n  1086\t            curator_output = self.curator.curate(\n  1087\t                reflection=reflection,\n  1088\t                playbook=self.playbook,\n  1089\t                question_context=f\"Task ({'success' if success else 'failure'}): {task}\",\n  1090\t                progress=\"Execution completed\"\n  1091\t            )\n  1092\t\n  1093\t            self.playbook.apply_delta(curator_output.delta)\n  1094\t\n  1095\t        except Exception as learning_error:\n  1096\t            # Never crash due to learning failures\n... (554 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "exception retry backoff resilience",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "ace/retrieval_optimized.py",
        "ace/retrieval.py",
        "ace/scaling.py",
        "ace/security.py"
      ],
      "ace_scores": [
        0.7717394399999999,
        0.4542426,
        0.44326758,
        0.4423411,
        0.43469915
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"Horizontal scaling for ACE Framework with sharded collections and clustering.\n\nPhase 4C: Horizontal Scaling Implementation\n\nThis module provides:\n- ShardedBulletIndex: Multi-tenant bullet storage with tenant/domain/hybrid sharding\n- QdrantCluster: Load-balanced Qdrant cluster with automatic failover\n- ClusterHealthCheck: Cluster health monitoring and metrics\n\nKey features:\n- Tenant isolation via sharded collections\n- Domain-specific bullet stores\n- Load balancing: round-robin, least-connectio",
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes\nclass AuthenticationError(Exception):\n    \"\"\"Raised when authenticat"
      ],
      "ace_line_counts": [
        348,
        91,
        61,
        59,
        89
      ],
      "auggie_files": [
        "ace/resilience.py",
        "docs/API_REFERENCE.md"
      ],
      "auggie_contents": [
        "...\n    48\t\n    49\t    Example:\n    50\t        >>> breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n    51\t        >>>\n    52\t        >>> @breaker\n    53\t        >>> def call_llm(prompt):\n    54\t        ...     return llm_client.complete(prompt)\n    55\t        >>>\n    56\t        >>> try:\n    57\t        ...     result = call_llm(\"Hello\")\n    58\t        >>> except CircuitOpenError:\n    59\t        ...     # Circuit is open, use fallback\n... (516 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"LLM client implementations, API calls that might fail, network requests, external service integrations that need retry logic\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   614\t\n   615\t## LLM Clients\n   616\t\n   617\t### LiteLLMClient\n   618\t\n   619\tSupport for 100+ LLM providers.\n   620\t\n   621\t```python\n   622\tfrom ace import LiteLLMClient\n   623\t\n   624\t# Basic usage\n   625\tclient = LiteLLMClient(model=\"gpt-4\")\n... (464 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"pyproject.toml\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m\nHere's the result of running `cat -n` on pyproject.toml:"
      ],
      "auggie_line_counts": [
        20,
        40
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 1)",
        "High confidence top score (0.772)"
      ],
      "auggie_advantages": [
        "Better chunk size (30 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "API rate limit retry exponential backoff",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "ace/retrieval_optimized.py",
        "benchmark_results/quality_comparison_20260103_024645.json",
        "benchmark_results/quality_comparison_20260103_024857.json",
        "benchmark_results/quality_comparison_20260103_024738.json"
      ],
      "ace_scores": [
        0.63937026,
        0.60098683,
        0.5554778,
        0.55426674,
        0.55392032
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "class LLMQueryRewriter:\n    \"\"\"\n    LLM-based query rewriting for short, ambiguous queries.\n    Uses Z.ai GLM to expand short queries into richer semantic variations.\n    \"\"\"\n\n    # Domain context for the ACE memory knowledge base\n    DOMAIN_CONTEXT = \"\"\"The knowledge base contains:\n- User preferences (coding style, tool choices, workflow patterns)\n- Task strategies (debugging approaches, optimization techniques)\n- Error patterns and fixes (common bugs, solutions, root causes)\n- Configuration be",
        "      ],\n      \"latency_ms\": \"1819.5\"\n    },\n    {\n      \"query\": \"async await promise\",\n      \"category\": \"keyword\",\n      \"description\": \"Async programming keywords\",\n      \"num_results\": 2,\n      \"relevant_at_1\": true,\n      \"relevant_at_5\": true,\n      \"top_result\": \"Enforce async adapter inheritance to block synchronous code from entering async paths\",\n      \"scores\": [\n        \"0.500\",\n        \"0.562\"\n      ],\n      \"latency_ms\": \"1385.2\"\n    },\n    {\n      \"query\": \"SQL injection preventi",
        "        \"1.000\"\n      ],\n      \"latency_ms\": \"1806.3\"\n    },\n    {\n      \"query\": \"async await promise\",\n      \"category\": \"keyword\",\n      \"description\": \"Async programming keywords\",\n      \"num_results\": 2,\n      \"relevant_at_1\": true,\n      \"relevant_at_5\": true,\n      \"top_result\": \"Enforce async adapter inheritance to block synchronous code from entering async paths\",\n      \"scores\": [\n        \"0.500\",\n        \"0.562\"\n      ],\n      \"latency_ms\": \"1191.4\"\n    },\n    {\n      \"query\": \"SQL in",
        "        \"1.000\"\n      ],\n      \"latency_ms\": \"3304.2\"\n    },\n    {\n      \"query\": \"async await promise\",\n      \"category\": \"keyword\",\n      \"description\": \"Async programming keywords\",\n      \"num_results\": 2,\n      \"relevant_at_1\": true,\n      \"relevant_at_5\": true,\n      \"top_result\": \"Enforce async adapter inheritance to block synchronous code from entering async paths\",\n      \"scores\": [\n        \"0.500\",\n        \"0.562\"\n      ],\n      \"latency_ms\": \"3309.0\"\n    },\n    {\n      \"query\": \"SQL in"
      ],
      "ace_line_counts": [
        348,
        67,
        120,
        120,
        120
      ],
      "auggie_files": [
        "ace/retrieval_presets.py"
      ],
      "auggie_contents": [
        "...\n    35\t\n    36\t\n    37\t@dataclass\n    38\tclass RetrievalConfig:\n    39\t    \"\"\"Configuration for a retrieval preset.\"\"\"\n    40\t\n    41\t    # Prefetch multipliers\n    42\t    dense_prefetch_multiplier: int = 3\n    43\t    sparse_prefetch_multiplier: int = 3\n    44\t\n    45\t    # BM25 boost factor (multiplies sparse vector values)\n    46\t    bm25_boost: float = 1.0\n... (563 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace/llm_providers\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        37
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "timeout exception handling httpx",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py",
        "ace/observability/health.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "ace/observability/health.py",
        "ace/retrieval_optimized.py",
        "ace/async_retrieval.py",
        "ace/security.py"
      ],
      "ace_scores": [
        0.5484237,
        0.5221695,
        0.49279323,
        0.48838794,
        0.48447797
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"\nHealth Check System for ACE Framework (Phase 3D)\n\nThis module provides health checks for external dependencies (Qdrant, LM Studio).\nTracks component availability, latency, and error states.\n\"\"\"\n\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict, Optional\n\nimport httpx\n\n\nclass HealthStatus(Enum):\n    \"\"\"Health status enumeration.\"\"\"\n    UP = \"up\"\n    DOWN = \"down\"\n\n\n@dataclass\nclass ComponentHealth:\n    \"\"\"\n    Health status for a single component.\n\n ",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes\nclass AuthenticationError(Exception):\n    \"\"\"Raised when authenticat"
      ],
      "ace_line_counts": [
        348,
        256,
        91,
        155,
        89
      ],
      "auggie_files": [
        "ace/scaling.py",
        "CLAUDE_CODE_INTEGRATION.md"
      ],
      "auggie_contents": [
        "...\n   495\t\n   496\t    def _execute_on_node(\n   497\t        self,\n   498\t        node_url: str,\n   499\t        operation: str,\n   500\t        **kwargs: Any\n   501\t    ) -> Any:\n   502\t        \"\"\"Execute operation on specific node (for failover testing).\n   503\t\n   504\t        Args:\n   505\t            node_url: Target node URL\n   506\t            operation: Operation name\n... (517 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"HTTP client timeout settings, request timeout handling, network timeout errors\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   235\t\n   236\t2. Test learning manually:\n   237\t```powershell\n   238\t# Create test input\n   239\t$input = @{\n   240\t    tool_name = \"Write\"\n   241\t    tool_input = @{\n   242\t        file_path = \"test.txt\"\n   243\t        content = \"test content\"\n   244\t    }\n   245\t    tool_response = @{\n   246\t        success = $true\n... (501 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "connection error handling",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/scaling.py",
        "ace/resilience.py",
        "ace/observability/health.py",
        "benchmark_results/quality_comparison_20260103_024645.json",
        "benchmark_results/quality_comparison_20260103_024738.json"
      ],
      "ace_scores": [
        0.4577062,
        0.4505879,
        0.42554405,
        0.4098339,
        0.4057699
      ],
      "ace_contents": [
        "class QdrantCluster:\n    \"\"\"Manage multiple Qdrant nodes with load balancing and failover.\n\n    Provides high-availability Qdrant access with:\n    - Load balancing strategies (round-robin, least-connections, weighted)\n    - Automatic failover on node failure\n    - Health monitoring\n    - Connection pooling\n\n    Example:\n        >>> cluster = QdrantCluster(\n        ...     nodes=[\"http://node1:6333\", \"http://node2:6333\"],\n        ...     strategy=LoadBalancingStrategy.ROUND_ROBIN\n        ...  )\n ",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"\nHealth Check System for ACE Framework (Phase 3D)\n\nThis module provides health checks for external dependencies (Qdrant, LM Studio).\nTracks component availability, latency, and error states.\n\"\"\"\n\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict, Optional\n\nimport httpx\n\n",
        "      ],\n      \"latency_ms\": \"1819.5\"\n    },\n    {\n      \"query\": \"async await promise\",\n      \"category\": \"keyword\",\n      \"description\": \"Async programming keywords\",\n      \"num_results\": 2,\n      \"relevant_at_1\": true,\n      \"relevant_at_5\": true,\n      \"top_result\": \"Enforce async adapter inheritance to block synchronous code from entering async paths\",\n      \"scores\": [\n        \"0.500\",\n        \"0.562\"\n      ],\n      \"latency_ms\": \"1385.2\"\n    },\n    {\n      \"query\": \"SQL injection preventi",
        "        \"1.000\"\n      ],\n      \"latency_ms\": \"3304.2\"\n    },\n    {\n      \"query\": \"async await promise\",\n      \"category\": \"keyword\",\n      \"description\": \"Async programming keywords\",\n      \"num_results\": 2,\n      \"relevant_at_1\": true,\n      \"relevant_at_5\": true,\n      \"top_result\": \"Enforce async adapter inheritance to block synchronous code from entering async paths\",\n      \"scores\": [\n        \"0.500\",\n        \"0.562\"\n      ],\n      \"latency_ms\": \"3309.0\"\n    },\n    {\n      \"query\": \"SQL in"
      ],
      "ace_line_counts": [
        227,
        149,
        15,
        220,
        120
      ],
      "auggie_files": [
        "ace/resilience.py"
      ],
      "auggie_contents": [
        "...\n    25\t\n    26\t\n    27\tclass CircuitOpenError(Exception):\n    28\t    \"\"\"Exception raised when circuit breaker is open.\"\"\"\n    29\t\n    30\t    def __init__(self, message: str = \"Circuit breaker is open\"):\n    31\t        self.message = message\n    32\t        super().__init__(self.message)\n    33\t\n    34\t\n    35\t@dataclass\n    36\tclass CircuitBreaker:\n... (521 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 4 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)",
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "validation error handling ValueError",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "demo_email_validation.py",
        "email_validator.py",
        "temperature_converter.py",
        "rag_training/training_data/crossencoder_training_pairs.json",
        "rag_training/test_suite/selected_memories.json"
      ],
      "ace_scores": [
        0.3148391,
        0.3048568,
        0.27924992,
        0.18718184999999998,
        0.18486690000000006
      ],
      "ace_contents": [
        "#!/usr/bin/env python3\n\"\"\"\nDemo script to showcase email validation functionality.\n\"\"\"\n\nfrom email_validator import EmailValidator\n",
        "#!/usr/bin/env python3\n\"\"\"\nEmail validation script with comprehensive error messages for common mistakes.\nUses regex patterns to validate email addresses and identify specific validation errors.\n\"\"\"\n\nimport re\nfrom typing import Optional, List, Tuple\n\n\nclass EmailValidator:\n    \"\"\"\n    Email address validator with detailed error reporting.\n\n    Features:\n    - Comprehensive regex-based validation\n    - Detailed error messages for common mistakes\n    - Support for internationalized domain names (",
        "#!/usr/bin/env python3\n\"\"\"\nTemperature Converter Tool\nConverts temperatures between Celsius, Fahrenheit, and Kelvin with input validation.\n\"\"\"\n\nimport argparse\nimport sys\nfrom typing import Union, Tuple\n\n",
        "  },\n  {\n    \"query\": \"handling inputs errors\",\n    \"memory\": \"Validate inputs strictly before processing to avoid false positives.\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"handling inputs errors\",\n    \"memory\": \"Group tests in sub-tables for logical clarity and easier maintenance.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"handling inputs errors\",\n    \"memory\": \"Validate inputs before async operations to prevent null reference errors\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"handling inputs errors\",\n",
        "  },\n  {\n    \"memory_id\": 1316445620,\n    \"content\": \"Use TYPE_CHECKING conditional imports with fallback types for compatibility\",\n    \"category\": \"ERROR_HANDLING\",\n    \"feedback_type\": \"ACE_LEARNING\",\n    \"severity\": 7,\n    \"context\": \"D:\\\\ApplicationDevelopment\\\\Tools\\\\agentic-context-engine\\\\ace\\\\unified_memory.py\",\n    \"sample_queries\": [\n      \"type_checking conditional\",\n      \"error_handling type_checking\",\n      \"py type_checking\"\n    ]\n  },\n  {\n    \"memory_id\": 1916539101,\n    \"content"
      ],
      "ace_line_counts": [
        7,
        62,
        11,
        320,
        220
      ],
      "auggie_files": [
        "temperature_converter.py",
        "temperature_converter.py",
        "ace/pattern_detector.py"
      ],
      "auggie_contents": [
        "...\n    46\t\n    47\t\n    48\tdef validate_temperature(value: Union[str, float], unit: str) -> Tuple[float, str]:\n    49\t    \"\"\"\n    50\t    Validate temperature input and return as float with unit.\n    51\t\n    52\t    Args:\n    53\t        value: Temperature value to validate\n    54\t        unit: Unit of the temperature (C, F, K)\n    55\t\n    56\t    Returns:\n    57\t        Tuple of (validated_temperature, normalized_unit)\n... (494 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"where ValueError is raised or caught for validation purposes\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    46\t\n    47\t\n    48\tdef validate_temperature(value: Union[str, float], unit: str) -> Tuple[float, str]:\n    49\t    \"\"\"\n    50\t    Validate temperature input and return as float with unit.\n    51\t\n    52\t    Args:\n    53\t        value: Temperature value to validate\n    54\t        unit: Unit of the temperature (C, F, K)\n    55\t\n    56\t    Returns:\n    57\t        Tuple of (validated_temperature, normalized_unit)\n... (506 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"error handling patterns and exception handling for validation\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    57\t\n    58\t\n    59\tclass PatternDetector:\n    60\t    \"\"\"\n    61\t    Detects common error patterns and provides fix templates.\n    62\t    \n    63\t    Features:\n    64\t    - Config-based enable/disable via ACE_ENABLE_PATTERN_DETECTION\n    65\t    - Pattern registration with regex and fix templates\n    66\t    - Occurrence caching and statistics\n    67\t    - Learning from successful resolutions\n    68\t    - Built-in common error patterns\n... (486 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 1)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "file not found error handling",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/code_indexer.py"
      ],
      "ace_files": [
        "ace/code_retrieval.py",
        "analyze_false_positives.py",
        "compare_ace_auggie_headtohead.py",
        "debug_docs.py",
        "debug_doc_ranking.py"
      ],
      "ace_scores": [
        0.5765734,
        0.5540293,
        0.5305769,
        0.45507273,
        0.43611795
      ],
      "ace_contents": [
        "    def _apply_filename_boost(self, query: str, file_path: str, score: float, content: str = \"\") -> float:\n        \"\"\"\n        Apply filename and content boost when query terms match file path or definitions.\n        \n        This mimics Auggie MCP's behavior where files with names matching\n        query terms OR containing class/function definitions get prioritized.\n        \n        Args:\n            query: Original search query\n            file_path: File path being scored\n            score: O",
        "\"\"\"Analyze ACE false positives and memory performance.\"\"\"\n\nfrom ace.code_retrieval import CodeRetrieval\nfrom ace.unified_memory import UnifiedMemoryIndex\n\ndef analyze_error_handling_query():\n    \"\"\"Analyze why fibonacci.py ranks high for error handling query.\"\"\"\n    print(\"=\" * 60)\n    print(\"ANALYZING: 'error handling patterns in Python with try except'\")\n    print(\"=\" * 60)\n    \n    r = CodeRetrieval()\n    results = r.search('error handling patterns in Python with try except', limit=20)\n    \n ",
        "\"\"\"Head-to-head ACE vs Auggie comparison test.\n\nFor each query, we call both systems and compare:\n1. Top file match\n2. Top 3 files overlap\n3. Content relevance\n\"\"\"\nimport subprocess\nimport sys\nimport json\nfrom typing import Dict, List, Any\n\n# Test queries - comprehensive coverage\nTEST_QUERIES = [\n    # Core ACE classes\n    \"EmbeddingConfig class definition dataclass\",\n    \"UnifiedMemoryIndex class Qdrant namespace hybrid search\",\n    \"CodeRetrieval class search method dense vector\",\n    \"ASTChun",
        "import os\nimport re\n\n# Simulate the actual function\nfile_path = \"docs/INTEGRATION_GUIDE.md\"\nquery = \"try except error handling pattern\"\n\nstop_words = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'how', 'what',\n              'where', 'when', 'why', 'can', 'will', 'method', 'function', 'class',\n              'code', 'file', 'def', 'implementation', 'search', 'find', 'get', 'set',\n              'pattern', 'error', 'handling', 'import', 'logging', 'logger', 'setup',\n              'exception",
        "\"\"\"Debug doc ranking for pattern queries.\"\"\"\n\nfrom ace.code_retrieval import CodeRetrieval\n\nr = CodeRetrieval()\n\n# Check if docs even get embedded/indexed\nquery = \"error handling patterns in Python with try except\"\nprint(f\"Query: {query}\")\nprint(\"=\" * 60)\n\n# Run search with more results to see docs\nresults = r.search(query, limit=50)\n\n# Separate code and docs\ncode_files = [res for res in results if not res['file_path'].endswith('.md')]\ndoc_files = [res for res in results if res['file_path'].ends"
      ],
      "ace_line_counts": [
        530,
        92,
        44,
        49,
        34
      ],
      "auggie_files": [
        "ace/code_indexer.py"
      ],
      "auggie_contents": [
        "...\n   572\t    \n   573\t    def chunk_file(self, file_path: str) -> List[CodeChunkIndexed]:\n   574\t        \"\"\"\n   575\t        Parse and chunk a code file.\n   576\t        \n   577\t        Args:\n   578\t            file_path: Absolute path to code file\n   579\t            \n   580\t        Returns:\n   581\t            List of CodeChunkIndexed instances\n   582\t        \"\"\"\n   583\t        chunks = []\n... (525 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"Python FileNotFoundError best practices error handling 2026\"\n   num_results: 3\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        33
      ],
      "winner": "AUGGIE",
      "reason": "Auggie found expected file at rank 1, ACE missed it",
      "ace_advantages": [],
      "auggie_advantages": [
        "Found expected file at rank 1"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": true,
      "ace_expected_rank": -1,
      "auggie_expected_rank": 1
    },
    {
      "query": "embedding error fallback",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/semantic_scorer.py"
      ],
      "ace_files": [
        "ace/semantic_scorer.py",
        "ace/code_retrieval.py",
        "ace/gemini_embeddings.py",
        "ace/unified_memory.py",
        "ace/openai_embeddings.py"
      ],
      "ace_scores": [
        0.58714428,
        0.5587598,
        0.48236164,
        0.4718604,
        0.46859208
      ],
      "ace_contents": [
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "class CodeRetrieval:\n    \"\"\"Semantic code search with Auggie-style output formatting.\n    \n    Queries indexed code chunks and formats results in a way compatible\n    with Auggie MCP output, supporting blended code + memory results.\n    \n    Uses Voyage-code-3 embeddings (1024d) for optimal code semantic \n    understanding compared to general-purpose embeddings.\n    \n    Configuration via environment variables:\n        QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n        ACE_CO",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "    def _get_embedding(self, text: str) -> Optional[List[float]]:\n        \"\"\"Get embedding from LM Studio with automatic EOS token handling.\n\n        Uses aggressive timeout to prevent hanging on unreachable embedding servers.\n        Returns None quickly if embedding server is unavailable.\n        \"\"\"\n        if not HTTPX_AVAILABLE:\n            return None\n\n        try:\n            # Add EOS token for Qwen models to fix GGUF tokenizer warning\n            # This ensures proper sentence boundary ",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration"
      ],
      "ace_line_counts": [
        104,
        108,
        157,
        35,
        113
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "import error optional dependency",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/features.py"
      ],
      "ace_files": [
        "ace/features.py",
        "ace/dependency_graph.py",
        "ace/llm_providers/__init__.py",
        "ace/__init__.py",
        "ace/llm_providers/langchain_client.py"
      ],
      "ace_scores": [
        0.7660070999999999,
        0.69314392,
        0.6756097,
        0.49091434,
        0.48286656
      ],
      "ace_contents": [
        "\"\"\"Centralized optional dependency detection for ACE framework.\n\nThis module provides a clean interface for checking which optional dependencies\nare available, avoiding scattered try/except imports throughout the codebase.\n\nUsage:\n    >>> from ace.features import has_opik, has_litellm, has_langchain\n    >>> if has_opik():\n    ...     from ace.observability import OpikIntegration\n    ...     integration = OpikIntegration()\n\"\"\"\n\nfrom typing import Dict, Optional\n\n\n# Cache for dependency checks to ",
        "\"\"\"Dependency graph analysis for code understanding.\n\nExtracts imports, function calls, and dependency relationships from source code\nusing tree-sitter for multiple programming languages.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nimport re\n\n\n@dataclass\nclass Import:\n    \"\"\"Represents an import statement in source code.\"\"\"\n\n    module: str\n    names: List[str] = field(default_factory=list)\n    alias: Optional[str] = None\n    lin",
        "\"\"\"Production LLM client implementations for ACE.\"\"\"\n\nfrom typing import Optional\nfrom .litellm_client import LiteLLMClient, LiteLLMConfig\n\nLangChainLiteLLMClient: Optional[type]\n\ntry:\n    from .langchain_client import LangChainLiteLLMClient as _LangChainLiteLLMClient\n\n    LangChainLiteLLMClient = _LangChainLiteLLMClient  # type: ignore[assignment]\nexcept ImportError:\n    LangChainLiteLLMClient = None  # Optional dependency  # type: ignore[assignment]\n\n__all__ = [\n    \"LiteLLMClient\",\n    \"LiteL",
        "\"\"\"Agentic Context Engineering (ACE) reproduction framework.\"\"\"\n\nfrom typing import Optional\nfrom .playbook import Bullet, EnrichedBullet, Playbook, enrich_bullet, migrate_bullet\nfrom .delta import DeltaOperation, DeltaBatch\nfrom .retrieval import SmartBulletIndex, ScoredBullet, IntentClassifier\nfrom .llm import LLMClient, DummyLLMClient, TransformersLLMClient\nfrom .roles import (\n    Generator,\n    ReplayGenerator,\n    Reflector,\n    Curator,\n    GeneratorOutput,\n    ReflectorOutput,\n    Curato",
        "\"\"\"LangChain integration for ACE using langchain-litellm.\"\"\"\n\nfrom typing import Optional, Dict, Any, AsyncIterator, Iterator\nimport logging\n\nRouter: Optional[type]\n\ntry:\n    from langchain_litellm import ChatLiteLLM, ChatLiteLLMRouter\n    from litellm import Router as LiteLLMRouter\n\n    LANGCHAIN_AVAILABLE = True\n    Router = LiteLLMRouter  # type: ignore[assignment]\nexcept ImportError:\n    LANGCHAIN_AVAILABLE = False\n    ChatLiteLLM = None  # type: ignore\n    ChatLiteLLMRouter = None  # type: "
      ],
      "ace_line_counts": [
        113,
        136,
        20,
        135,
        102
      ],
      "auggie_files": [
        "ace/features.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized optional dependency detection for ACE framework.\n     2\t\n     3\tThis module provides a clean interface for checking which optional dependencies\n     4\tare available, avoiding scattered try/except imports throughout the codebase.\n     5\t\n     6\tUsage:\n     7\t    >>> from ace.features import has_opik, has_litellm, has_langchain\n     8\t    >>> if has_opik():\n     9\t    ...     from ace.observability import OpikIntegration\n    10\t    ...     integration = OpikIntegration()\n    11\t\"\"\"\n    12\t\n    13\tfrom typing import Dict, Optional\n... (550 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.766)"
      ],
      "auggie_advantages": [
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "graceful degradation circuit breaker",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/resilience.py",
        "ace/observability/tracing.py",
        "ace/observability/opik_integration.py",
        "ace/roles.py",
        "ace/scaling.py"
      ],
      "ace_scores": [
        0.7410059499999999,
        0.66314783,
        0.62958386,
        0.61463635,
        0.48103148
      ],
      "ace_contents": [
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"\nDistributed Tracing for ACE Framework (Phase 3D)\n\nThis module provides OpenTelemetry integration for distributed tracing.\nWhen OpenTelemetry is not installed, importing TracingManager raises ImportError\nto allow tests to properly skip. The trace_operation decorator gracefully degrades.\n\"\"\"\n\nimport functools\nfrom typing import Optional, Any\n\n# Try to import OpenTelemetry\ntry:\n    from opentelemetry import trace\n    from opentelemetry.trace import Status, StatusCode\n\n    TRACING_AVAILABLE = Tr",
        "\"\"\"\nOpik Integration for ACE Framework\n\nProvides enterprise-grade observability and tracing for ACE components.\nReplaces custom explainability with production-ready Opik platform.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import asdict\n\nOpikLogger: Optional[type]\n\ntry:\n    import opik\n    from opik import track, opik_context\n\n    OPIK_AVAILABLE = True\n\n    # Try to import LiteLLM Opik",
        "\"\"\"Generator, Reflector, and Curator components.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom .delta import DeltaBatch\nfrom .llm import LLMClient\nfrom .playbook import Playbook\nfrom .prompts import CURATOR_PROMPT, GENERATOR_PROMPT, REFLECTOR_PROMPT\n\n# Import Opik tracing with graceful degradation\ntry:\n    from .observability.tracers import maybe_track\nexcept ImportError:",
        "\"\"\"Horizontal scaling for ACE Framework with sharded collections and clustering.\n\nPhase 4C: Horizontal Scaling Implementation\n\nThis module provides:\n- ShardedBulletIndex: Multi-tenant bullet storage with tenant/domain/hybrid sharding\n- QdrantCluster: Load-balanced Qdrant cluster with automatic failover\n- ClusterHealthCheck: Cluster health monitoring and metrics\n\nKey features:\n- Tenant isolation via sharded collections\n- Domain-specific bullet stores\n- Load balancing: round-robin, least-connectio"
      ],
      "ace_line_counts": [
        348,
        101,
        41,
        115,
        59
      ],
      "auggie_files": [
        "ace/resilience.py",
        "ace/roles.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Resilience patterns for robust LLM interactions.\n     2\t\n     3\tThis module provides circuit breaker, retry, and other resilience patterns\n     4\tfor handling transient failures in LLM API calls.\n     5\t\"\"\"\n     6\t\n     7\tfrom __future__ import annotations\n     8\t\n     9\timport functools\n    10\timport time\n    11\tfrom dataclasses import dataclass, field\n    12\tfrom enum import Enum\n    13\tfrom threading import Lock\n... (523 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"LLM client error handling, API failures, timeout handling, rate limiting, exception handling in roles (Generator, Reflector, Curator)\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   116\t\n   117\t\n   118\tclass Generator:\n   119\t    \"\"\"\n   120\t    Produces answers using the current playbook of strategies.\n   121\t\n   122\t    The Generator is one of three core ACE roles. It takes a question and\n   123\t    uses the accumulated strategies in the playbook to produce reasoned answers.\n   124\t\n   125\t    Args:\n   126\t        llm: The LLM client to use for generation\n   127\t        prompt_template: Custom prompt template (uses GENERATOR_PROMPT by default)\n... (505 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        20,
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (0.741)"
      ],
      "auggie_advantages": [
        "Better chunk size (30 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "finally cleanup block",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "cleanup_and_validate_learned_typos.py",
        "ace/typo_correction.py",
        "ace/query_preprocessor.py",
        "rag_training/optimizations/v3_bge_reranker.py",
        "ace/deduplication.py"
      ],
      "ace_scores": [
        0.34199053,
        0.33626294,
        0.33307067,
        0.32718983999999995,
        0.32445583
      ],
      "ace_contents": [
        "#!/usr/bin/env python3\n\"\"\"Clean up learned_typos.json by removing bad corrections and cycle mappings.\n\nThis script:\n1. Loads the current learned_typos.json\n2. Removes low-similarity corrections (< 0.70)\n3. Removes cycle mappings (A->B and B->A)\n4. Backs up the original file\n5. Saves the cleaned version\n\"\"\"\n\nimport json\nimport shutil\nimport difflib\nfrom pathlib import Path\nfrom datetime import datetime\n\n\ndef cleanup_learned_typos(\n    typos_path: Path,\n    min_similarity: float = 0.70,\n    backup",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im",
        "\"\"\"\nQuery preprocessing module for ACE framework.\n\nProvides text normalization, non-query detection, conversational wrapper removal,\nand typo correction.\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import List\n\nfrom .typo_correction import TypoCorrector\n\n\n@dataclass\nclass PreprocessResult:\n    \"\"\"Result of query preprocessing.\"\"\"\n    cleaned_query: str\n    is_valid_query: bool\n    original_query: str\n    transformations_applied: List[str] = field(default_factory=list)",
        "    def close(self):\n        self.client.close()\n\n\ndef main():\n    base_path = Path(__file__).parent.parent\n    test_suite = base_path / \"test_suite\" / \"enhanced_test_suite.json\"\n    baseline_path = base_path / \"baseline_results\" / \"hybrid_baseline.json\"\n    v2_path = base_path / \"optimization_results\" / \"v2_query_expansion.json\"\n    output = base_path / \"optimization_results\" / \"v3_bge_reranker.json\"\n\n    evaluator = BGERerankerEvaluator()\n    try:\n        result = evaluator.run_evaluation(test",
        "\"\"\"\nAdvanced Memory Deduplication System for RAG.\n\nThis module provides clustering-based deduplication for Qdrant collections:\n- HDBSCAN/DBSCAN clustering for efficient duplicate detection (O(n log n) vs O(n^2))\n- Multi-collection support (ace_memories_hybrid, ace_unified)\n- Multiple merge strategies (keep_best, merge_content, canonical_form)\n- Cluster quality metrics (silhouette score, Davies-Bouldin index)\n- Dry-run mode for safe preview\n\nArchitecture:\n    1. Load memories with embeddings from"
      ],
      "ace_line_counts": [
        144,
        652,
        21,
        18,
        104
      ],
      "auggie_files": [
        ".gitignore"
      ],
      "auggie_contents": [
        "     1\t# Runtime data files\n     2\ttenant_data/learned_typos.json\n     3\t\n     4\t# Byte-compiled / optimized / DLL files\n     5\t__pycache__/\n     6\t*.py[cod]\n     7\t*$py.class\n     8\t\n     9\t# C extensions\n    10\t*.so\n    11\t\n    12\t# Distribution / packaging\n    13\t.Python\n... (668 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: launch-process\u001b[0m\n   command: \"git status\"\n   wait: true\n   max_wait_seconds: 10\n   cwd: \"d:\\ApplicationDevelopment\\Tools\\agentic-context-engine\""
      ],
      "auggie_line_counts": [
        52
      ],
      "winner": "TIE",
      "reason": "Both systems performed equally",
      "ace_advantages": [
        "More unique files (5 vs 1)"
      ],
      "auggie_advantages": [
        "Better chunk size (52 lines avg)"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "exception chaining from",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/integrations/langchain.py",
        "ace/security.py",
        "ace/llm_providers/langchain_client.py",
        "ace/retrieval_optimized.py",
        "ace/hyde_retrieval.py"
      ],
      "ace_scores": [
        0.45345446,
        0.43691576,
        0.43653876,
        0.4342679,
        0.43261164
      ],
      "ace_contents": [
        "\"\"\"\nACE + LangChain integration for learning from chain/agent execution.\n\nThis module provides ACELangChain, a wrapper that adds ACE learning capabilities\nto any LangChain Runnable (chains, agents, custom runnables).\n\nWhen to Use ACELangChain:\n- Complex workflows: Multi-step LangChain chains\n- Tool-using agents: LangChain agents with tools\n- Custom runnables: Your own LangChain components\n- Production workflows: LangChain orchestration with learning\n\nWhen NOT to Use ACELangChain:\n- Simple Q&A \u2192 ",
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes",
        "\"\"\"LangChain integration for ACE using langchain-litellm.\"\"\"\n\nfrom typing import Optional, Dict, Any, AsyncIterator, Iterator\nimport logging\n\nRouter: Optional[type]\n\ntry:\n    from langchain_litellm import ChatLiteLLM, ChatLiteLLMRouter\n    from litellm import Router as LiteLLMRouter\n\n    LANGCHAIN_AVAILABLE = True\n    Router = LiteLLMRouter  # type: ignore[assignment]\nexcept ImportError:\n    LANGCHAIN_AVAILABLE = False\n    ChatLiteLLM = None  # type: ignore\n    ChatLiteLLMRouter = None  # type: ",
        "\"\"\"\nOptimized Retrieval Module for ACE\n\nThis module implements state-of-the-art retrieval techniques based on\nextensive RAG optimization research and testing:\n\nBest Configuration (V2 - 62.52% R@5):\n- Hybrid search (dense + BM25 sparse + RRF)\n- Query expansion (4 variations)\n- Multi-query retrieval with RRF fusion\n- Cross-encoder re-ranking (ms-marco-MiniLM-L-6-v2)\n\nPerformance:\n- Recall@1: 41.71% (vs 22.06% baseline)\n- Recall@5: 62.52% (vs 53.28% baseline)\n- MRR: 0.5077 (vs 0.3583 baseline)\n\nUsa",
        "\"\"\"HyDE-enhanced retrieval pipeline for ACE memory system.\n\nIntegrates HyDE (Hypothetical Document Embeddings) with existing hybrid search\ninfrastructure for improved retrieval accuracy on ambiguous/implicit queries.\n\nPipeline:\n1. Query -> HyDE expansion -> Generate hypothetical documents\n2. Embed hypotheticals -> Average embeddings\n3. Search Qdrant with averaged embedding + BM25 sparse\n4. Return results with hybrid RRF fusion\n\nPerformance target: +5-10% for implicit/scenario/template queries\n\"\""
      ],
      "ace_line_counts": [
        501,
        18,
        102,
        91,
        419
      ],
      "auggie_files": [
        "docs/INTEGRATION_GUIDE.md"
      ],
      "auggie_contents": [
        "...\n    62\t\n    63\t```python\n    64\tfrom ace.integrations.base import wrap_playbook_context\n    65\tfrom ace import Playbook\n    66\t\n    67\tplaybook = Playbook()  # or load existing: Playbook.load_from_file(\"expert.json\")\n    68\ttask = \"Process user request\"\n    69\t\n    70\t# Inject playbook context\n    71\tif playbook.bullets():\n    72\t    enhanced_task = f\"{task}\\n\\n{wrap_playbook_context(playbook)}\"\n    73\telse:\n... (540 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "logging error stack trace",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/audit.py"
      ],
      "ace_files": [
        "ace/audit.py",
        "ace/observability/opik_integration.py",
        "ace/security.py",
        "ace/observability/tracing.py",
        "ace/observability/tracers.py"
      ],
      "ace_scores": [
        0.4436965,
        0.42646927,
        0.39387304,
        0.39266503,
        0.3876782
      ],
      "ace_contents": [
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "\"\"\"\nOpik Integration for ACE Framework\n\nProvides enterprise-grade observability and tracing for ACE components.\nReplaces custom explainability with production-ready Opik platform.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import asdict\n\nOpikLogger: Optional[type]\n\ntry:\n    import opik\n    from opik import track, opik_context\n\n    OPIK_AVAILABLE = True\n\n    # Try to import LiteLLM Opik",
        "\"\"\"\nSecurity Module - Enterprise Authentication & Authorization\nImplements API key validation, JWT authentication, RBAC, and security middleware.\n\"\"\"\n\nimport secrets\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Union, Any\n\ntry:\n    import jwt\nexcept ImportError:\n    raise ImportError(\n        \"PyJWT is required for security module. Install with: pip install PyJWT\"\n    )\n\n\n# Exception Classes",
        "\"\"\"\nDistributed Tracing for ACE Framework (Phase 3D)\n\nThis module provides OpenTelemetry integration for distributed tracing.\nWhen OpenTelemetry is not installed, importing TracingManager raises ImportError\nto allow tests to properly skip. The trace_operation decorator gracefully degrades.\n\"\"\"\n\nimport functools\nfrom typing import Optional, Any\n\n# Try to import OpenTelemetry\ntry:\n    from opentelemetry import trace\n    from opentelemetry.trace import Status, StatusCode\n\n    TRACING_AVAILABLE = Tr",
        "\"\"\"\nACE-specific tracing utilities for Opik integration.\n\nProvides utilities for conditionally applying Opik tracing to ACE framework components.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Any, Callable, Optional, TypeVar\n\nlogger = logging.getLogger(__name__)\n\n# Type variable for decorated functions\nF = TypeVar(\"F\", bound=Callable[..., Any])\n\n# Flag to check if Opik is available\n_OPIK_AVAILABLE = False\ntry:\n    import opik\n    from opik import track\n\n    _OPIK_AVA"
      ],
      "ace_line_counts": [
        130,
        279,
        18,
        101,
        62
      ],
      "auggie_files": [
        "ace/adaptation.py"
      ],
      "auggie_contents": [
        "...\n   242\t\n   243\t        # Track adaptation metrics with Opik\n   244\t        try:\n   245\t            self.opik_integration.log_adaptation_metrics(\n   246\t                epoch=epoch,\n   247\t                step=step,\n   248\t                performance_score=performance_score,\n   249\t                bullet_count=len(self.playbook.bullets()),\n   250\t                successful_predictions=1 if performance_score > 0.5 else 0,\n   251\t                total_predictions=1,\n   252\t                metadata={\n   253\t                    \"sample_id\": sample_id,\n... (520 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "error recovery strategy",
      "category": "ErrorHandling",
      "expected_files": [
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/prompts_v2.py",
        "ace/typo_correction.py",
        "ace/chain_of_verification.py",
        "ace/pattern_detector.py",
        "ace/roles.py"
      ],
      "ace_scores": [
        0.6041499,
        0.46609855,
        0.45845267,
        0.45812303,
        0.44184968
      ],
      "ace_contents": [
        "\"\"\"\nState-of-the-art prompt templates for ACE roles - Version 2.0\n\nDEPRECATION WARNING: This module (prompts_v2) is superseded by prompts_v2_1.\nPlease use prompts_v2_1.py for new projects, which includes:\n- MCP (Model Context Protocol) enhancements\n- Improved error handling and validation\n- Better structured reasoning templates\n- Enhanced meta-cognitive instructions\n\nFor migration guide, see docs/PROMPTS.md\n\nThese prompts incorporate best practices from production AI systems including:\n- Identit",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im",
        "\"\"\"Chain-of-Verification (CoVe) for improved reflection accuracy.\n\nThis module implements Chain-of-Verification, which generates verification\nquestions about the initial reflection, answers them independently, and\nuses the answers to refine the final output. This technique improves\naccuracy through self-verification.\n\nReference: Dhuliawala et al., \"Chain-of-Verification Reduces Hallucination\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom typing impor",
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef lo",
        "class Reflector:\n    \"\"\"\n    Analyzes generator outputs to extract lessons and improve strategies.\n\n    The Reflector is the second ACE role. It analyzes the Generator's output\n    and environment feedback to understand what went right or wrong, classifying\n    which playbook bullets were helpful, harmful, or neutral.\n\n    Args:\n        llm: The LLM client to use for reflection\n        prompt_template: Custom prompt template (uses REFLECTOR_PROMPT by default)\n        max_retries: Maximum attempt"
      ],
      "ace_line_counts": [
        894,
        652,
        118,
        166,
        154
      ],
      "auggie_files": [
        "ace/resilience.py",
        "ace/roles.py",
        "ace/retrieval_presets.py"
      ],
      "auggie_contents": [
        "...\n    48\t\n    49\t    Example:\n    50\t        >>> breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n    51\t        >>>\n    52\t        >>> @breaker\n    53\t        >>> def call_llm(prompt):\n    54\t        ...     return llm_client.complete(prompt)\n    55\t        >>>\n    56\t        >>> try:\n    57\t        ...     result = call_llm(\"Hello\")\n    58\t        >>> except CircuitOpenError:\n    59\t        ...     # Circuit is open, use fallback\n... (523 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"JSON parsing errors, retry prompts, error handling in Generator Reflector Curator roles\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    31\t\n    32\t\n    33\tdef _safe_json_loads(text: str) -> Dict[str, Any]:\n    34\t    # Strip markdown code blocks if present\n    35\t    text = text.strip()\n    36\t\n    37\t    # Handle opening fence (with or without language identifier)\n    38\t    if text.startswith(\"```json\"):\n    39\t        text = text[7:].strip()\n    40\t    elif text.startswith(\"```\"):\n    41\t        text = text[3:].strip()\n    42\t\n... (507 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"tenacity retry decorators, exponential backoff, error handling in LLM clients\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   616\t\n   617\t        # Determine endpoint path (local LLM uses /v1/, Z.ai uses direct path)\n   618\t        endpoint = f\"{llm_url}/v1/chat/completions\" if llm_config.use_local_llm else f\"{llm_url}/chat/completions\"\n   619\t\n   620\t        # Retry with exponential backoff for rate limits\n   621\t        max_retries = 3\n   622\t        for retry in range(max_retries):\n   623\t            response = httpx.post(\n   624\t                endpoint,\n   625\t                headers=headers,\n   626\t                json={\n   627\t                    \"model\": model,\n... (533 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        20,
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie found expected file at rank 1, ACE missed it",
      "ace_advantages": [],
      "auggie_advantages": [
        "Found expected file at rank 1"
      ],
      "ace_found_expected": false,
      "auggie_found_expected": true,
      "ace_expected_rank": -1,
      "auggie_expected_rank": 1
    },
    {
      "query": "async def retrieve await",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_retrieval.py",
        "ace/unified_memory.py",
        "ace/async_retrieval.py",
        "ace/retrieval.py",
        "compare_ace_auggie_headtohead.py"
      ],
      "ace_scores": [
        0.8789177,
        0.7451644500000001,
        0.7326466600000001,
        0.7098165700000001,
        0.68504375
      ],
      "ace_contents": [
        "    async def retrieve(\n        self,\n        query: str,\n        limit: int = 10,\n        query_type: Optional[str] = None,\n        min_score: float = 0.0,\n    ) -> List[QdrantScoredResult]:\n        \"\"\"Retrieve bullets using hybrid search asynchronously.\n\n        Combines dense semantic search with BM25 keyword matching\n        using Reciprocal Rank Fusion (RRF).\n\n        Args:\n            query: Natural language query\n            limit: Maximum number of results\n            query_type: Optiona",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non",
        "\"\"\"Head-to-head ACE vs Auggie comparison test.\n\nFor each query, we call both systems and compare:\n1. Top file match\n2. Top 3 files overlap\n3. Content relevance\n\"\"\"\nimport subprocess\nimport sys\nimport json\nfrom typing import Dict, List, Any\n\n# Test queries - comprehensive coverage\nTEST_QUERIES = [\n    # Core ACE classes\n    \"EmbeddingConfig class definition dataclass\",\n    \"UnifiedMemoryIndex class Qdrant namespace hybrid search\",\n    \"CodeRetrieval class search method dense vector\",\n    \"ASTChun"
      ],
      "ace_line_counts": [
        106,
        471,
        155,
        393,
        44
      ],
      "auggie_files": [
        "ace/retrieval.py"
      ],
      "auggie_contents": [
        "...\n   207\t\n   208\t    def retrieve(\n   209\t        self,\n   210\t        query: Optional[str] = None,\n   211\t        task_type: Optional[str] = None,\n   212\t        domain: Optional[str] = None,\n   213\t        complexity: Optional[str] = None,\n   214\t        intent: Optional[IntentType] = None,\n   215\t        limit: Optional[int] = None,\n   216\t        rank_by_effectiveness: bool = False,\n   217\t        min_effectiveness: Optional[float] = None,\n   218\t        query_type: Optional[str] = None,\n... (506 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "asyncio gather parallel",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "compare_ace_auggie_headtohead.py",
        "ace/hyde.py",
        "benchmark_ace_vs_auggie.py"
      ],
      "ace_scores": [
        0.5598209,
        0.53570116,
        0.4945208,
        0.470066,
        0.46050084
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Head-to-head ACE vs Auggie comparison test.\n\nFor each query, we call both systems and compare:\n1. Top file match\n2. Top 3 files overlap\n3. Content relevance\n\"\"\"\nimport subprocess\nimport sys\nimport json\nfrom typing import Dict, List, Any\n\n# Test queries - comprehensive coverage\nTEST_QUERIES = [\n    # Core ACE classes\n    \"EmbeddingConfig class definition dataclass\",\n    \"UnifiedMemoryIndex class Qdrant namespace hybrid search\",\n    \"CodeRetrieval class search method dense vector\",\n    \"ASTChun",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"Expanded ACE vs Auggie benchmark comparison.\n\nTests diverse query types across 5 categories:\n1. Code queries (functions, classes, patterns)\n2. Doc queries (guides, references, tutorials)\n3. Architecture queries (design, components, patterns)\n4. Config queries (settings, environment, credentials)\n5. Edge cases (specific symbols, error messages, imports)\n\nRuns ACE CodeRetrieval and optionally compares against Auggie MCP.\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nimport argparse\nfrom typing im"
      ],
      "ace_line_counts": [
        364,
        155,
        94,
        58,
        120
      ],
      "auggie_files": [
        "ace/async_retrieval.py",
        "ace/async_retrieval.py"
      ],
      "auggie_contents": [
        "...\n   188\t\n   189\t    async def batch_get_embeddings(self, texts: List[str]) -> List[List[float]]:\n   190\t        \"\"\"Retrieve embeddings for multiple texts in parallel.\n   191\t\n   192\t        Uses asyncio.gather for concurrent execution.\n   193\t\n   194\t        Args:\n   195\t            texts: List of texts to embed\n   196\t\n   197\t        Returns:\n   198\t            List of embedding vectors (same order as input).\n   199\t\n... (484 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"async functions that could benefit from parallel execution with asyncio.gather\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   188\t\n   189\t    async def batch_get_embeddings(self, texts: List[str]) -> List[List[float]]:\n   190\t        \"\"\"Retrieve embeddings for multiple texts in parallel.\n   191\t\n   192\t        Uses asyncio.gather for concurrent execution.\n   193\t\n   194\t        Args:\n   195\t            texts: List of texts to embed\n   196\t\n   197\t        Returns:\n   198\t            List of embedding vectors (same order as input).\n   199\t\n... (519 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "async with httpx.AsyncClient",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_retrieval.py",
        "ace/llm_providers/litellm_client.py",
        "ace/hyde.py",
        "ace/openai_embeddings.py",
        "ace/async_adaptation.py"
      ],
      "ace_scores": [
        0.91419575,
        0.36435809999999996,
        0.3642372,
        0.35436870000000004,
        0.34083250000000004
      ],
      "ace_contents": [
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad"
      ],
      "ace_line_counts": [
        155,
        65,
        58,
        27,
        33
      ],
      "auggie_files": [
        "debug_httpx_query.py"
      ],
      "auggie_contents": [
        "     1\t#!/usr/bin/env python3\n     2\t\"\"\"Debug the httpx query where Auggie wins.\"\"\"\n     3\timport sys\n     4\tsys.path.insert(0, \".\")\n     5\t\n     6\tfrom ace.code_retrieval import CodeRetrieval\n     7\t\n     8\tr = CodeRetrieval()\n     9\tquery = \"import httpx async\"\n    10\tprint(f\"Query: {query}\")\n    11\tprint(\"-\" * 50)\n    12\t\n    13\tresults = r.search(query, limit=5)\n... (523 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "async for chunk stream",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/llm_providers/langchain_client.py",
        "ace/code_chunker.py",
        "ace/llm_providers/litellm_client.py",
        "ace/async_retrieval.py"
      ],
      "ace_scores": [
        0.6782045,
        0.62707245,
        0.55214818,
        0.43977076,
        0.41645437
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "    async def acomplete_with_stream(self, prompt: str, **kwargs) -> AsyncIterator[str]:\n        \"\"\"\n        Asynchronously complete a prompt with streaming using the LangChain LiteLLM client.\n\n        Args:\n            prompt: The prompt to complete\n            **kwargs: Additional parameters for the completion\n\n        Yields:\n            Tokens as they are generated\n        \"\"\"\n        filtered_kwargs = self._filter_kwargs(kwargs)\n\n        try:\n            async for chunk in self.llm.astream(p",
        "\"\"\"AST-based semantic code chunking module.\n\nThis module provides intelligent code chunking that respects language syntax\nboundaries (functions, classes, methods) rather than arbitrary line counts.\n\nSupports multiple languages via tree-sitter:\n- Python (via built-in ast module or tree-sitter)\n- JavaScript/TypeScript (via tree-sitter)\n- Go (via tree-sitter)\n\nConfiguration:\n    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    ACE_AST_MAX_LINES: Maximum lines per chunk (def",
        "    async def acomplete(\n        self, prompt: str, system: Optional[str] = None, **kwargs: Any\n    ) -> LLMResponse:\n        \"\"\"\n        Async version of complete.\n\n        Args:\n            prompt: Input prompt text\n            system: Optional system message to prepend to the conversation\n            **kwargs: Additional parameters to pass to the model\n\n        Returns:\n            LLMResponse containing the generated text and metadata\n        \"\"\"\n        # Prepare messages in chat format\n   ",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash"
      ],
      "ace_line_counts": [
        364,
        20,
        420,
        222,
        269
      ],
      "auggie_files": [
        "ace/code_chunker.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"AST-based semantic code chunking module.\n     2\t\n     3\tThis module provides intelligent code chunking that respects language syntax\n     4\tboundaries (functions, classes, methods) rather than arbitrary line counts.\n     5\t\n     6\tSupports multiple languages via tree-sitter:\n     7\t- Python (via built-in ast module or tree-sitter)\n     8\t- JavaScript/TypeScript (via tree-sitter)\n     9\t- Go (via tree-sitter)\n    10\t\n    11\tConfiguration:\n    12\t    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    13\t    ACE_AST_MAX_LINES: Maximum lines per chunk (default: 120)\n... (496 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 5, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 5"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 5,
      "auggie_expected_rank": -1
    },
    {
      "query": "await embedding generation",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/hyde_retrieval.py",
        "ace/gemini_embeddings.py",
        "ace/openai_embeddings.py",
        "ace/async_retrieval.py",
        "ace/semantic_scorer.py"
      ],
      "ace_scores": [
        0.68219053,
        0.6485634,
        0.6329415,
        0.61324537,
        0.6120094
      ],
      "ace_contents": [
        "\"\"\"HyDE-enhanced retrieval pipeline for ACE memory system.\n\nIntegrates HyDE (Hypothetical Document Embeddings) with existing hybrid search\ninfrastructure for improved retrieval accuracy on ambiguous/implicit queries.\n\nPipeline:\n1. Query -> HyDE expansion -> Generate hypothetical documents\n2. Embed hypotheticals -> Average embeddings\n3. Search Qdrant with averaged embedding + BM25 sparse\n4. Return results with hybrid RRF fusion\n\nPerformance target: +5-10% for implicit/scenario/template queries\n\"\"",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_"
      ],
      "ace_line_counts": [
        473,
        280,
        211,
        363,
        104
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 4,
      "auggie_expected_rank": -1
    },
    {
      "query": "async context manager enter exit",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_retrieval.py",
        "ace/multitenancy.py",
        "ace/observability/tracing.py",
        "ace/observability/metrics.py",
        "ace/async_adaptation.py"
      ],
      "ace_scores": [
        0.9096567,
        0.62637975,
        0.6043103999999999,
        0.58837067,
        0.5245117
      ],
      "ace_contents": [
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"\nMulti-tenancy support for ACE framework.\n\nThis module provides tenant isolation for playbooks and Qdrant collections,\nensuring that different tenants cannot access each other's data.\n\nFeatures:\n- Thread-local tenant context tracking\n- Tenant-scoped playbook storage\n- Tenant-scoped Qdrant collections\n- Cross-tenant access prevention\n- Path traversal attack protection\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nimport threading\nfrom pathlib import Path\nfrom typing import List, Optional\n",
        "\"\"\"\nDistributed Tracing for ACE Framework (Phase 3D)\n\nThis module provides OpenTelemetry integration for distributed tracing.\nWhen OpenTelemetry is not installed, importing TracingManager raises ImportError\nto allow tests to properly skip. The trace_operation decorator gracefully degrades.\n\"\"\"\n\nimport functools\nfrom typing import Optional, Any\n\n# Try to import OpenTelemetry\ntry:\n    from opentelemetry import trace\n    from opentelemetry.trace import Status, StatusCode\n\n    TRACING_AVAILABLE = Tr",
        "\"\"\"\nPrometheus Metrics Collection for ACE Framework (Phase 3D)\n\nThis module provides production-grade metrics collection using Prometheus client library.\nTracks retrieval latency, operation counts, and bullet counts across tenants.\n\nDESIGN NOTE: To support both labeled and unlabeled usage (for test compatibility),\nwe create wrapper classes that provide default label values when called without labels.\n\"\"\"\n\nimport time\nfrom contextlib import contextmanager\nfrom typing import Generator, Optional\n\nf",
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad"
      ],
      "ace_line_counts": [
        269,
        128,
        101,
        178,
        364
      ],
      "auggie_files": [
        "ace/context_injector.py",
        "ace/context_injector.py"
      ],
      "auggie_contents": [
        "...\n   135\t    \n   136\t    def inject(self, prompt: str) -> str:\n   137\t        \"\"\"Inject relevant context into a prompt.\n   138\t        \n   139\t        Args:\n   140\t            prompt: Original prompt/query\n   141\t        \n   142\t        Returns:\n   143\t            Prompt with context prepended (or original if disabled/no context)\n   144\t        \"\"\"\n   145\t        if not self._enabled:\n   146\t            return prompt\n... (538 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"classes that need async context manager support\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   135\t    \n   136\t    def inject(self, prompt: str) -> str:\n   137\t        \"\"\"Inject relevant context into a prompt.\n   138\t        \n   139\t        Args:\n   140\t            prompt: Original prompt/query\n   141\t        \n   142\t        Returns:\n   143\t            Prompt with context prepended (or original if disabled/no context)\n   144\t        \"\"\"\n   145\t        if not self._enabled:\n   146\t            return prompt\n... (530 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "asyncio.create_task background",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "ace/integrations/browser_use.py",
        "ace/hyde.py",
        "ace/observability/health.py"
      ],
      "ace_scores": [
        0.48672026,
        0.45241338,
        0.45082968,
        0.43947917,
        0.4385929
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "    def __init__(\n        self,\n        task: Optional[str] = None,\n        llm: Any = None,\n        browser: Optional[Any] = None,\n        ace_model: str = \"openai/glm-4.6\",\n        ace_llm: Optional[LiteLLMClient] = None,\n        ace_max_tokens: int = 2048,\n        ace_api_key: Optional[str] = None,\n        ace_api_base: Optional[str] = None,\n        playbook: Optional[Playbook] = None,\n        playbook_path: Optional[str] = None,\n        is_learning: bool = True,\n        **agent_kwargs,\n    )",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"\nHealth Check System for ACE Framework (Phase 3D)\n\nThis module provides health checks for external dependencies (Qdrant, LM Studio).\nTracks component availability, latency, and error states.\n\"\"\"\n\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict, Optional\n\nimport httpx\n\n"
      ],
      "ace_line_counts": [
        364,
        269,
        100,
        58,
        15
      ],
      "auggie_files": [
        "ace/async_adaptation.py",
        "ace/async_adaptation.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Async adaptation loops for parallel sample processing.\n     2\t\n     3\tThis module provides async versions of OfflineAdapter and OnlineAdapter\n     4\tthat enable parallel processing of samples for improved throughput.\n     5\t\"\"\"\n     6\t\n     7\tfrom __future__ import annotations\n     8\t\n     9\timport asyncio\n    10\tfrom dataclasses import dataclass\n    11\tfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n    12\t\n    13\tif TYPE_CHECKING:\n... (525 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"async def functions, asyncio patterns, event loop usage\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    34\t\n    35\t\n    36\tclass AsyncOfflineAdapter:\n    37\t    \"\"\"Async version of OfflineAdapter for parallel sample processing.\n    38\t\n    39\t    Enables concurrent processing of multiple samples while respecting\n    40\t    max_parallel limits to avoid overwhelming LLM APIs.\n    41\t\n    42\t    Example:\n    43\t        >>> adapter = AsyncOfflineAdapter(playbook, generator, reflector, curator)\n    44\t        >>> results = await adapter.run(samples, environment, epochs=3, max_parallel=5)\n    45\t    \"\"\"\n... (527 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "async generator yield",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/llm_providers/langchain_client.py",
        "ace/async_retrieval.py",
        "ace/hyde.py",
        "ace/self_consistency.py"
      ],
      "ace_scores": [
        0.7009227,
        0.47532836,
        0.4739495,
        0.46920586,
        0.454336
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "    async def acomplete_with_stream(self, prompt: str, **kwargs) -> AsyncIterator[str]:\n        \"\"\"\n        Asynchronously complete a prompt with streaming using the LangChain LiteLLM client.\n\n        Args:\n            prompt: The prompt to complete\n            **kwargs: Additional parameters for the completion\n\n        Yields:\n            Tokens as they are generated\n        \"\"\"\n        filtered_kwargs = self._filter_kwargs(kwargs)\n\n        try:\n            async for chunk in self.llm.astream(p",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"Self-consistency sampling for improved generation accuracy.\n\nThis module implements self-consistency decoding, which generates multiple\nresponses for the same prompt and selects the most consistent answer via\nmajority voting. This technique improves accuracy for tasks where reasoning\npaths can vary but the final answer should converge.\n\nReference: Wang et al., \"Self-Consistency Improves Chain of Thought Reasoning\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom collections import Co"
      ],
      "ace_line_counts": [
        364,
        20,
        269,
        172,
        119
      ],
      "auggie_files": [
        "docs/API_REFERENCE.md"
      ],
      "auggie_contents": [
        "...\n   288\t# Task strategies with high helpful_count are boosted\n   289\t```\n   290\t\n   291\t#### Collection Management\n   292\t\n   293\t```python\n   294\t# Initialize collection (idempotent)\n   295\tindex.initialize_collection()\n   296\t\n   297\t# Check if collection exists\n   298\texists = index._collection_exists()\n   299\t\n... (567 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 3, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 3"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 3,
      "auggie_expected_rank": -1
    },
    {
      "query": "semaphore rate limiting async",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/retrieval_optimized.py",
        "ace/async_retrieval.py",
        "ace/resilience.py",
        "ace/hyde.py"
      ],
      "ace_scores": [
        0.73900784,
        0.60013395,
        0.42101544,
        0.39758074,
        0.39499238
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "class LLMQueryRewriter:\n    \"\"\"\n    LLM-based query rewriting for short, ambiguous queries.\n    Uses Z.ai GLM to expand short queries into richer semantic variations.\n    \"\"\"\n\n    # Domain context for the ACE memory knowledge base\n    DOMAIN_CONTEXT = \"\"\"The knowledge base contains:\n- User preferences (coding style, tool choices, workflow patterns)\n- Task strategies (debugging approaches, optimization techniques)\n- Error patterns and fixes (common bugs, solutions, root causes)\n- Configuration be",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp"
      ],
      "ace_line_counts": [
        364,
        67,
        155,
        149,
        58
      ],
      "auggie_files": [
        "docs/INTEGRATION_GUIDE.md",
        "ace/llm_providers/langchain_client.py"
      ],
      "auggie_contents": [
        "...\n  1290\t\n  1291\t# Usage\n  1292\tasync def main():\n  1293\t    agent = MyAsyncAgent()\n  1294\t    ace_agent = ACEAsyncEnterpriseAgent(agent)\n  1295\t    result = await ace_agent.run(\"Process data\")\n  1296\t\n  1297\tasyncio.run(main())\n  1298\t```\n  1299\t\n  1300\t**Key Features:**\n  1301\t- `AsyncQdrantBulletIndex` uses `httpx.AsyncClient` for non-blocking I/O\n... (511 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"LLM client implementations and API calls that might benefit from rate limiting\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "     1\t\"\"\"LangChain integration for ACE using langchain-litellm.\"\"\"\n     2\t\n     3\tfrom typing import Optional, Dict, Any, AsyncIterator, Iterator\n     4\timport logging\n     5\t\n     6\tRouter: Optional[type]\n     7\t\n     8\ttry:\n     9\t    from langchain_litellm import ChatLiteLLM, ChatLiteLLMRouter\n    10\t    from litellm import Router as LiteLLMRouter\n    11\t\n    12\t    LANGCHAIN_AVAILABLE = True\n    13\t    Router = LiteLLMRouter  # type: ignore[assignment]\n... (482 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        20,
        41
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 3, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 3"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 3,
      "auggie_expected_rank": -1
    },
    {
      "query": "asyncio timeout cancel",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "ace/resilience.py",
        "ace/observability/health.py",
        "ace/hyde.py"
      ],
      "ace_scores": [
        0.4495933,
        0.4349711,
        0.40924567,
        0.37775022,
        0.3759008
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"\nHealth Check System for ACE Framework (Phase 3D)\n\nThis module provides health checks for external dependencies (Qdrant, LM Studio).\nTracks component availability, latency, and error states.\n\"\"\"\n\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict, Optional\n\nimport httpx\n\n\nclass HealthStatus(Enum):\n    \"\"\"Health status enumeration.\"\"\"\n    UP = \"up\"\n    DOWN = \"down\"\n\n\n@dataclass\nclass ComponentHealth:\n    \"\"\"\n    Health status for a single component.\n\n ",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp"
      ],
      "ace_line_counts": [
        364,
        269,
        251,
        256,
        58
      ],
      "auggie_files": [
        "CLAUDE_CODE_INTEGRATION.md",
        "examples/browser-use/form-filler/ace_browser_use.py"
      ],
      "auggie_contents": [
        "...\n   235\t\n   236\t2. Test learning manually:\n   237\t```powershell\n   238\t# Create test input\n   239\t$input = @{\n   240\t    tool_name = \"Write\"\n   241\t    tool_input = @{\n   242\t        file_path = \"test.txt\"\n   243\t        content = \"test content\"\n   244\t    }\n   245\t    tool_response = @{\n   246\t        success = $true\n... (518 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"asyncio.timeout asyncio.wait_for CancelledError timeout exceptions\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   229\t\n   230\t            # Run with timeout\n   231\t            history = await asyncio.wait_for(agent.run(max_steps=10), timeout=240.0)\n   232\t            return history\n   233\t        except asyncio.TimeoutError:\n   234\t            # Try to get steps from history if it exists\n   235\t            number_of_steps = 25  # default to max_steps\n   236\t            try:\n   237\t                if \"history\" in locals() and history is not None:\n   238\t                    number_of_steps = (\n   239\t                        history.number_of_steps()\n   240\t                        if hasattr(history, \"number_of_steps\")\n... (539 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "asyncio.run main",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "benchmark_ace_vs_auggie.py",
        "examples/browser-use/domain-checker/ace_domain_checker.py",
        "verify_setup.py"
      ],
      "ace_scores": [
        0.54144657,
        0.5278014,
        0.52236164,
        0.52075166,
        0.4942839
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "def main():\n    parser = argparse.ArgumentParser(description=\"ACE vs Auggie benchmark\")\n    parser.add_argument(\"--auggie\", action=\"store_true\", help=\"Include Auggie MCP comparison\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    args = parser.parse_args()\n    \n    results = run_benchmark(include_auggie=args.auggie, verbose=args.verbose)\n    \n    # Return exit code based on doc coverage (should be > 50%)\n    doc_results = [r for r in results if r.categ",
        "async def main():\n    \"\"\"Run domain checking with ACE learning.\"\"\"\n\n    # Capture start time for trace filtering\n    run_start_time = datetime.datetime.now(datetime.timezone.utc)\n\n    # Configure observability\n    try:\n        configure_opik(project_name=\"ace-domain-checker\")\n        print(\"\ud83d\udcca Opik observability enabled\")\n    except:\n        print(\"\ud83d\udcca Opik not available, continuing without observability\")\n\n    print(\"\\n\ud83d\udd0d ACE + Browser-Use Domain Checker\")\n    print(\"\ud83e\udde0 Automated domain checking wit",
        "def main():\n    print(\"\ud83d\ude80 ACE Framework Development Environment Verification\")\n    print(\"=\" * 60)\n\n    # Check Python version\n    python_version = sys.version_info\n    print(f\"\\n\ud83d\udccb Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n    if python_version < (3, 11):\n        print(\"\u274c Python 3.11+ required\")\n        return False\n    else:\n        print(\"\u2705 Python version OK\")\n\n    # Check if UV is available\n    has_uv = run_command(\"uv --version\", \"Checking UV insta"
      ],
      "ace_line_counts": [
        364,
        155,
        16,
        193,
        102
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "event loop get_event_loop",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "ace/retrieval.py",
        "ace/retrieval_optimized.py",
        "ace/resilience.py"
      ],
      "ace_scores": [
        0.5215406,
        0.5043769,
        0.49232548,
        0.473184,
        0.47192168
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n"
      ],
      "ace_line_counts": [
        33,
        155,
        61,
        87,
        18
      ],
      "auggie_files": [
        ".serena/memories/ace_mcp_asyncio_fix_2025_01.md"
      ],
      "auggie_contents": [
        "     1\t# ACE MCP Server Async Fix (January 2025)\n     2\t\n     3\t## Problem\n     4\tACE MCP server tool calls (`ace_retrieve`, `ace_store`, `ace_search`) were hanging for 40+ minutes when called via MCP.\n     5\t\n     6\t## Root Cause\n     7\tThe `async def handle_*` functions were calling **blocking synchronous code** directly, which blocked the event loop:\n     8\t- `code_retrieval.search()` - blocking Voyage API call\n     9\t- `index.retrieve()` - blocking Qdrant + embedding call\n    10\t- `index.index_bullet()` - blocking Qdrant call\n    11\t- `indexer.index_workspace()` - blocking filesystem + embedding calls\n    12\t\n    13\t## Solution\n... (525 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"asyncio get_event_loop deprecated python 3.12 2026\"\n   num_results: 5\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        39
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "asyncio.Queue producer consumer",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/llm.py",
        "ace/typo_correction.py",
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "ace/hyde.py"
      ],
      "ace_scores": [
        0.61171783,
        0.33573463000000003,
        0.32255094,
        0.29853456,
        0.25625868
      ],
      "ace_contents": [
        "\"\"\"LLM client abstractions used by ACE components.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nimport json\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Deque, Dict, Optional, Union\n\nif TYPE_CHECKING:\n    import torch\n\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Container for LLM outputs.\"\"\"\n\n    text: str\n    raw: Optional[Dict[str, Any]] = None\n\n\nclass LLMClient(ABC):\n    \"\"\"Abstract interface so ACE can plug in",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im",
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp"
      ],
      "ace_line_counts": [
        48,
        652,
        364,
        155,
        58
      ],
      "auggie_files": [
        "ace/async_adaptation.py",
        "ace/async_adaptation.py"
      ],
      "auggie_contents": [
        "...\n    34\t\n    35\t\n    36\tclass AsyncOfflineAdapter:\n    37\t    \"\"\"Async version of OfflineAdapter for parallel sample processing.\n    38\t\n    39\t    Enables concurrent processing of multiple samples while respecting\n    40\t    max_parallel limits to avoid overwhelming LLM APIs.\n    41\t\n    42\t    Example:\n    43\t        >>> adapter = AsyncOfflineAdapter(playbook, generator, reflector, curator)\n    44\t        >>> results = await adapter.run(samples, environment, epochs=3, max_parallel=5)\n    45\t    \"\"\"\n... (472 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"async queue usage examples with producers and consumers\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    34\t\n    35\t\n    36\tclass AsyncOfflineAdapter:\n    37\t    \"\"\"Async version of OfflineAdapter for parallel sample processing.\n    38\t\n    39\t    Enables concurrent processing of multiple samples while respecting\n    40\t    max_parallel limits to avoid overwhelming LLM APIs.\n    41\t\n    42\t    Example:\n    43\t        >>> adapter = AsyncOfflineAdapter(playbook, generator, reflector, curator)\n    44\t        >>> results = await adapter.run(samples, environment, epochs=3, max_parallel=5)\n    45\t    \"\"\"\n... (514 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 4, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 4"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 4,
      "auggie_expected_rank": -1
    },
    {
      "query": "asyncio.Lock mutex",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "ace/resilience.py",
        "ace/retrieval_optimized.py",
        "ace/typo_correction.py"
      ],
      "ace_scores": [
        0.44744587,
        0.42341194000000004,
        0.41169378,
        0.39240062,
        0.38500179999999995
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "class LLMQueryRewriter:\n    \"\"\"\n    LLM-based query rewriting for short, ambiguous queries.\n    Uses Z.ai GLM to expand short queries into richer semantic variations.\n    \"\"\"\n\n    # Domain context for the ACE memory knowledge base\n    DOMAIN_CONTEXT = \"\"\"The knowledge base contains:\n- User preferences (coding style, tool choices, workflow patterns)\n- Task strategies (debugging approaches, optimization techniques)\n- Error patterns and fixes (common bugs, solutions, root causes)\n- Configuration be",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im"
      ],
      "ace_line_counts": [
        364,
        269,
        149,
        67,
        652
      ],
      "auggie_files": [
        ".serena/memories/ace_mcp_asyncio_fix_2025_01.md"
      ],
      "auggie_contents": [
        "     1\t# ACE MCP Server Async Fix (January 2025)\n     2\t\n     3\t## Problem\n     4\tACE MCP server tool calls (`ace_retrieve`, `ace_store`, `ace_search`) were hanging for 40+ minutes when called via MCP.\n     5\t\n     6\t## Root Cause\n     7\tThe `async def handle_*` functions were calling **blocking synchronous code** directly, which blocked the event loop:\n     8\t- `code_retrieval.search()` - blocking Voyage API call\n     9\t- `index.retrieve()` - blocking Qdrant + embedding call\n    10\t- `index.index_bullet()` - blocking Qdrant call\n    11\t- `indexer.index_workspace()` - blocking filesystem + embedding calls\n    12\t\n    13\t## Solution\n... (532 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "loop.run_in_executor thread",
      "category": "AsyncPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "ace/hyde.py",
        "ace/resilience.py",
        "ace/adaptation.py"
      ],
      "ace_scores": [
        0.531545,
        0.45947412,
        0.44304708,
        0.4392843,
        0.42952374
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"Adaptation loops for offline and online ACE training.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Sequence\n\nif TYPE_CHECKING:\n    from .observability.opik_integration import OpikIntegration\n    from .session_tracking import SessionOutcomeTracker\n\nfrom .playbook import Playbook\nfrom .roles import (\n    Curator,\n    CuratorOut"
      ],
      "ace_line_counts": [
        221,
        155,
        58,
        32,
        91
      ],
      "auggie_files": [
        "ace/integrations/langchain.py",
        "rag_training/optimizations/v7_fortune100_combined.py"
      ],
      "auggie_contents": [
        "...\n   203\t\n   204\t    def invoke(self, input: Any, **kwargs) -> Any:\n   205\t        \"\"\"\n   206\t        Execute runnable with ACE learning (sync).\n   207\t\n   208\t        Args:\n   209\t            input: Input for the runnable (string, dict, etc.)\n   210\t            **kwargs: Additional arguments passed to runnable.invoke()\n   211\t\n   212\t        Returns:\n   213\t            Output from the runnable\n   214\t\n... (517 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"asyncio event loop executor thread pool\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    13\t\n    14\tTarget: >95% Recall@5, >85% Recall@1, MRR >0.90\n    15\t\"\"\"\n    16\t\n    17\timport json\n    18\timport math\n    19\timport os\n    20\timport re\n    21\timport sys\n    22\timport time\n    23\timport hashlib\n    24\timport logging\n... (495 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "from qdrant_client import QdrantClient",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/unified_memory.py",
        "ace/deduplication.py"
      ],
      "ace_files": [
        "ace/deduplication.py",
        "ace/unified_memory.py",
        "ace/unified_memory.py",
        "ace/embedding_finetuning/finetuned_retrieval.py",
        "ace/embedding_finetuning/data_generator.py"
      ],
      "ace_scores": [
        1.04726154,
        1.0430557999999999,
        0.74862655,
        0.7293917,
        0.72091675
      ],
      "ace_contents": [
        "\"\"\"\nAdvanced Memory Deduplication System for RAG.\n\nThis module provides clustering-based deduplication for Qdrant collections:\n- HDBSCAN/DBSCAN clustering for efficient duplicate detection (O(n log n) vs O(n^2))\n- Multi-collection support (ace_memories_hybrid, ace_unified)\n- Multiple merge strategies (keep_best, merge_content, canonical_form)\n- Cluster quality metrics (silhouette score, Davies-Bouldin index)\n- Dry-run mode for safe preview\n\nArchitecture:\n    1. Load memories with embeddings from",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\"",
        "class UnifiedMemoryIndex:\n    \"\"\"\n    Unified memory index using Qdrant with namespace support.\n\n    Provides:\n    - Hybrid search (dense + sparse/BM25)\n    - Namespace filtering\n    - Batch operations\n    - Integration with ACE SmartBulletIndex\n\n    Usage:\n        >>> index = UnifiedMemoryIndex(qdrant_url=\"http://localhost:6333\")\n        >>> index.create_collection()\n        >>> index.index_bullet(bullet)\n        >>> results = index.retrieve(\"query\", namespace=UnifiedNamespace.USER_PREFS)\n    \"",
        "\"\"\"Production retrieval using fine-tuned embeddings.\n\nProvides a drop-in replacement for the baseline retrieval system that uses\nfine-tuned embeddings with fallback to original nomic embeddings.\n\nSupports:\n- Hybrid search (fine-tuned dense + BM25 sparse + RRF fusion)\n- Automatic fallback to baseline embeddings\n- Compatible with existing Qdrant infrastructure\n\"\"\"\n\nimport logging\nfrom typing import Dict, List, Optional\n\nimport httpx\nfrom qdrant_client import QdrantClient\nfrom sentence_transformers",
        "\"\"\"Generate training data from test suite for embedding fine-tuning.\n\nThis module extracts query-memory pairs from the enhanced test suite and generates\nhard negatives by querying the current system for top-K wrong results.\n\nOutput format: JSON with [query, positive, negative1, negative2, ...]\n\"\"\"\n\nimport json\nimport logging\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Set\n\nimport httpx\nfrom qdrant_client import QdrantClient\n\nlog"
      ],
      "ace_line_counts": [
        93,
        118,
        114,
        88,
        32
      ],
      "auggie_files": [
        "ace/qdrant_retrieval.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Vector-based bullet retrieval using Qdrant hybrid search.\n     2\t\n     3\tThis module provides QdrantBulletIndex for O(1) semantic retrieval of playbook\n     4\tbullets using Qdrant vector database with hybrid search (dense + BM25 sparse).\n     5\t\n     6\tPhase 1: Vector Search Integration for ACE Fortune 100 Production Readiness.\n     7\t\n     8\tKey features:\n     9\t- Dense embeddings via LM Studio (nomic-embed-text-v1.5, 768-dim)\n    10\t- BM25 sparse vectors for keyword matching (technical terms)\n    11\t- Hybrid search with RRF fusion for best of both approaches\n    12\t- Seamless integration with existing SmartBulletIndex\n    13\t\"\"\"\n... (542 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "import httpx async",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_retrieval.py",
        "ace/async_retrieval.py",
        "ace/observability/health.py",
        "ace/openai_embeddings.py",
        "ace/gemini_embeddings.py"
      ],
      "ace_scores": [
        0.9901997,
        0.82888973,
        0.72640543,
        0.7077184000000001,
        0.7013266
      ],
      "ace_contents": [
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "    async def retrieve(\n        self,\n        query: str,\n        limit: int = 10,\n        query_type: Optional[str] = None,\n        min_score: float = 0.0,\n    ) -> List[QdrantScoredResult]:\n        \"\"\"Retrieve bullets using hybrid search asynchronously.\n\n        Combines dense semantic search with BM25 keyword matching\n        using Reciprocal Rank Fusion (RRF).\n\n        Args:\n            query: Natural language query\n            limit: Maximum number of results\n            query_type: Optiona",
        "\"\"\"\nHealth Check System for ACE Framework (Phase 3D)\n\nThis module provides health checks for external dependencies (Qdrant, LM Studio).\nTracks component availability, latency, and error states.\n\"\"\"\n\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict, Optional\n\nimport httpx\n\n\nclass HealthStatus(Enum):\n    \"\"\"Health status enumeration.\"\"\"\n    UP = \"up\"\n    DOWN = \"down\"\n\n\n@dataclass\nclass ComponentHealth:\n    \"\"\"\n    Health status for a single component.\n\n ",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n"
      ],
      "ace_line_counts": [
        269,
        106,
        54,
        113,
        88
      ],
      "auggie_files": [
        "debug_httpx_query.py",
        "ace/async_retrieval.py"
      ],
      "auggie_contents": [
        "     1\t#!/usr/bin/env python3\n     2\t\"\"\"Debug the httpx query where Auggie wins.\"\"\"\n     3\timport sys\n     4\tsys.path.insert(0, \".\")\n     5\t\n     6\tfrom ace.code_retrieval import CodeRetrieval\n     7\t\n     8\tr = CodeRetrieval()\n     9\tquery = \"import httpx async\"\n    10\tprint(f\"Query: {query}\")\n    11\tprint(\"-\" * 50)\n    12\t\n    13\tresults = r.search(query, limit=5)\n... (512 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"AsyncClient usage from httpx library\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "     1\t\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n     2\t\n     3\tThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\n     4\tbullets using Qdrant vector database with async operations.\n     5\t\n     6\tPhase 4A: Async Operations for ACE Framework.\n     7\t\n     8\tKey features:\n     9\t- Async embedding retrieval via httpx.AsyncClient\n    10\t- Parallel batch processing with asyncio.gather\n    11\t- Concurrent query handling\n    12\t- Non-blocking Qdrant operations\n    13\t\"\"\"\n... (506 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 5 advantages vs 0",
      "ace_advantages": [
        "Higher rank for expected file (1 vs 2)",
        "More unique files (3 vs 1)",
        "High confidence top score (0.990)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 2
    },
    {
      "query": "from dataclasses import dataclass field",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/playbook.py",
        "ace/config.py"
      ],
      "ace_files": [
        "ace/delta.py",
        "ace/unified_memory.py",
        "ace/roles.py",
        "ace/hyde.py",
        "ace/code_retrieval.py"
      ],
      "ace_scores": [
        0.74445706,
        0.7307433400000001,
        0.7302974,
        0.7233746000000001,
        0.7176313999999999
      ],
      "ace_contents": [
        "\"\"\"Delta operations produced by the ACE Curator.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Iterable, List, Literal, Optional, cast\n\n\nOperationType = Literal[\"ADD\", \"UPDATE\", \"TAG\", \"REMOVE\"]\n\n\n@dataclass\nclass DeltaOperation:\n    \"\"\"Single mutation to apply to the playbook.\n\n    Attributes:\n        type: Operation type (ADD, UPDATE, TAG, REMOVE)\n        section: Section name for the bullet\n        content: Bullet content text (",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\"",
        "\"\"\"Generator, Reflector, and Curator components.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom .delta import DeltaBatch\nfrom .llm import LLMClient\nfrom .playbook import Playbook\nfrom .prompts import CURATOR_PROMPT, GENERATOR_PROMPT, REFLECTOR_PROMPT\n\n# Import Opik tracing with graceful degradation\ntry:\n    from .observability.tracers import maybe_track\nexcept ImportError:",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca"
      ],
      "ace_line_counts": [
        107,
        341,
        115,
        58,
        37
      ],
      "auggie_files": [
        "tests/test_dependency_graph.py"
      ],
      "auggie_contents": [
        "...\n   331\t\n   332\t\n   333\tclass TestDataClasses:\n   334\t    \"\"\"Test the dataclass structures.\"\"\"\n   335\t\n   336\t    def test_import_dataclass(self):\n   337\t        \"\"\"Test Import dataclass structure.\"\"\"\n   338\t        imp = Import(\n   339\t            module=\"os\",\n   340\t            names=[\"path\", \"environ\"],\n   341\t            alias=\"operating_system\",\n   342\t            line_number=5\n... (583 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "High confidence top score (0.744)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "from typing import Optional List Dict",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/"
      ],
      "ace_files": [
        "ace/features.py",
        "ace/delta.py",
        "ace/audit.py",
        "ace/llm.py",
        "ace/llm_providers/litellm_client.py"
      ],
      "ace_scores": [
        0.65301044,
        0.65067657,
        0.6276598999999999,
        0.6247767399999999,
        0.624757
      ],
      "ace_contents": [
        "\"\"\"Centralized optional dependency detection for ACE framework.\n\nThis module provides a clean interface for checking which optional dependencies\nare available, avoiding scattered try/except imports throughout the codebase.\n\nUsage:\n    >>> from ace.features import has_opik, has_litellm, has_langchain\n    >>> if has_opik():\n    ...     from ace.observability import OpikIntegration\n    ...     integration = OpikIntegration()\n\"\"\"\n\nfrom typing import Dict, Optional\n\n\n# Cache for dependency checks to ",
        "\"\"\"Delta operations produced by the ACE Curator.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Iterable, List, Literal, Optional, cast\n\n\nOperationType = Literal[\"ADD\", \"UPDATE\", \"TAG\", \"REMOVE\"]\n\n\n@dataclass\nclass DeltaOperation:\n    \"\"\"Single mutation to apply to the playbook.\n\n    Attributes:\n        type: Operation type (ADD, UPDATE, TAG, REMOVE)\n        section: Section name for the bullet\n        content: Bullet content text (",
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "\"\"\"LLM client abstractions used by ACE components.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nimport json\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Deque, Dict, Optional, Union\n\nif TYPE_CHECKING:\n    import torch\n\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Container for LLM outputs.\"\"\"\n\n    text: str\n    raw: Optional[Dict[str, Any]] = None\n\n\nclass LLMClient(ABC):\n    \"\"\"Abstract interface so ACE can plug in",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L"
      ],
      "ace_line_counts": [
        113,
        107,
        37,
        48,
        65
      ],
      "auggie_files": [
        "ace/pattern_detector.py"
      ],
      "auggie_contents": [
        "...\n    77\t    \n    78\t    # Built-in patterns for common errors\n    79\t    BUILTIN_PATTERNS = [\n    80\t        {\n    81\t            \"pattern_id\": \"null_reference\",\n    82\t            \"regex\": r\"(?:NoneType|null|undefined).*(?:has no attribute|is not|cannot read)\",\n    83\t            \"fix_template\": \"Check if object is None/null before accessing. Use 'if obj is not None:' or optional chaining.\",\n    84\t            \"severity\": \"high\"\n    85\t        },\n    86\t        {\n    87\t            \"pattern_id\": \"import_error\",\n    88\t            \"regex\": r\"(?:ModuleNotFoundError|ImportError).*(?:No module named|cannot import)\",\n... (562 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "Better chunk size (74 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "import voyageai client",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/code_retrieval.py",
        "ace/code_indexer.py",
        "ace/llm_providers/litellm_client.py",
        "ace/__init__.py",
        "ace/config.py"
      ],
      "ace_scores": [
        0.65888616,
        0.59927916,
        0.5002033,
        0.49997234,
        0.49708262
      ],
      "ace_contents": [
        "class CodeRetrieval:\n    \"\"\"Semantic code search with Auggie-style output formatting.\n    \n    Queries indexed code chunks and formats results in a way compatible\n    with Auggie MCP output, supporting blended code + memory results.\n    \n    Uses Voyage-code-3 embeddings (1024d) for optimal code semantic \n    understanding compared to general-purpose embeddings.\n    \n    Configuration via environment variables:\n        QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n        ACE_CO",
        "    def _init_qdrant(self) -> None:\n        \"\"\"Initialize Qdrant client and collection with hybrid search support.\"\"\"\n        try:\n            from qdrant_client import QdrantClient\n            from qdrant_client.models import Distance, VectorParams, SparseVectorParams\n            \n            self._client = QdrantClient(url=self.qdrant_url)\n            \n            # Create collection if not exists - with hybrid vectors (dense + sparse)\n            if not self._client.collection_exists(self.col",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L",
        "\"\"\"Agentic Context Engineering (ACE) reproduction framework.\"\"\"\n\nfrom typing import Optional\nfrom .playbook import Bullet, EnrichedBullet, Playbook, enrich_bullet, migrate_bullet\nfrom .delta import DeltaOperation, DeltaBatch\nfrom .retrieval import SmartBulletIndex, ScoredBullet, IntentClassifier\nfrom .llm import LLMClient, DummyLLMClient, TransformersLLMClient\nfrom .roles import (\n    Generator,\n    ReplayGenerator,\n    Reflector,\n    Curator,\n    GeneratorOutput,\n    ReflectorOutput,\n    Curato",
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s"
      ],
      "ace_line_counts": [
        108,
        191,
        186,
        135,
        117
      ],
      "auggie_files": [
        "ace/integrations/__init__.py",
        "ace/gemini_embeddings.py"
      ],
      "auggie_contents": [
        "...\n    17\t\n    18\tExample:\n    19\t    # LiteLLM (quick start)\n    20\t    from ace.integrations import ACELiteLLM\n    21\t    agent = ACELiteLLM(model=\"gpt-4o-mini\")\n    22\t    answer = agent.ask(\"What is 2+2?\")\n    23\t\n    24\t    # Browser-use\n    25\t    from ace.integrations import ACEAgent\n    26\t    from browser_use import ChatBrowserUse\n    27\t    agent = ACEAgent(llm=ChatBrowserUse())\n    28\t    await agent.run(task=\"Find top HN post\")\n... (515 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"embedding clients, vector database integrations, and similarity search implementations\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "     1\t\"\"\"Gemini Embedding Client for ACE Framework.\n     2\t\n     3\tProvides embeddings using Google's gemini-embedding-001 model\n     4\twith proper task type optimization for retrieval (document vs query).\n     5\t\n     6\tUsage:\n     7\t    from ace.gemini_embeddings import GeminiEmbeddingClient\n     8\t\n     9\t    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n    10\t\n    11\t    # For indexing documents\n    12\t    doc_embedding = client.embed_document(\"This is a document about...\")\n    13\t\n... (520 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "from pathlib import Path",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/"
      ],
      "ace_files": [
        "ace/audit.py",
        "benchmark_ace_vs_auggie.py",
        "ace/gpu_reranker.py",
        "ace/pattern_detector.py",
        "ace/roles.py"
      ],
      "ace_scores": [
        0.9326287600000001,
        0.92825056,
        0.9267074,
        0.9249232999999999,
        0.9176709699999999
      ],
      "ace_contents": [
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "\"\"\"Expanded ACE vs Auggie benchmark comparison.\n\nTests diverse query types across 5 categories:\n1. Code queries (functions, classes, patterns)\n2. Doc queries (guides, references, tutorials)\n3. Architecture queries (design, components, patterns)\n4. Config queries (settings, environment, credentials)\n5. Edge cases (specific symbols, error messages, imports)\n\nRuns ACE CodeRetrieval and optionally compares against Auggie MCP.\n\"\"\"\n\nimport subprocess\nimport sys\nimport os\nimport argparse\nfrom typing im",
        "\"\"\"GPU-Accelerated Cross-Encoder Reranker using ONNX Runtime + DirectML.\n\nSupports AMD GPUs via DirectML on Windows.\n\"\"\"\n\nimport logging\nfrom typing import List, Tuple, Optional\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n# Check for ONNX Runtime with DirectML\ntry:\n    import onnxruntime as ort\n    from transformers import AutoTokenizer\n    ONNX_AVAILABLE = True\n    DML_AVAILABLE = 'DmlExecutionProvider' in ort.get_available_providers()\nexcept ImportError:\n    ONNX_AVAILABLE ",
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef lo",
        "\"\"\"Generator, Reflector, and Curator components.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom .delta import DeltaBatch\nfrom .llm import LLMClient\nfrom .playbook import Playbook\nfrom .prompts import CURATOR_PROMPT, GENERATOR_PROMPT, REFLECTOR_PROMPT\n\n# Import Opik tracing with graceful degradation\ntry:\n    from .observability.tracers import maybe_track\nexcept ImportError:"
      ],
      "ace_line_counts": [
        37,
        120,
        89,
        56,
        115
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "import logging logger",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/audit.py"
      ],
      "ace_files": [
        "ace/audit.py",
        "ace/observability/opik_integration.py",
        "ace/code_retrieval.py",
        "ace/typo_correction.py",
        "ace/adaptation.py"
      ],
      "ace_scores": [
        0.57204,
        0.5409527,
        0.54080683,
        0.52545345,
        0.51880085
      ],
      "ace_contents": [
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "\"\"\"\nOpik Integration for ACE Framework\n\nProvides enterprise-grade observability and tracing for ACE components.\nReplaces custom explainability with production-ready Opik platform.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import asdict\n\nOpikLogger: Optional[type]\n\ntry:\n    import opik\n    from opik import track, opik_context\n\n    OPIK_AVAILABLE = True\n\n    # Try to import LiteLLM Opik",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "\"\"\"Typo correction for ACE framework queries using fuzzy matching.\n\nFeatures:\n- Fast fuzzy matching against technical terms (~1ms)\n- Auto-learning: Remembers user's common typos for instant O(1) lookup\n- Async GLM validation: Background process validates learned corrections\n- Spellchecker validation: Skip LLM for words already in English dictionary\n\"\"\"\n\nimport atexit\nimport difflib\nimport json\nimport logging\nimport os\nimport re\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing im",
        "\"\"\"Adaptation loops for offline and online ACE training.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Sequence\n\nif TYPE_CHECKING:\n    from .observability.opik_integration import OpikIntegration\n    from .session_tracking import SessionOutcomeTracker\n\nfrom .playbook import Playbook\nfrom .roles import (\n    Curator,\n    CuratorOut"
      ],
      "ace_line_counts": [
        130,
        41,
        37,
        37,
        91
      ],
      "auggie_files": [
        "ace/observability/opik_integration.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"\n     2\tOpik Integration for ACE Framework\n     3\t\n     4\tProvides enterprise-grade observability and tracing for ACE components.\n     5\tReplaces custom explainability with production-ready Opik platform.\n     6\t\"\"\"\n     7\t\n     8\tfrom __future__ import annotations\n     9\t\n    10\timport logging\n    11\tfrom datetime import datetime\n    12\tfrom typing import Any, Dict, List, Optional, Union\n    13\tfrom dataclasses import asdict\n... (603 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "from functools import lru_cache",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/caching.py",
        "ace/gemini_embeddings.py"
      ],
      "ace_files": [
        "ace/gemini_embeddings.py",
        "ace/caching.py",
        "ace/retrieval_caching.py",
        "benchmarks/loaders/huggingface.py",
        "ace/pattern_detector.py"
      ],
      "ace_scores": [
        0.8428852,
        0.636595,
        0.62105,
        0.59466726,
        0.5800597
      ],
      "ace_contents": [
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"Response caching for efficient multi-epoch training.\n\nThis module provides caching mechanisms to avoid redundant LLM calls\nduring multi-epoch training, saving time and costs.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom threading import Lock\nfrom typing import Any, Dict, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .llm import LLMClient\n\n\n@datacl",
        "\"\"\"\nRetrieval-specific caching layer for ACE Framework (Phase 4B).\n\nThis module caches RETRIEVAL data (embeddings, query results), NOT LLM responses.\nFor LLM response caching, see ace/caching.py.\n\nCaching Strategy:\n- EmbeddingCache: Text -> embedding vector (768-dim floats)\n- QueryResultCache: Query -> List[QdrantScoredResult] with bullet-aware invalidation\n\nBoth caches use LRU eviction with optional TTL expiration.\n\"\"\"\n\nfrom threading import Lock\nfrom collections import OrderedDict\nfrom datacla",
        "\"\"\"\nHuggingFace datasets loader with streaming support.\n\nThis module provides efficient data loading from HuggingFace Hub using streaming\nto avoid downloading large datasets while supporting caching for repeated access.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom functools import lru_cache\nfrom typing import Dict, Iterator, List, Optional, Any\n\nfrom ..base import DataLoader, get_cache_dir\nfrom ..processors import get_processor\n\n\nclass HuggingFaceLoader(DataLoader):\n    \"\"\"\n    Data l",
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef lo"
      ],
      "ace_line_counts": [
        88,
        146,
        250,
        51,
        56
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "import json os sys",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/"
      ],
      "ace_files": [
        "ace/audit.py",
        "cleanup_and_validate_learned_typos.py",
        "compare_ace_auggie_headtohead.py",
        "ace/self_consistency.py",
        "ace_vs_auggie_headtohead.py"
      ],
      "ace_scores": [
        0.6747151,
        0.66672318,
        0.6528909700000001,
        0.65012982,
        0.6424759600000001
      ],
      "ace_contents": [
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "#!/usr/bin/env python3\n\"\"\"Clean up learned_typos.json by removing bad corrections and cycle mappings.\n\nThis script:\n1. Loads the current learned_typos.json\n2. Removes low-similarity corrections (< 0.70)\n3. Removes cycle mappings (A->B and B->A)\n4. Backs up the original file\n5. Saves the cleaned version\n\"\"\"\n\nimport json\nimport shutil\nimport difflib\nfrom pathlib import Path\nfrom datetime import datetime\n\n",
        "\"\"\"Head-to-head ACE vs Auggie comparison test.\n\nFor each query, we call both systems and compare:\n1. Top file match\n2. Top 3 files overlap\n3. Content relevance\n\"\"\"\nimport subprocess\nimport sys\nimport json\nfrom typing import Dict, List, Any\n\n# Test queries - comprehensive coverage\nTEST_QUERIES = [\n    # Core ACE classes\n    \"EmbeddingConfig class definition dataclass\",\n    \"UnifiedMemoryIndex class Qdrant namespace hybrid search\",\n    \"CodeRetrieval class search method dense vector\",\n    \"ASTChun",
        "\"\"\"Self-consistency sampling for improved generation accuracy.\n\nThis module implements self-consistency decoding, which generates multiple\nresponses for the same prompt and selects the most consistent answer via\nmajority voting. This technique improves accuracy for tasks where reasoning\npaths can vary but the final answer should converge.\n\nReference: Wang et al., \"Self-Consistency Improves Chain of Thought Reasoning\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom collections import Co",
        "#!/usr/bin/env python3\n\"\"\"\nACE vs Auggie MCP Head-to-Head Benchmark.\n\nThis script directly compares ACE CodeRetrieval against Auggie MCP\nusing real-world queries. For each query:\n1. Call ACE code retrieval\n2. Parse Auggie MCP results (provided manually or via MCP call)\n3. Compare files retrieved, rankings, content relevancy\n4. Determine winner based on code context quality\n\nKey metrics:\n- File coverage: Did both systems find the right file?\n- Ranking: Which system ranked the best file higher?\n- "
      ],
      "ace_line_counts": [
        37,
        18,
        44,
        59,
        38
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "from collections import Counter defaultdict",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/"
      ],
      "ace_files": [
        "ace/self_consistency.py",
        "rag_training/scripts/analyze_uniqueness_by_word_count.py",
        "rag_training/test_suite/inspect_samples.py",
        "rag_training/scripts/diagnose_first_words_v2.py",
        "rag_training/scripts/diagnose_first_words_failure.py"
      ],
      "ace_scores": [
        0.53437148,
        0.4547495,
        0.44645905,
        0.42464965,
        0.40694826
      ],
      "ace_contents": [
        "\"\"\"Self-consistency sampling for improved generation accuracy.\n\nThis module implements self-consistency decoding, which generates multiple\nresponses for the same prompt and selects the most consistent answer via\nmajority voting. This technique improves accuracy for tasks where reasoning\npaths can vary but the final answer should converge.\n\nReference: Wang et al., \"Self-Consistency Improves Chain of Thought Reasoning\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom collections import Co",
        "#!/usr/bin/env python3\n\"\"\"Analyze uniqueness of content at different word counts.\"\"\"\n\nimport sys\nimport io\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')\n\nimport httpx\nfrom collections import Counter\n\nQDRANT_URL = \"http://localhost:6333\"\nCOLLECTION = \"ace_memories_hybrid\"\n\n",
        "#!/usr/bin/env python3\n\"\"\"\nQuick inspection utility for test suite samples\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom collections import Counter\n",
        "#!/usr/bin/env python3\n\"\"\"Diagnose why first_words (5 words) fails at 91.5%.\"\"\"\n\nimport sys\nimport io\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')\n\nimport httpx\nimport random\nfrom collections import Counter\n\n\nQDRANT_URL = \"http://localhost:6333\"\nEMBEDDING_URL = \"http://192.168.10.64:1234\"\nCOLLECTION = \"ace_memories_hybrid\"\nMODEL = \"text-embedding-qwen3-embedding-8b\"\n\n\ndef get_all_memories():\n    client = httpx.Client(timeout=60.0)\n    resp = client.post(\n ",
        "#!/usr/bin/env python3\n\"\"\"Diagnostic script to analyze first_words retrieval failure (76% Recall@5).\n\nIdentifies why queries with only first 3 words fail to retrieve their source memory.\n\"\"\"\n\nimport sys\nimport io\n\n# Fix Windows console encoding\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')\n\nimport httpx\nimport random\nimport json\nfrom collections import Counter\n\n\nQDRANT_URL = \"http://localhost:6333\"\nEMBEDDING_URL = \"http://192.168.10.64:1234\"\nCOLLECTION = \"a"
      ],
      "ace_line_counts": [
        26,
        14,
        9,
        60,
        24
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "import asyncio await",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/async_retrieval.py"
      ],
      "ace_files": [
        "ace/async_adaptation.py",
        "ace/async_retrieval.py",
        "ace/hyde.py",
        "ace/integrations/browser_use.py",
        "ace/llm_providers/litellm_client.py"
      ],
      "ace_scores": [
        0.7645865999999999,
        0.74124043,
        0.6986117999999999,
        0.69618373,
        0.6843568
      ],
      "ace_contents": [
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad",
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"HyDE (Hypothetical Document Embeddings) implementation.\n\nThis module implements HyDE for bridging the semantic gap between short queries\nand detailed memory documents. HyDE transforms queries into hypothetical documents\nthat would answer the query, then uses their embeddings for more accurate retrieval.\n\nKey Features:\n- LLM-based hypothetical document generation (Z.ai GLM-4.6 by default)\n- Configurable number of hypothetical documents (default: 3-5)\n- Caching for repeated queries\n- Async supp",
        "\"\"\"\nBrowser-use integration for ACE framework.\n\nThis module provides ACEAgent, a drop-in replacement for browser-use Agent\nthat automatically learns from execution feedback.\n\nThis is the reference implementation for ACE integrations with external agentic\nframeworks. It demonstrates the pattern:\n1. External framework (browser-use) executes task\n2. ACE injects playbook context beforehand\n3. ACE learns from execution afterward (Reflector + Curator)\n\nExample:\n    from ace.integrations import ACEAgen",
        "\"\"\"LiteLLM client for unified access to 100+ LLM providers.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\n\nfrom ..llm import LLMClient, LLMResponse\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import litellm\n    from litellm import completion, acompletion, Router\n\n    LITELLM_AVAILABLE = True\nexcept ImportError:\n    LITELLM_AVAILABLE = False\n    logger.warning(\"L"
      ],
      "ace_line_counts": [
        364,
        269,
        58,
        564,
        65
      ],
      "auggie_files": [
        "comprehensive_ace_auggie_benchmark.py"
      ],
      "auggie_contents": [
        "...\n   396\t    \n   397\t    # ==========================================================================\n   398\t    # Category 5: Async patterns (50 queries)\n   399\t    # ==========================================================================\n   400\t    (\"Async\", \"async def retrieve await\"),\n   401\t    (\"Async\", \"asyncio gather parallel\"),\n   402\t    (\"Async\", \"async with httpx.AsyncClient\"),\n   403\t    (\"Async\", \"async for chunk stream\"),\n   404\t    (\"Async\", \"await embedding generation\"),\n   405\t    (\"Async\", \"async context manager enter exit\"),\n   406\t    (\"Async\", \"asyncio.create_task background\"),\n   407\t    (\"Async\", \"async generator yield\"),\n... (554 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "from abc import ABC abstractmethod",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/"
      ],
      "ace_files": [
        "ace/llm.py",
        "ace/adaptation.py",
        "benchmarks/base.py",
        "ace/adaptation.py",
        "ace/async_adaptation.py"
      ],
      "ace_scores": [
        0.8047393,
        0.7476639,
        0.6141571,
        0.5257806,
        0.52007806
      ],
      "ace_contents": [
        "\"\"\"LLM client abstractions used by ACE components.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nimport json\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Deque, Dict, Optional, Union\n\nif TYPE_CHECKING:\n    import torch\n\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Container for LLM outputs.\"\"\"\n\n    text: str\n    raw: Optional[Dict[str, Any]] = None\n\n\nclass LLMClient(ABC):\n    \"\"\"Abstract interface so ACE can plug in",
        "\"\"\"Adaptation loops for offline and online ACE training.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Sequence\n\nif TYPE_CHECKING:\n    from .observability.opik_integration import OpikIntegration\n    from .session_tracking import SessionOutcomeTracker\n\nfrom .playbook import Playbook\nfrom .roles import (\n    Curator,\n    CuratorOut",
        "\"\"\"\nBase classes and interfaces for benchmark integration.\n\nThis module provides the foundation for configuration-driven benchmark\nevaluation that follows production patterns from lm-evaluation-harness.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterator, List, Optional, Union\n\nfrom ace import Sample, TaskEnvironment, EnvironmentResult\n\n\n@dataclass\nclass BenchmarkC",
        "class AdapterBase:\n    \"\"\"Shared orchestration logic for offline and online ACE adaptation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        playbook: Optional[Playbook] = None,\n        generator: Generator,\n        reflector: Reflector,\n        curator: Curator,\n        max_refinement_rounds: int = 1,\n        reflection_window: int = 3,\n        enable_observability: bool = True,\n        session_tracker: Optional[SessionOutcomeTracker] = None,\n        session_type: Optional[str] = None,\n  ",
        "\"\"\"Async adaptation loops for parallel sample processing.\n\nThis module provides async versions of OfflineAdapter and OnlineAdapter\nthat enable parallel processing of samples for improved throughput.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .adaptation import (\n        OfflineAdapter,\n        OnlineAdapter,\n        Sample,\n        TaskEnvironment,\n        Ad"
      ],
      "ace_line_counts": [
        48,
        91,
        109,
        117,
        33
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "import re regex",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/query_features.py",
        "ace/pattern_detector.py",
        "ace/dependency_graph.py",
        "debug_docs.py",
        "tenant_data/learned_typos.json"
      ],
      "ace_scores": [
        0.52648354,
        0.49635047,
        0.48150015,
        0.47979134,
        0.47646034
      ],
      "ace_contents": [
        "\"\"\"\nQuery Feature Extractor for LinUCB Bandit.\n\nPart of P7 ARIA (Adaptive Retrieval Intelligence Architecture).\n\nExtracts 10-dimension feature vector from queries for contextual bandit routing decisions.\nOptimized for <5ms extraction latency with >90% detection accuracy.\n\nThis module is an original contribution for adapting contextual bandits to RAG retrieval.\nThe feature set was designed empirically for query complexity classification.\n\"\"\"\n\nfrom typing import List\nimport re\n\n",
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef lo",
        "\"\"\"Dependency graph analysis for code understanding.\n\nExtracts imports, function calls, and dependency relationships from source code\nusing tree-sitter for multiple programming languages.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nimport re\n\n\n@dataclass\nclass Import:\n    \"\"\"Represents an import statement in source code.\"\"\"\n\n    module: str\n    names: List[str] = field(default_factory=list)\n    alias: Optional[str] = None\n    lin",
        "import os\nimport re\n\n# Simulate the actual function\nfile_path = \"docs/INTEGRATION_GUIDE.md\"\nquery = \"try except error handling pattern\"\n\nstop_words = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'how', 'what',\n              'where', 'when', 'why', 'can', 'will', 'method', 'function', 'class',\n              'code', 'file', 'def', 'implementation', 'search', 'find', 'get', 'set',\n              'pattern', 'error', 'handling', 'import', 'logging', 'logger', 'setup',\n              'exception",
        "    \"matching\": \"marching\",\n    \"full\": \"fully\",\n    \"skills\": \"skill\",\n    \"non\": \"none\",\n    \"wanting\": \"wandering\",\n    \"failing\": \"falling\",\n    \"bank\": \"ban\",\n    \"lost\": \"loss\",\n    \"about\": \"abou\",\n    \"engine\": \"engineer\",\n    \"node\": \"mode\",\n    \"broton\": \"proton\",\n    \"scraperepg\": \"scrapeepg\",\n    \"usr\": \"user\",\n    \"dbconfig\": \"config\",\n    \"\\u591a\\u5b50\\u4efb\\u52a1\": \"\\u591a\\u4efb\\u52a1\",\n    \"coderetrieval\": \"retrieval\",\n    \"qdrant_\": \"qdrant\",\n    \"auggie\": \"augie\",\n    \"namespac"
      ],
      "ace_line_counts": [
        16,
        166,
        29,
        49,
        33
      ],
      "auggie_files": [
        "ace/pattern_detector.py"
      ],
      "auggie_contents": [
        "...\n    77\t    \n    78\t    # Built-in patterns for common errors\n    79\t    BUILTIN_PATTERNS = [\n    80\t        {\n    81\t            \"pattern_id\": \"null_reference\",\n    82\t            \"regex\": r\"(?:NoneType|null|undefined).*(?:has no attribute|is not|cannot read)\",\n    83\t            \"fix_template\": \"Check if object is None/null before accessing. Use 'if obj is not None:' or optional chaining.\",\n    84\t            \"severity\": \"high\"\n    85\t        },\n    86\t        {\n    87\t            \"pattern_id\": \"import_error\",\n    88\t            \"regex\": r\"(?:ModuleNotFoundError|ImportError).*(?:No module named|cannot import)\",\n... (471 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "Better chunk size (59 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "from datetime import datetime",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/"
      ],
      "ace_files": [
        "ace/deduplication.py",
        "ace/audit.py",
        "ace/playbook.py",
        "ace/retrieval.py",
        "ace/observability/opik_integration.py"
      ],
      "ace_scores": [
        0.98,
        0.9783583,
        0.91671367,
        0.91340507,
        0.9086573499999999
      ],
      "ace_contents": [
        "\"\"\"\nAdvanced Memory Deduplication System for RAG.\n\nThis module provides clustering-based deduplication for Qdrant collections:\n- HDBSCAN/DBSCAN clustering for efficient duplicate detection (O(n log n) vs O(n^2))\n- Multi-collection support (ace_memories_hybrid, ace_unified)\n- Multiple merge strategies (keep_best, merge_content, canonical_form)\n- Cluster quality metrics (silhouette score, Davies-Bouldin index)\n- Dry-run mode for safe preview\n\nArchitecture:\n    1. Load memories with embeddings from",
        "\"\"\"Enterprise audit logging for ACE operations.\n\nProvides comprehensive logging of:\n- Retrieval operations (queries, latency, results)\n- Index operations (bullet creation, updates)\n- Playbook operations (loading, saving)\n\nLogs are written to daily JSONL files for efficient storage and analysis.\n\"\"\"\n\nimport csv\nimport json\nimport uuid\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass AuditEnt",
        "\"\"\"Playbook storage and mutation logic for ACE.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom dataclasses import asdict, dataclass, field\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Union, cast\n\nfrom .delta import DeltaBatch, DeltaOperation\n\n\n# Phase 1C: Asymmetric penalty weights for bullet tagging\n# Harmful tags penalized 2x to suppress bad strategies faster\nPENALTY_WEIGHTS = {\"helpful\": 1, \"harmf",
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "\"\"\"\nOpik Integration for ACE Framework\n\nProvides enterprise-grade observability and tracing for ACE components.\nReplaces custom explainability with production-ready Opik platform.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import asdict\n\nOpikLogger: Optional[type]\n\ntry:\n    import opik\n    from opik import track, opik_context\n\n    OPIK_AVAILABLE = True\n\n    # Try to import LiteLLM Opik"
      ],
      "ace_line_counts": [
        93,
        37,
        108,
        61,
        41
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "from enum import Enum auto",
      "category": "ImportPatterns",
      "expected_files": [
        "ace/"
      ],
      "ace_files": [
        "ace/retrieval_presets.py",
        "ace/structured_enhancer.py",
        "ace/resilience.py",
        "ace/unified_memory.py",
        "fibonacci.py"
      ],
      "ace_scores": [
        0.5932056499999999,
        0.5733043999999999,
        0.56171878,
        0.55579232,
        0.55421288
      ],
      "ace_contents": [
        "\"\"\"\nRetrieval Presets - Optimized configurations for different query types.\n\nBased on empirical testing:\n- Baseline precision: 75.6%\n- Architecture queries: 33.3% (worst)\n- Target: 95%+ precision across all categories\n\nWinning optimizations:\n1. BM25-heavy weighting (dense=0.3, sparse=0.7) -> +50% P@3\n2. Post-retrieval deduplication (0.90 threshold) -> +2.7%\n3. Query expansion with domain synonyms -> +3% (conditional)\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing",
        "#!/usr/bin/env python\n\"\"\"\nStructured Query Enhancer based on .enhancedprompt.md methodology.\n\nEXHAUSTIVE IMPLEMENTATION - Covers ALL software engineering domains.\n\nTransforms vague user queries into structured, actionable prompts using:\n1. Intent Classification (ANALYTICAL/IMPLEMENTATION/TROUBLESHOOTING/EXPLORATORY/LEARNING/REFACTORING)\n2. Domain Detection (40+ domains covering all software engineering areas)\n3. Context Expansion with domain-specific terminology (1000+ expansion terms)\n4. Query ",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass CircuitState(str, Enum):\n    \"\"\"Circuit breaker states.\"\"\"\n\n    CLOSED = \"clos",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\"",
        "\"\"\"\nFibonacci Number Calculator\n\nA high-performance, production-ready implementation for calculating Fibonacci numbers\nwith comprehensive error handling, multiple algorithm options, and extensive documentation.\n\nAuthor: Elite Software Engineer\nVersion: 1.0.0\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Union, Optional, Callable, Any\nfrom enum import Enum\n\n\nclass FibonacciAlgorithm(Enum):\n    \"\"\"Enumeration of available Fibonacci calculation algorithms.\"\"\"\n    ITER"
      ],
      "ace_line_counts": [
        58,
        132,
        149,
        341,
        157
      ],
      "auggie_files": [
        "debug_httpx_query.py"
      ],
      "auggie_contents": [
        "...\n    16\t\n    17\t# Check what files actually import httpx\n    18\tprint(\"\\n\" + \"=\" * 50)\n    19\tprint(\"Files that import httpx:\")\n    20\timport os\n    21\tfrom pathlib import Path\n    22\t\n    23\tfor root, dirs, files in os.walk(\"ace\"):\n    24\t    dirs[:] = [d for d in dirs if not d.startswith((\"__\", \".\"))]\n    25\t    for f in files:\n    26\t        if f.endswith(\".py\"):\n    27\t            fp = Path(root) / f\n... (478 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 1, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 1"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "README installation guide",
      "category": "DocumentationPatterns",
      "expected_files": [
        "README.md"
      ],
      "ace_files": [
        "docs/SETUP_GUIDE.md",
        "examples/README.md",
        "docs/SETUP_GUIDE.md",
        "examples/langchain/README.md",
        "README.md"
      ],
      "ace_scores": [
        0.82388917,
        0.7957844999999999,
        0.79287938,
        0.76405374,
        0.76323107
      ],
      "ace_contents": [
        "\n**Option 2: Local Binary**\nDownload from [qdrant.tech/documentation/quick-start](https://qdrant.tech/documentation/quick-start/)\n\n**Verify Connection:**\n```python\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(url=\"http://localhost:6333\")\nprint(client.get_collections())  # Should list collections\n```\n\n---\n\n## Troubleshooting\n\n### Import Errors\n\n```bash\n# Upgrade to latest version\npip install --upgrade ace-framework\n\n# Check installation\npip show ace-framework\n```\n\n### API Key Not",
        "# ACE Framework Examples\n\nNavigation guide for all ACE examples. Each directory has its own detailed README.\n\n*Last updated: 2025-01-15*\n\n## Getting Started\n\n**New to ACE?** Start with these:\n\n- **[simple_ace_example.py](simple_ace_example.py)** - Minimal ACE usage (5 minutes)\n- **[seahorse_emoji_ace.py](seahorse_emoji_ace.py)** - Self-reflection demo\n- **[Quick Start Guide](../docs/QUICK_START.md)** - Step-by-step tutorial\n\n## Quick Start Templates\n\nCopy-paste starting points for different prov",
        "# ACE Framework Setup Guide\n\nQuick setup and configuration guide for ACE Framework.\n\n*Last updated: 2025-01-15*\n\n## Requirements\n\n- **Python 3.11 or higher**\n- API key for your LLM provider (Z.ai GLM, OpenAI, Anthropic, Google, etc.)\n\nCheck Python version:\n```bash\npython --version  # Should show 3.11+\n```\n\n---\n\n## Installation\n\n### For Users\n\n```bash\n# Basic installation\npip install ace-framework\n\n# With optional features\npip install ace-framework[observability]  # Opik monitoring + cost trackin",
        "ace_chain.enable_learning()\nresult = ace_chain.invoke(\"real task\")\n```\n\n## Common Patterns\n\n### Basic Chain\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom ace.integrations import ACELangChain\n\n# Create chain\nprompt = ChatPromptTemplate.from_template(\"Answer: {input}\")\nchain = prompt | ChatOpenAI(temperature=0)\n\n# Wrap with ACE\nace_chain = ACELangChain(runnable=chain)\n\n# Use it\nresult = ace_chain.invoke({\"input\": \"What is ACE?\"})\n```\n",
        "\n# Prometheus metrics\nwith track_latency(operation=\"retrieval\", tenant_id=\"tenant-123\"):\n    results = index.retrieve(query)\n\n# Health checks\nchecker = HealthChecker(qdrant_url=\"http://localhost:6333\")\nstatus = checker.check_all()  # {\"qdrant\": {\"healthy\": True, \"latency_ms\": 5.2}}\n\n# OpenTelemetry distributed tracing\n@trace_operation(\"search\")\ndef search_strategies(query):\n    return index.retrieve(query)\n```\n\n**[Full Enterprise Guide](docs/Fortune100.md)**\n\n---\n\n## Configuration\n\nACE works wit"
      ],
      "ace_line_counts": [
        73,
        127,
        120,
        99,
        120
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "API reference documentation",
      "category": "DocumentationPatterns",
      "expected_files": [
        "docs/API_REFERENCE.md"
      ],
      "ace_files": [
        "docs/MCP_INTEGRATION.md",
        "docs/API_REFERENCE.md",
        "ace/code_retrieval.py",
        "docs/API_REFERENCE.md",
        "docs/PROMPT_ENGINEERING.md"
      ],
      "ace_scores": [
        0.65303626,
        0.64833053,
        0.6385139200000001,
        0.63471165,
        0.63414928
      ],
      "ace_contents": [
        "```bash\npytest tests/test_ace_mcp_server.py -v\n```\n\nManual testing:\n\n```python\nfrom ace_mcp_server import handle_retrieve, handle_store\nimport asyncio\n\n# Test retrieve\nresult = asyncio.run(handle_retrieve({\"query\": \"coding preferences\", \"limit\": 3}))\nprint(result[0].text)\n\n# Test store\nresult = asyncio.run(handle_store({\"content\": \"Test memory\", \"category\": \"DEBUGGING\"}))\nprint(result[0].text)\n```\n\n## Integration with Copilot Instructions\n\nAdd to your `copilot-instructions.md`:\n\n```markdown\n# AC",
        "# \ud83d\udcda ACE Framework API Reference\n\nComplete API documentation for the ACE Framework.\n\n## Configuration\n\n### Centralized Configuration (ace/config.py)\n\n**All embedding and Qdrant configuration is centralized** in `ace/config.py` using dataclasses with environment variable support.\n\n#### EmbeddingConfig\n\n```python\nfrom ace.config import EmbeddingConfig\n\n# Use defaults\nconfig = EmbeddingConfig()\n# config.url = \"http://192.168.10.64:1234\"\n# config.model = \"qwen/qwen3-embedding-8b\"\n# config.dimension =",
        "    def _apply_filename_boost(self, query: str, file_path: str, score: float, content: str = \"\") -> float:\n        \"\"\"\n        Apply filename and content boost when query terms match file path or definitions.\n        \n        This mimics Auggie MCP's behavior where files with names matching\n        query terms OR containing class/function definitions get prioritized.\n        \n        Args:\n            query: Original search query\n            file_path: File path being scored\n            score: O",
        "    shard_strategy=ShardStrategy.TENANT\n)\nsharded.index_bullet(bullet, tenant_id=\"acme_corp\")\nresults = sharded.retrieve(\"query\", tenant_id=\"acme_corp\")\n\n# Clustered Qdrant\ncluster = QdrantCluster(\n    nodes=[\"http://node1:6333\", \"http://node2:6333\"],\n    strategy=LoadBalancingStrategy.LEAST_CONNECTIONS,\n    max_consecutive_failures=3\n)\nresults = cluster.retrieve(\"query\")\n\n# Health monitoring\nhealth_checker = ClusterHealthCheck(cluster)\nstatus = health_checker.check_all_nodes()\n```\n\n### Code Ana",
        "\n## Resources\n\n- [Original ACE Paper](https://arxiv.org/abs/2510.04618)\n- [Prompt Engineering Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n- [LiteLLM Documentation](https://docs.litellm.ai/)\n- [Example Implementations](../examples/) - See examples directory for prompt engineering patterns\n\n## Contributing\n\nWhen contributing new prompts:\n\n1. Follow the v2 template structure\n2. Include at least 2 good/bad examples\n3. Add domain-specific optimizations if applicable\n4."
      ],
      "ace_line_counts": [
        320,
        120,
        530,
        220,
        22
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "configuration options docs",
      "category": "DocumentationPatterns",
      "expected_files": [
        "docs/"
      ],
      "ace_files": [
        "docs/API_REFERENCE.md",
        "docs/CONTEXT_ENGINE_INTEGRATION.md",
        "docs/QUERY_CLASSIFIER.md",
        "ace/embedding_finetuning/models/ace_finetuned/README.md",
        "rag_training/training_data/crossencoder_training_pairs.json"
      ],
      "ace_scores": [
        0.8225123299999999,
        0.8180175199999999,
        0.7871509999999999,
        0.63867148,
        0.6330355
      ],
      "ace_contents": [
        "# \ud83d\udcda ACE Framework API Reference\n\nComplete API documentation for the ACE Framework.\n\n## Configuration\n\n### Centralized Configuration (ace/config.py)\n\n**All embedding and Qdrant configuration is centralized** in `ace/config.py` using dataclasses with environment variable support.\n\n#### EmbeddingConfig\n\n```python\nfrom ace.config import EmbeddingConfig\n\n# Use defaults\nconfig = EmbeddingConfig()\n# config.url = \"http://192.168.10.64:1234\"\n# config.model = \"qwen/qwen3-embedding-8b\"\n# config.dimension =",
        "### 5.1 ACE Configuration Extension\n\n```python\n# ace/config.py additions\n\n@dataclass\nclass ContextEngineConfig:\n    \"\"\"Configuration for Context Engine integration.\"\"\"\n    \n    # Enable/disable toggle\n    enabled: bool = False\n    \n    # Connection settings\n    base_url: str = \"http://localhost:8003\"\n    timeout: float = 30.0\n    \n    # Search behavior\n    default_limit: int = 10\n    languages: List[str] = field(default_factory=list)  # Empty = all languages\n    file_patterns: List[str] = field(",
        "**Use case**: Optimal balance of cost, latency, and quality\n\n**Benefits**:\n- 60-80% reduction in LLM calls\n- 10-20x faster for technical queries\n- ~70% cost savings\n- Maintained quality\n\n#### Debugging\n\n```python\nconfig.enable_query_classifier = False\n```\n\n**Use case**: Consistent LLM behavior for all queries\n\n**Benefits**:\n- Predictable behavior\n- Easier to debug LLM-related issues\n- Useful for A/B testing\n\n#### Research\n\n```python\nconfig.enable_query_classifier = True\nconfig.technical_terms_by",
        "- `max_steps`: -1\n- `lr_scheduler_type`: linear\n- `lr_scheduler_kwargs`: {}\n- `warmup_ratio`: 0.0\n- `warmup_steps`: 0\n- `log_level`: passive\n- `log_level_replica`: warning\n- `log_on_each_node`: True\n- `logging_nan_inf_filter`: True\n- `save_safetensors`: True\n- `save_on_each_node`: False\n- `save_only_model`: False\n- `restore_callback_states_from_checkpoint`: False\n- `no_cuda`: False\n- `use_cpu`: False\n- `use_mps_device`: False\n- `seed`: 42\n- `data_seed`: None\n- `jit_mode_eval`: False\n- `bf16`: Fa",
        "  },\n  {\n    \"query\": \"how to implement\",\n    \"memory\": \"Document all config options with commented examples\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"how to implement\",\n    \"memory\": \"Implement centralized ToC for standardized service docs.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"how to implement\",\n    \"memory\": \"Atomically replace files, then verify writes.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"how to implement\",\n    \"memory\": \"Implement Prisma middleware to log all data access and modificatio"
      ],
      "ace_line_counts": [
        120,
        120,
        120,
        120,
        420
      ],
      "auggie_files": [
        "docs/QUICK_START.md"
      ],
      "auggie_contents": [
        "...\n    93\t\n    94\t\n    95\t# Initialize LLM client\n    96\tclient = LiteLLMClient(model=\"gpt-4o-mini\")\n    97\t\n    98\t# Create ACE components (three roles)\n    99\tgenerator = Generator(client)  # Produces answers\n   100\treflector = Reflector(client)  # Analyzes performance\n   101\tcurator = Curator(client)      # Updates playbook\n   102\t\n   103\t# Create adapter to orchestrate everything\n   104\tadapter = OfflineAdapter(generator=generator, reflector=reflector, curator=curator)\n... (475 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \"ace\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        63
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (5 vs 1)",
        "High confidence top score (0.823)"
      ],
      "auggie_advantages": [
        "Better chunk size (63 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "quickstart tutorial",
      "category": "DocumentationPatterns",
      "expected_files": [
        "QUICKSTART_CLAUDE_CODE.md"
      ],
      "ace_files": [
        "QUICKSTART_CLAUDE_CODE.md",
        "docs/GOLDEN_RULES_QUICKSTART.md",
        "docs/QUICK_START.md",
        "examples/browser-use/form-filler/task2_form.txt",
        "examples/litellm/README.md"
      ],
      "ace_scores": [
        0.8677149,
        0.81037745,
        0.711063,
        0.6773461000000001,
        0.67441037
      ],
      "ace_contents": [
        "# Quick Start: ACE with Claude Code CLI\n\nGet automatic learning from Claude Code CLI in **2 minutes**.\n\n## \u26a1 Installation\n\n```powershell\n# 1. Install ACE framework\npip install ace-framework\n\n# 2. Set your API key\n$env:OPENAI_API_KEY = \"your-key-here\"\n```\n\nThat's it! The hooks are already configured in `.claude/settings.json`.\n\n## \ud83c\udfaf How to Use\n\n### Just Start Claude Code CLI Normally!\n\n```powershell\n# That's literally it\nclaude\n```\n\nThe ACE hooks run automatically:\n- \u2705 **SessionStart**: Loads lea",
        "# Golden Rules Quick Start\n\n## 5-Minute Setup\n\n### 1. Enable the Feature\n\nAdd to `.env` file:\n```bash\nACE_GOLDEN_RULES=True\nACE_GOLDEN_THRESHOLD=10\nACE_GOLDEN_MAX_HARMFUL=0\nACE_GOLDEN_DEMOTION_HARMFUL=3\n```\n\n### 2. Basic Usage\n\n```python\nfrom ace.playbook import Playbook\nfrom ace.config import get_elf_config\n\n# Create playbook\nplaybook = Playbook()\n\n# Add bullets (simulating feedback)\nb1 = playbook.add_bullet(\"strategies\", \"Validate input\", metadata={\"helpful\": 15, \"harmful\": 0})\nb2 = playbook.a",
        "# ACE Framework Quick Start\n\nGet your first self-learning AI agent running!\n\n*Last updated: 2025-01-15*\n\n---\n\n## Simple Quickstart (5 minutes)\n\nThe fastest way to get started with ACE.\n\n### Step 1: Install\n\n```bash\npip install ace-framework\n```\n\n### Step 2: Set API Key\n\n**ACE uses Z.ai GLM-4.6 by default** (fastest, most cost-effective):\n\n```bash\nexport ZAI_API_KEY=\"your-zai-api-key\"\n```\n\nOr use OpenAI instead:\n```bash\nexport OPENAI_API_KEY=\"your-key-here\"\n```\n\n### Step 3: Create `my_first_ace.p",
        "Go to the following website:\nhttp://127.0.0.1:8765/form.html\n\nSign up for an account:\n- First Name: Miguel\n- Last Name: Martinez\n- Email: miguel.martinez@example.com\n\nThen click the submit button. Once the form is submitted successfully you can stop.",
        "# ACELiteLLM Examples\n\nSimple examples showing how to use ACELiteLLM for quick learning agents.\n\n## What is ACELiteLLM?\n\nACELiteLLM is the simplest way to add learning to any LLM:\n\n```python\nfrom ace.integrations import ACELiteLLM\nfrom ace import Sample, SimpleEnvironment\n\n# Create agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\")\n\n# Ask questions\nanswer = agent.ask(\"What is Python?\")\n\n# Learn from examples\nsamples = [Sample(question=\"What is Python?\", ground_truth=\"A programming language\")]\nagent.l"
      ],
      "ace_line_counts": [
        114,
        119,
        220,
        9,
        94
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "contributing guidelines",
      "category": "DocumentationPatterns",
      "expected_files": [
        "CONTRIBUTING.md"
      ],
      "ace_files": [
        "CONTRIBUTING.md",
        "docs/PROMPT_ENGINEERING.md",
        "benchmarks/ACE_RETRIEVAL_BENCHMARK.md",
        "ace/embedding_finetuning/models/ace_finetuned/vocab.txt",
        "ace/prompts.py"
      ],
      "ace_scores": [
        0.9181123999999999,
        0.76041176,
        0.6905641,
        0.63323098,
        0.49701318
      ],
      "ace_contents": [
        "# Contributing to ACE Framework\n\nThank you for your interest in contributing to the Agentic Context Engine! We welcome contributions from the community.\n\n## How to Contribute\n\n### Reporting Bugs\n\nBefore creating bug reports, please check existing issues to avoid duplicates. When creating a bug report, include:\n\n- A clear and descriptive title\n- Steps to reproduce the issue\n- Expected behavior vs actual behavior\n- Environment details (OS, Python version, package versions)\n- Any relevant error mes",
        "\n## Resources\n\n- [Original ACE Paper](https://arxiv.org/abs/2510.04618)\n- [Prompt Engineering Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n- [LiteLLM Documentation](https://docs.litellm.ai/)\n- [Example Implementations](../examples/) - See examples directory for prompt engineering patterns\n\n## Contributing\n\nWhen contributing new prompts:\n\n1. Follow the v2 template structure\n2. Include at least 2 good/bad examples\n3. Add domain-specific optimizations if applicable\n4.",
        "| `benchmarks/data/representative.json` | 50 normal test cases |\n| `benchmarks/data/adversarial.json` | 50 challenging test cases |\n| `tests/test_ace_retrieval_benchmark.py` | Test suite for benchmark |\n| `benchmarks/ACE_RETRIEVAL_BENCHMARK.md` | This documentation |\n\n## Contributing\n\nWhen modifying the benchmark:\n\n1. **Follow TDD**: Write tests FIRST, then implementation\n2. **Update both datasets**: Keep representative and adversarial balanced (50 each)\n3. **Document changes**: Update this READ",
        "objects\ncontributed\ndragon\ndouglas\naware\nsenator\njohnny\njordan\nsisters\nengines\nflag\ninvestment\nsamuel\nshock\ncapable\nclark\nrow\nwheel\nrefers\nsession\nfamiliar\nbiggest\nwins\nhate\nmaintained\ndrove\nhamilton\nrequest\nexpressed\ninjured\nunderground\nchurches\nwalker\nwars\ntunnel\npasses\nstupid\nagriculture\nsoftly\ncabinet\nregarded\njoining\nindiana\n##ea\n##ms\npush\ndates\nspend\nbehavior\nwoods\nprotein\ngently\nchase\nmorgan\nmention\nburning\nwake\ncombination\noccur\nmirror\nleads\njimmy\nindeed\nimpossible\nsingapore\npaintings\nco",
        "{playbook_excerpt}\n\nReturn JSON:\n{{\n  \"reasoning\": \"<analysis>\",\n  \"error_identification\": \"<what went wrong>\",\n  \"root_cause_analysis\": \"<why it happened>\",\n  \"correct_approach\": \"<what should be done>\",\n  \"key_insight\": \"<reusable takeaway>\",\n  \"bullet_tags\": [\n    {{\"id\": \"<bullet-id>\", \"tag\": \"helpful|harmful|neutral\"}}\n  ]\n}}\n\"\"\"\n\n\n# Default Curator prompt - updates playbook based on reflections\nCURATOR_PROMPT = \"\"\"\\\nYou are the curator of the ACE playbook. Merge the latest reflection into "
      ],
      "ace_line_counts": [
        109,
        22,
        22,
        120,
        50
      ],
      "auggie_files": [
        "CONTRIBUTING.md"
      ],
      "auggie_contents": [
        "     1\t# Contributing to ACE Framework\n     2\t\n     3\tThank you for your interest in contributing to the Agentic Context Engine! We welcome contributions from the community.\n     4\t\n     5\t## How to Contribute\n     6\t\n     7\t### Reporting Bugs\n     8\t\n     9\tBefore creating bug reports, please check existing issues to avoid duplicates. When creating a bug report, include:\n    10\t\n    11\t- A clear and descriptive title\n    12\t- Steps to reproduce the issue\n    13\t- Expected behavior vs actual behavior\n... (594 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 3 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.918)",
        "Better chunk size (65 lines avg)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "changelog version history",
      "category": "DocumentationPatterns",
      "expected_files": [
        "CHANGELOG.md"
      ],
      "ace_files": [
        "CHANGELOG.md",
        "CHANGELOG.md",
        "CHANGELOG.md",
        "ace/embedding_finetuning/models/ace_finetuned/vocab.txt",
        "ace/unified_memory.py"
      ],
      "ace_scores": [
        0.8462270999999999,
        0.84382168,
        0.77386883,
        0.61655983,
        0.6084385
      ],
      "ace_contents": [
        "# Changelog\n\nAll notable changes to ACE Framework will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Fixed - 2025-01-06\n\n- **Missing voyageai Dependency** - Code retrieval was silently failing due to missing package\n  - **Root Cause**: `voyageai` Python package was not installed in venv, causing ImportError in `ace/code_retr",
        "  - Removed `UNIFIED_AVAILABLE` conditional checks\n  - Removed fallback to `ace_qdrant_memory` in hooks\n  - Removed dual-write logic (was writing to both old and new collections)\n  - Removed `[LEGACY]` output indicator from session start\n\n### Performance\n- **50% faster memory stores** - Single write instead of dual-write\n- **50% faster memory searches** - Single retrieval instead of merge\n- **57% code reduction** - `ace_qdrant_memory.py`: 1,307 -> 560 lines\n- **Deduplication savings** - 112 dupl",
        "  - 14 replacements across 8 core modules: `hyde.py`, `async_retrieval.py`, `hyde_retrieval.py`, `qdrant_retrieval.py`, `retrieval_optimized.py`, `roles.py`, `unified_memory.py`\n  - Used for point ID generation and BM25 term hashing (non-cryptographic but security-consistent)\n  - Zero MD5 usages remaining in `ace/` module\n\n- **HTTP Security Headers Documentation** - Added production deployment security guidance\n  - CSP, X-Frame-Options, HSTS, X-Content-Type-Options headers documented\n  - Nginx a",
        "very\nalbum\ntake\nend\ngood\ntoo\nfollowing\nreleased\ngame\nplayed\nlittle\nbegan\ndistrict\n##m\nold\nwant\nthose\nside\nheld\nown\nearly\ncounty\nll\nleague\nuse\nwest\n##u\nface\nthink\n##es\n2010\ngovernment\n##h\nmarch\ncame\nsmall\ngeneral\ntown\njune\n##on\nline\nbased\nsomething\n##k\nseptember\nthought\nlooked\nalong\ninternational\n2011\nair\njuly\nclub\nwent\njanuary\noctober\nour\naugust\napril\nyork\n12\nfew\n2012\n2008\neast\nshow\nmember\ncollege\n2009\nfather\npublic\n##us\ncome\nmen\nfive\nset\nstation\nchurch\n##c\nnext\nformer\nnovember\nroom\nparty\nlocate",
        "    def update_bullet(\n        self,\n        bullet_id: str,\n        content: Optional[str] = None,\n        **kwargs\n    ) -> Optional[UnifiedBullet]:\n        \"\"\"\n        Create a new version of a bullet, marking the old as inactive.\n\n        Implements soft-delete version history. The old bullet is marked\n        is_active=False with superseded_at timestamp, and a new bullet\n        is created with incremented version number.\n\n        Requires ACE_VERSION_HISTORY=true (default: enabled).\n\n     "
      ],
      "ace_line_counts": [
        220,
        232,
        120,
        220,
        199
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "integration guide howto",
      "category": "DocumentationPatterns",
      "expected_files": [
        "docs/INTEGRATION_GUIDE.md"
      ],
      "ace_files": [
        "ace/integrations/base.py",
        "tests/integrations/__init__.py",
        "debug_docs.py",
        "ace/integrations/litellm.py",
        "ace/integrations/browser_use.py"
      ],
      "ace_scores": [
        0.6612722,
        0.4925442,
        0.47900772,
        0.46259242,
        0.4589994
      ],
      "ace_contents": [
        "\"\"\"\nBase classes and utilities for ACE integrations with external agentic frameworks.\n\nThis module provides the foundation for integrating ACE learning capabilities\nwith external agentic systems like browser-use, LangChain, CrewAI, and custom agents.\n\n## When to Use Integrations vs Full ACE Pipeline\n\n### Use INTEGRATIONS (this module) when:\n- You have an existing agentic system (browser-use, LangChain, custom agent)\n- The external agent handles task execution\n- You want ACE to learn from that ag",
        "\"\"\"Tests for ACE integrations with external agentic frameworks.\"\"\"\n",
        "import os\nimport re\n\n# Simulate the actual function\nfile_path = \"docs/INTEGRATION_GUIDE.md\"\nquery = \"try except error handling pattern\"\n\nstop_words = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'how', 'what',\n              'where', 'when', 'why', 'can', 'will', 'method', 'function', 'class',\n              'code', 'file', 'def', 'implementation', 'search', 'find', 'get', 'set',\n              'pattern', 'error', 'handling', 'import', 'logging', 'logger', 'setup',\n              'exception",
        "\"\"\"\nACE + LiteLLM integration for quick-start learning agents.\n\nThis module provides ACELiteLLM, a high-level wrapper bundling ACE learning\nwith LiteLLM for easy prototyping and simple tasks.\n\nWhen to Use ACELiteLLM:\n- Quick start: Want to try ACE with minimal setup\n- Simple tasks: Q&A, classification, reasoning\n- Prototyping: Experimenting with ACE learning\n- No framework needed: Direct LLM usage with learning\n\nWhen NOT to Use ACELiteLLM:\n- Browser automation \u2192 Use ACEAgent (browser-use)\n- Lang",
        "\"\"\"\nBrowser-use integration for ACE framework.\n\nThis module provides ACEAgent, a drop-in replacement for browser-use Agent\nthat automatically learns from execution feedback.\n\nThis is the reference implementation for ACE integrations with external agentic\nframeworks. It demonstrates the pattern:\n1. External framework (browser-use) executes task\n2. ACE injects playbook context beforehand\n3. ACE learns from execution afterward (Reflector + Curator)\n\nExample:\n    from ace.integrations import ACEAgen"
      ],
      "ace_line_counts": [
        190,
        2,
        49,
        49,
        41
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "embedding config documentation",
      "category": "DocumentationPatterns",
      "expected_files": [
        "docs/CODE_EMBEDDING_CONFIG.md"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/config.py",
        "ace/gemini_embeddings.py",
        "ace/openai_embeddings.py",
        "docs/API_REFERENCE.md"
      ],
      "ace_scores": [
        1.1501648,
        0.81169504,
        0.7120071000000001,
        0.68858592,
        0.6755220000000001
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "def get_config() -> ACEConfig:\n    \"\"\"Get the global ACE configuration singleton.\"\"\"\n    global _config\n    if _config is None:\n        _config = ACEConfig()\n    return _config\n\n\ndef reset_config() -> None:\n    \"\"\"Reset configuration (for testing).\"\"\"\n    global _config\n    _config = None\n\n\n# Convenience accessors\ndef get_embedding_config() -> EmbeddingConfig:\n    \"\"\"Get embedding configuration (general-purpose for memory/lessons).\"\"\"\n    return get_config().embedding\n\n\ndef get_code_embedding_co",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "# \ud83d\udcda ACE Framework API Reference\n\nComplete API documentation for the ACE Framework.\n\n## Configuration\n\n### Centralized Configuration (ace/config.py)\n\n**All embedding and Qdrant configuration is centralized** in `ace/config.py` using dataclasses with environment variable support.\n\n#### EmbeddingConfig\n\n```python\nfrom ace.config import EmbeddingConfig\n\n# Use defaults\nconfig = EmbeddingConfig()\n# config.url = \"http://192.168.10.64:1234\"\n# config.model = \"qwen/qwen3-embedding-8b\"\n# config.dimension ="
      ],
      "ace_line_counts": [
        117,
        58,
        157,
        113,
        120
      ],
      "auggie_files": [
        "ace/config.py",
        "ace/config.py"
      ],
      "auggie_contents": [
        "...\n    41\t\n    42\t\n    43\t@dataclass\n    44\tclass EmbeddingConfig:\n    45\t    \"\"\"Embedding model configuration for memory/lessons (general-purpose).\"\"\"\n    46\t\n    47\t    # LM Studio server\n    48\t    url: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_URL\", \"http://192.168.10.64:1234\"))\n    49\t\n    50\t    # Model name (Qwen3-Embedding-8B - proper embedding model, 4096 dims)\n    51\t    model: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_MODEL\", \"text-embedding-qwen3-embedding-8b\"))\n    52\t\n... (522 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"embedding client implementation, embedding API configuration, embedding providers\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    41\t\n    42\t\n    43\t@dataclass\n    44\tclass EmbeddingConfig:\n    45\t    \"\"\"Embedding model configuration for memory/lessons (general-purpose).\"\"\"\n    46\t\n    47\t    # LM Studio server\n    48\t    url: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_URL\", \"http://192.168.10.64:1234\"))\n    49\t\n    50\t    # Model name (Qwen3-Embedding-8B - proper embedding model, 4096 dims)\n    51\t    model: str = field(default_factory=lambda: _get_env(\"ACE_EMBEDDING_MODEL\", \"text-embedding-qwen3-embedding-8b\"))\n    52\t\n... (500 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (1.150)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "fibonacci sequence calculation",
      "category": "EdgeCases",
      "expected_files": [
        "fibonacci.py"
      ],
      "ace_files": [
        "fibonacci.py",
        "ace/self_consistency.py",
        "ace/embedding_finetuning/models/ace_finetuned/tokenizer.json",
        "ace/embedding_finetuning/models/ace_finetuned/tokenizer.json",
        "ace/embedding_finetuning/models/ace_finetuned/tokenizer.json"
      ],
      "ace_scores": [
        0.93024314,
        0.40085015,
        0.2453945,
        0.24099455,
        0.24096814000000003
      ],
      "ace_contents": [
        "\"\"\"\nFibonacci Number Calculator\n\nA high-performance, production-ready implementation for calculating Fibonacci numbers\nwith comprehensive error handling, multiple algorithm options, and extensive documentation.\n\nAuthor: Elite Software Engineer\nVersion: 1.0.0\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Union, Optional, Callable, Any\nfrom enum import Enum\n\n\nclass FibonacciAlgorithm(Enum):\n    \"\"\"Enumeration of available Fibonacci calculation algorithms.\"\"\"\n    ITER",
        "\"\"\"Self-consistency sampling for improved generation accuracy.\n\nThis module implements self-consistency decoding, which generates multiple\nresponses for the same prompt and selects the most consistent answer via\nmajority voting. This technique improves accuracy for tasks where reasoning\npaths can vary but the final answer should converge.\n\nReference: Wang et al., \"Self-Consistency Improves Chain of Thought Reasoning\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom collections import Co",
        "      \"jesus\": 4441,\n      \"cells\": 4442,\n      \"entry\": 4443,\n      \"1920\": 4444,\n      \"neither\": 4445,\n      \"trail\": 4446,\n      \"claims\": 4447,\n      \"atlantic\": 4448,\n      \"orders\": 4449,\n      \"labor\": 4450,\n      \"nose\": 4451,\n      \"afraid\": 4452,\n      \"identified\": 4453,\n      \"intelligence\": 4454,\n      \"calls\": 4455,\n      \"cancer\": 4456,\n      \"attacked\": 4457,\n      \"passing\": 4458,\n      \"stephen\": 4459,\n      \"positions\": 4460,\n      \"imperial\": 4461,\n      \"grey\": 4462,\n      ",
        "      \"maintain\": 5441,\n      \"knife\": 5442,\n      \"vs\": 5443,\n      \"voted\": 5444,\n      \"degrees\": 5445,\n      \"finance\": 5446,\n      \"quebec\": 5447,\n      \"opinion\": 5448,\n      \"translation\": 5449,\n      \"manner\": 5450,\n      \"ruled\": 5451,\n      \"operate\": 5452,\n      \"productions\": 5453,\n      \"choose\": 5454,\n      \"musician\": 5455,\n      \"discovery\": 5456,\n      \"confused\": 5457,\n      \"tired\": 5458,\n      \"separated\": 5459,\n      \"stream\": 5460,\n      \"techniques\": 5461,\n      \"committed",
        "      \"closest\": 7541,\n      \"##ction\": 7542,\n      \"surely\": 7543,\n      \"sultan\": 7544,\n      \"brings\": 7545,\n      \"riley\": 7546,\n      \"preparation\": 7547,\n      \"aboard\": 7548,\n      \"slammed\": 7549,\n      \"baptist\": 7550,\n      \"experiment\": 7551,\n      \"ongoing\": 7552,\n      \"interstate\": 7553,\n      \"organic\": 7554,\n      \"playoffs\": 7555,\n      \"##ika\": 7556,\n      \"1877\": 7557,\n      \"130\": 7558,\n      \"##tar\": 7559,\n      \"hindu\": 7560,\n      \"error\": 7561,\n      \"tours\": 7562,\n      "
      ],
      "ace_line_counts": [
        416,
        59,
        220,
        420,
        220
      ],
      "auggie_files": [
        "fibonacci.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"\n     2\tFibonacci Number Calculator\n     3\t\n     4\tA high-performance, production-ready implementation for calculating Fibonacci numbers\n     5\twith comprehensive error handling, multiple algorithm options, and extensive documentation.\n     6\t\n     7\tAuthor: Elite Software Engineer\n     8\tVersion: 1.0.0\n     9\t\"\"\"\n    10\t\n    11\tfrom __future__ import annotations\n    12\t\n    13\timport functools\n... (490 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.930)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "temperature converter celsius fahrenheit",
      "category": "EdgeCases",
      "expected_files": [
        "temperature_converter.py"
      ],
      "ace_files": [
        "temperature_converter.py",
        "ace/resilience.py",
        "ace/embedding_finetuning/models/ace_finetuned/tokenizer.json",
        "ace/embedding_finetuning/models/ace_finetuned/tokenizer.json",
        "scripts/run_questions_direct.py"
      ],
      "ace_scores": [
        1.2601194,
        0.36639342,
        0.28261293,
        0.24901072,
        0.23954663
      ],
      "ace_contents": [
        "#!/usr/bin/env python3\n\"\"\"\nTemperature Converter Tool\nConverts temperatures between Celsius, Fahrenheit, and Kelvin with input validation.\n\"\"\"\n\nimport argparse\nimport sys\nfrom typing import Union, Tuple\n\n\nclass TemperatureConverter:\n    \"\"\"A class to handle temperature conversions between Celsius, Fahrenheit, and Kelvin.\"\"\"\n\n    @staticmethod\n    def celsius_to_fahrenheit(celsius: float) -> float:\n        \"\"\"Convert Celsius to Fahrenheit.\"\"\"\n        return (celsius * 9/5) + 32\n\n    @staticmethod",
        "\"\"\"Resilience patterns for robust LLM interactions.\n\nThis module provides circuit breaker, retry, and other resilience patterns\nfor handling transient failures in LLM API calls.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom threading import Lock\nfrom typing import Any, Callable, Dict, Optional, TypeVar\n\nT = TypeVar(\"T\")\n\n",
        "      \"e\": 1041,\n      \"f\": 1042,\n      \"g\": 1043,\n      \"h\": 1044,\n      \"i\": 1045,\n      \"j\": 1046,\n      \"k\": 1047,\n      \"l\": 1048,\n      \"m\": 1049,\n      \"n\": 1050,\n      \"o\": 1051,\n      \"p\": 1052,\n      \"q\": 1053,\n      \"r\": 1054,\n      \"s\": 1055,\n      \"t\": 1056,\n      \"u\": 1057,\n      \"v\": 1058,\n      \"w\": 1059,\n      \"x\": 1060,\n      \"y\": 1061,\n      \"z\": 1062,\n      \"{\": 1063,\n      \"|\": 1064,\n      \"}\": 1065,\n      \"~\": 1066,\n      \"\u00a1\": 1067,\n      \"\u00a2\": 1068,\n      \"\u00a3\": 1069,\n      \"",
        "      \"##|\": 29641,\n      \"##}\": 29642,\n      \"##~\": 29643,\n      \"##\u00a1\": 29644,\n      \"##\u00a2\": 29645,\n      \"##\u00a3\": 29646,\n      \"##\u00a4\": 29647,\n      \"##\u00a5\": 29648,\n      \"##\u00a6\": 29649,\n      \"##\u00a7\": 29650,\n      \"##\u00a8\": 29651,\n      \"##\u00a9\": 29652,\n      \"##\u00aa\": 29653,\n      \"##\u00ab\": 29654,\n      \"##\u00ac\": 29655,\n      \"##\u00ae\": 29656,\n      \"##\u00b1\": 29657,\n      \"##\u00b4\": 29658,\n      \"##\u00b5\": 29659,\n      \"##\u00b6\": 29660,\n      \"##\u00b7\": 29661,\n      \"##\u00ba\": 29662,\n      \"##\u00bb\": 29663,\n      \"##\u00bc\": 29664,\n      \"##\u00be\": 29665,\n",
        "def main() -> None:\n    args = parse_args()\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_visible_devices\n\n    questions_path = Path(args.questions)\n    data = load_questions(questions_path)\n    print(f\"Loaded {len(data)} questions from {questions_path}.\")\n\n    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)\n    text_pipe = pipeline(\n        \"text-generation\",\n        model=args.model_path,\n        tokenizer=tokenizer,\n        device_map=\"auto\",\n        tr"
      ],
      "ace_line_counts": [
        263,
        18,
        620,
        520,
        102
      ],
      "auggie_files": [],
      "auggie_contents": [],
      "auggie_line_counts": [],
      "winner": "ACE",
      "reason": "Auggie returned no results",
      "ace_advantages": [
        "Has results"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 1,
      "auggie_expected_rank": -1
    },
    {
      "query": "email validation regex pattern",
      "category": "EdgeCases",
      "expected_files": [
        "email_validator.py"
      ],
      "ace_files": [
        "demo_email_validation.py",
        "email_validator.py",
        "email_validator.py",
        "glm_test2.json",
        "ace/pattern_detector.py"
      ],
      "ace_scores": [
        1.164806,
        0.7982716999999999,
        0.6492887,
        0.44900465,
        0.41243595
      ],
      "ace_contents": [
        "#!/usr/bin/env python3\n\"\"\"\nDemo script to showcase email validation functionality.\n\"\"\"\n\nfrom email_validator import EmailValidator\n\ndef demo():\n    validator = EmailValidator()\n\n    print(\"Email Validation Demo\")\n    print(\"=\" * 40)\n\n    test_emails = [\n        (\"user@gmail.com\", \"Valid standard email\"),\n        (\"john.doe@company.org\", \"Valid with dot in local part\"),\n        (\"test+tag@domain.co.uk\", \"Valid with plus tag\"),\n        (\"user@\", \"Missing domain\"),\n        (\"@domain.com\", \"Missing ",
        "#!/usr/bin/env python3\n\"\"\"\nEmail validation script with comprehensive error messages for common mistakes.\nUses regex patterns to validate email addresses and identify specific validation errors.\n\"\"\"\n\nimport re\nfrom typing import Optional, List, Tuple\n\n\nclass EmailValidator:\n    \"\"\"\n    Email address validator with detailed error reporting.\n\n    Features:\n    - Comprehensive regex-based validation\n    - Detailed error messages for common mistakes\n    - Support for internationalized domain names (",
        "    def _get_common_mistake_suggestions(self, email: str) -> List[str]:\n        \"\"\"Provide suggestions for common email mistakes.\"\"\"\n        suggestions = []\n\n        # Missing @ symbol\n        if '@' not in email:\n            if ' ' in email:\n                suggestions.append(\"Add '@' symbol between local part and domain\")\n            else:\n                # Try to suggest where to put @\n                if '.' in email:\n                    parts = email.rsplit('.', 1)\n                    if le",
        "{\n  \"choices\": [\n    {\n      \"finish_reason\": \"length\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"\",\n        \"reasoning_content\": \"\\n1.  **\u5206\u6790\u6838\u5fc3\u67e5\u8be2\uff1a**\\n    *   **\u6838\u5fc3\u540d\u8bcd\uff1a**\u201cuser input\u201d\uff08\u7528\u6237\u8f93\u5165\uff09\\n    *   **\u6838\u5fc3\u52a8\u8bcd\uff1a**\u201cvalidate\u201d\uff08\u9a8c\u8bc1\uff09\\n    *   **\u9690\u542b\u76ee\u6807\uff1a**\u7528\u6237\u5e0c\u671b\u5b66\u4e60\u5982\u4f55\u786e\u4fdd\u6765\u81ea\u7528\u6237\u7684\u6570\u636e\u662f\u6b63\u786e\u7684\u3001\u5b89\u5168\u7684\uff0c\u5e76\u7b26\u5408\u9884\u671f\u683c\u5f0f\u3002\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5e38\u89c1\u7684\u7f16\u7a0b\u4efb\u52a1\u3002\\n\\n2.  **\u6784\u601d\u5173\u952e\u8bcd\u548c\u540c\u4e49\u8bcd\uff1a**\\n    *   **\u201cValidate\u201d\uff08\u9a8c\u8bc1\uff09\uff1a** check\uff08\u68c0\u67e5\uff09\u3001verify\uff08\u6838\u5b9e\uff09\u3001sanitize\uff08\u6e05\u7406\uff09\u3001clean\uff08\u6e05\u9664\uff09\u3001filter\uff08\u8fc7\u6ee4\uff09\u3001ensure\uff08\u786e\u4fdd\uff09\u3001confirm\uff08\u786e\u8ba4\uff09\u3001test\uff08\u6d4b\u8bd5\uff09\u3002\\n    *   **\u201cUser Input\u201d\uff08\u7528\u6237\u8f93\u5165\uff09\uff1a** form data\uff08\u8868\u5355\u6570\u636e\uff09\u3001data entry\uff08",
        "\"\"\"\nPattern Detector module for ACE.\n\nProvides pattern detection and caching for common issues,\nwith learned fix templates for recurring problems.\n\nConfiguration:\n    ACE_ENABLE_PATTERN_DETECTION: Enable/disable pattern detection (default: false)\n    ACE_PATTERN_CACHE_SIZE: Maximum cached patterns (default: 100)\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef lo"
      ],
      "ace_line_counts": [
        47,
        62,
        242,
        26,
        268
      ],
      "auggie_files": [
        "email_validator.py"
      ],
      "auggie_contents": [
        "     1\t#!/usr/bin/env python3\n     2\t\"\"\"\n     3\tEmail validation script with comprehensive error messages for common mistakes.\n     4\tUses regex patterns to validate email addresses and identify specific validation errors.\n     5\t\"\"\"\n     6\t\n     7\timport re\n     8\tfrom typing import Optional, List, Tuple\n     9\t\n    10\t\n    11\tclass EmailValidator:\n    12\t    \"\"\"\n    13\t    Email address validator with detailed error reporting.\n... (484 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 2",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (1.165)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "sparse BM25 term frequency calculation",
      "category": "EdgeCases",
      "expected_files": [
        "ace/unified_memory.py",
        "ace/hyde_retrieval.py"
      ],
      "ace_files": [
        "rag_training/optimizations/v8_bm25_hybrid.py",
        "ace/hyde_retrieval.py",
        "ace/retrieval_optimized.py",
        "ace/unified_memory.py",
        "rag_training/optimizations/v8_baseline_revert.py"
      ],
      "ace_scores": [
        0.6421714,
        0.62130404,
        0.5835133,
        0.5728882,
        0.5636353
      ],
      "ace_contents": [
        "\"\"\"\nV8: BM25 Hybrid Scoring with Tunable Weights\n=============================================\n\nTests hybrid scoring combining dense semantic vectors with BM25 sparse vectors\nto balance semantic understanding with exact phrase matching.\n\nFormula: score = (weight * dense_score) + ((1-weight) * bm25_score)\n\nKey Hypothesis:\n- Dense vectors capture semantic meaning and context\n- BM25 sparse vectors capture exact phrase matches and technical terms\n- Optimal weight is expected in 0.4-0.6 range (expert",
        "    def _search_qdrant(\n        self,\n        query_embedding: List[float],\n        query_text: str,\n        limit: int\n    ) -> List[QdrantScoredResult]:\n        \"\"\"Execute hybrid search in Qdrant.\n\n        Args:\n            query_embedding: Dense embedding vector (possibly HyDE-enhanced)\n            query_text: Original query text for BM25\n            limit: Maximum results\n\n        Returns:\n            List of scored results\n        \"\"\"\n        # Compute BM25 sparse vector from original query",
        "def tokenize_bm25(text: str) -> List[str]:\n    \"\"\"\n    Tokenize text for BM25, preserving technical terms.\n    \"\"\"\n    # Split CamelCase\n    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n    # Split snake_case\n    text = text.replace('_', ' ')\n    # Extract tokens\n    tokens = re.findall(r'[a-zA-Z0-9]+', text.lower())\n    # Filter\n    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n    return tokens\n\n\ndef compute_bm25_sparse(\n    text: str,\n    k1: float = 1.5,\n    b: float ",
        "    class _MatchValue:\n        value: Any\n    MatchValue = _MatchValue\n\n    @dataclass\n    class _MatchAny:\n        any: List[Any]\n    MatchAny = _MatchAny\n\n    @dataclass\n    class _Filter:\n        must: Optional[List[Any]] = None\n        should: Optional[List[Any]] = None\n    Filter = _Filter\n\n\n# =============================================================================\n# NAMESPACE AND SOURCE ENUMS\n# =============================================================================\n\nclass Unifie",
        "\"\"\"\nV8: Baseline Revert - Verify 62%+ Recall@5 Restoration\n========================================================\n\nPURPOSE: Verify we can restore original 62.52% Recall@5 baseline performance\nthat existed BEFORE adding HyDE/reranker optimizations.\n\nThis script uses ONLY:\n- Simple hybrid search (dense + BM25 sparse)\n- LM Studio embeddings (nomic-embed-text-v1.5, 768 dims)\n- No HyDE generation\n- No reranker\n- RRF fusion for combining dense + sparse results\n\nExpected: ~62% Recall@5 (original base"
      ],
      "ace_line_counts": [
        389,
        196,
        81,
        101,
        225
      ],
      "auggie_files": [
        "rag_training/optimizations/v8_bm25_hybrid.py",
        "rag_training/optimizations/v8_bm25_hybrid.py"
      ],
      "auggie_contents": [
        "...\n    51\t\n    52\tQDRANT_URL = \"http://localhost:6333\"\n    53\tEMBEDDING_URL = \"http://192.168.10.64:1234\"  # LM Studio\n    54\tCOLLECTION_NAME = \"ace_memories_hybrid\"\n    55\t\n    56\t# Test weights for hybrid scoring\n    57\tTEST_WEIGHTS = [0.3, 0.4, 0.5, 0.6, 0.7]\n    58\t\n    59\t# Retrieval Configuration\n    60\tRETRIEVAL_LIMIT = 100  # Retrieve more candidates for RRF fusion\n    61\tFINAL_K = 10\n    62\t\n... (564 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"BM25 algorithm, BM25 scorer, ranking functions, information retrieval\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    51\t\n    52\tQDRANT_URL = \"http://localhost:6333\"\n    53\tEMBEDDING_URL = \"http://192.168.10.64:1234\"  # LM Studio\n    54\tCOLLECTION_NAME = \"ace_memories_hybrid\"\n    55\t\n    56\t# Test weights for hybrid scoring\n    57\tTEST_WEIGHTS = [0.3, 0.4, 0.5, 0.6, 0.7]\n    58\t\n    59\t# Retrieval Configuration\n    60\tRETRIEVAL_LIMIT = 100  # Retrieve more candidates for RRF fusion\n    61\tFINAL_K = 10\n    62\t\n... (578 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "vector similarity cosine distance",
      "category": "EdgeCases",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/semantic_scorer.py",
        "ace/qdrant_retrieval.py",
        "ace/query_features.py",
        "ace/unified_memory.py",
        "ace/hyde_retrieval.py"
      ],
      "ace_scores": [
        0.5751615,
        0.47004125,
        0.46161747,
        0.45726848,
        0.4562524
      ],
      "ace_contents": [
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "\"\"\"Vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides QdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with hybrid search (dense + BM25 sparse).\n\nPhase 1: Vector Search Integration for ACE Fortune 100 Production Readiness.\n\nKey features:\n- Dense embeddings via LM Studio (nomic-embed-text-v1.5, 768-dim)\n- BM25 sparse vectors for keyword matching (technical terms)\n- Hybrid search with RRF fusion for best of both approaches\n",
        "\"\"\"\nQuery Feature Extractor for LinUCB Bandit.\n\nPart of P7 ARIA (Adaptive Retrieval Intelligence Architecture).\n\nExtracts 10-dimension feature vector from queries for contextual bandit routing decisions.\nOptimized for <5ms extraction latency with >90% detection accuracy.\n\nThis module is an original contribution for adapting contextual bandits to RAG retrieval.\nThe feature set was designed empirically for query complexity classification.\n\"\"\"\n\nfrom typing import List\nimport re\n\n\nclass QueryFeature",
        "    class _MatchValue:\n        value: Any\n    MatchValue = _MatchValue\n\n    @dataclass\n    class _MatchAny:\n        any: List[Any]\n    MatchAny = _MatchAny\n\n    @dataclass\n    class _Filter:\n        must: Optional[List[Any]] = None\n        should: Optional[List[Any]] = None\n    Filter = _Filter\n\n\n# =============================================================================\n# NAMESPACE AND SOURCE ENUMS\n# =============================================================================\n\nclass Unifie",
        "    def _compute_bm25_sparse(self, text: str) -> Dict[str, Any]:\n        \"\"\"Compute BM25-style sparse vector for Qdrant.\n\n        Args:\n            text: Text to vectorize\n\n        Returns:\n            Dict with 'indices' (term hashes) and 'values' (BM25 weights)\n        \"\"\"\n        tokens = self._tokenize_for_bm25(text)\n        if not tokens:\n            return {\"indices\": [], \"values\": []}\n\n        tf = Counter(tokens)\n        doc_length = len(tokens)\n\n        indices = []\n        values = []\n"
      ],
      "ace_line_counts": [
        104,
        281,
        263,
        101,
        85
      ],
      "auggie_files": [
        "ace/semantic_scorer.py",
        "ace/embedding_finetuning/USAGE_EXAMPLES.md"
      ],
      "auggie_contents": [
        "...\n    53\t    \n    54\t    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n    55\t        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    56\t        norm1 = np.linalg.norm(vec1)\n    57\t        norm2 = np.linalg.norm(vec2)\n    58\t        if norm1 == 0 or norm2 == 0:\n    59\t            return 0.0\n    60\t        return float(np.dot(vec1, vec2) / (norm1 * norm2))\n    61\t    \n    62\t    def score_result(self, query: str, result_content: str) -> float:\n    63\t        \"\"\"\n    64\t        Score how relevant a result is to the query using semantic similarity.\n... (520 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"embedding similarity scoring distance metrics\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   249\tfrom sentence_transformers import SentenceTransformer\n   250\t\n   251\t# Create evaluator\n   252\tevaluator = EmbeddingEvaluator(\n   253\t    test_suite_path=\"rag_training/test_suite/enhanced_test_suite.json\",\n   254\t    qdrant_url=\"http://localhost:6333\",\n   255\t)\n   256\tevaluator.load_test_suite()\n   257\t\n   258\t# Load models\n   259\tbaseline = BaselineEmbeddingClient(embedding_url=\"http://192.168.10.64:1234\")\n   260\tfinetuned = SentenceTransformer(\"ace/embedding_finetuning/models/ace_finetuned\")\n... (559 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 1 advantages vs 0",
      "ace_advantages": [
        "More unique files (4 vs 1)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "hybrid search dense sparse fusion",
      "category": "EdgeCases",
      "expected_files": [
        "ace/unified_memory.py"
      ],
      "ace_files": [
        "ace/qdrant_retrieval.py",
        "ace/unified_memory.py",
        "ace/retrieval_optimized.py",
        "ace/hyde_retrieval.py",
        "ace/async_retrieval.py"
      ],
      "ace_scores": [
        0.7498631,
        0.74204665,
        0.7347534,
        0.7251255999999999,
        0.7171274400000001
      ],
      "ace_contents": [
        "    def retrieve(\n        self,\n        query: str,\n        limit: int = 10,\n        query_type: Optional[str] = None,\n        min_score: float = 0.0,\n    ) -> List[QdrantScoredResult]:\n        \"\"\"Retrieve bullets using hybrid search.\n\n        Combines dense semantic search with BM25 keyword matching\n        using Reciprocal Rank Fusion (RRF).\n\n        Args:\n            query: Natural language query\n            limit: Maximum number of results\n            query_type: Optional query type for filt",
        "    def retrieve(\n        self,\n        query: str,\n        namespace: Optional[Union[UnifiedNamespace, str, List[Union[UnifiedNamespace, str]]]] = None,\n        limit: int = 10,\n        threshold: float = 0.35,\n        include_superseded: Optional[bool] = None,\n        created_after: Optional[datetime] = None,\n        created_before: Optional[datetime] = None,\n        updated_after: Optional[datetime] = None,\n        preset: Optional[RetrievalPreset] = None,\n        auto_detect_preset: bool = T",
        "class OptimizedRetriever:\n    \"\"\"\n    State-of-the-art retriever with hybrid search, query expansion,\n    multi-query RRF fusion, and cross-encoder re-ranking.\n    \"\"\"\n\n    def __init__(self, config: Dict[str, Any] = None):\n        \"\"\"\n        Initialize retriever with configuration.\n\n        Args:\n            config: Override default configuration values\n        \"\"\"\n        self.config = {**_get_default_config(), **(config or {})}\n\n        if not HTTPX_AVAILABLE:\n            raise ImportError(\"",
        "\"\"\"HyDE-enhanced retrieval pipeline for ACE memory system.\n\nIntegrates HyDE (Hypothetical Document Embeddings) with existing hybrid search\ninfrastructure for improved retrieval accuracy on ambiguous/implicit queries.\n\nPipeline:\n1. Query -> HyDE expansion -> Generate hypothetical documents\n2. Embed hypotheticals -> Average embeddings\n3. Search Qdrant with averaged embedding + BM25 sparse\n4. Return results with hybrid RRF fusion\n\nPerformance target: +5-10% for implicit/scenario/template queries\n\"\"",
        "    async def retrieve(\n        self,\n        query: str,\n        limit: int = 10,\n        query_type: Optional[str] = None,\n        min_score: float = 0.0,\n    ) -> List[QdrantScoredResult]:\n        \"\"\"Retrieve bullets using hybrid search asynchronously.\n\n        Combines dense semantic search with BM25 keyword matching\n        using Reciprocal Rank Fusion (RRF).\n\n        Args:\n            query: Natural language query\n            limit: Maximum number of results\n            query_type: Optiona"
      ],
      "ace_line_counts": [
        112,
        471,
        313,
        419,
        106
      ],
      "auggie_files": [
        "rag_training/run_hybrid_evaluation.py",
        "docs/Fortune100.md",
        "rag_training/optimizations/v7_fortune100_combined.py"
      ],
      "auggie_contents": [
        "...\n   203\t\n   204\t    def hybrid_search(self, query: str, limit: int = TOP_K) -> Tuple[List[Dict], float]:\n   205\t        \"\"\"\n   206\t        Execute PROPER hybrid search with dense + sparse + RRF.\n   207\t\n   208\t        This is the corrected version that includes BM25 sparse vectors.\n   209\t        \"\"\"\n   210\t        start = time.perf_counter()\n   211\t\n   212\t        # 1. Get dense embedding\n   213\t        dense_embedding = self.get_embedding(query)\n   214\t        if not dense_embedding:\n... (536 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"dense vector embeddings, sparse vector representations like BM25 or TF-IDF, and any vector database or search index implementations\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   135\t\n   136\t```python\n   137\t# From ace_qdrant_memory.py - ALREADY IMPLEMENTED\n   138\tQDRANT_URL = \"http://localhost:6333\"\n   139\tLMSTUDIO_URL = \"http://192.168.10.64:1234\"\n   140\tEMBEDDING_MODEL = \"text-embedding-snowflake-arctic-embed-m-v1.5\"  # Snowflake model\n   141\tCOLLECTION_NAME = \"ace_memories_hybrid\"\n   142\tEMBEDDING_DIM = 768  # snowflake-arctic-embed-m dimension\n   143\t\n   144\t# BM25 parameters (already tuned)\n   145\tBM25_K1 = 1.5\n   146\tBM25_B = 0.75\n... (537 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"search ranking, score fusion methods like RRF (Reciprocal Rank Fusion), weighted fusion, or other ranking combination strategies\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   168\t\n   169\t\n   170\tdef reciprocal_rank_fusion(result_sets: List[List[Dict]], k: int = RRF_K) -> List[Dict]:\n   171\t    scores: Dict[str, float] = {}\n   172\t    doc_data: Dict[str, Dict] = {}\n   173\t\n   174\t    for results in result_sets:\n   175\t        for rank, doc in enumerate(results, start=1):\n   176\t            doc_id = str(doc.get(\"id\", \"\"))\n   177\t            if doc_id not in scores:\n   178\t                scores[doc_id] = 0.0\n   179\t            scores[doc_id] += 1.0 / (k + rank)\n... (515 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 2, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 2"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 2,
      "auggie_expected_rank": -1
    },
    {
      "query": "code chunking AST parsing",
      "category": "EdgeCases",
      "expected_files": [
        "ace/code_chunker.py"
      ],
      "ace_files": [
        "ace/code_chunker.py",
        "ace/code_indexer.py",
        "ace/code_analysis.py",
        "ace/code_indexer.py",
        "ace/code_retrieval.py"
      ],
      "ace_scores": [
        0.85808276,
        0.54986656,
        0.5306732,
        0.52508783,
        0.50749356
      ],
      "ace_contents": [
        "\"\"\"AST-based semantic code chunking module.\n\nThis module provides intelligent code chunking that respects language syntax\nboundaries (functions, classes, methods) rather than arbitrary line counts.\n\nSupports multiple languages via tree-sitter:\n- Python (via built-in ast module or tree-sitter)\n- JavaScript/TypeScript (via tree-sitter)\n- Go (via tree-sitter)\n\nConfiguration:\n    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    ACE_AST_MAX_LINES: Maximum lines per chunk (def",
        "\"\"\"Code indexer module for workspace code indexing.\n\nThis module provides code indexing capabilities that scan a workspace,\nparse code files using ASTChunker, and store indexed chunks in Qdrant\nfor semantic code search.\n\nConfiguration:\n    ACE_CODE_COLLECTION: Qdrant collection name (default: ace_code_context)\n    ACE_CODE_EMBEDDING_DIM: Embedding dimension (default: from EmbeddingConfig)\n    QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n\nThe indexer supports:\n- Multi-language p",
        "\"\"\"ACE Code Analysis module (Phase 2A: Tree-sitter Integration).\n\nThis module provides AST-based code understanding for code-specific queries\nusing tree-sitter parsing. Supports Python, TypeScript, JavaScript, and Go.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport tree_sitter_go as tsgo\nimport tree_sitter_javascript as tsjs\nimport tree_sitter_python as tspython\nimport tree_sitter_typescript as tstype\nfrom tree_sitter import Langua",
        "    def chunk_file(self, file_path: str) -> List[CodeChunkIndexed]:\n        \"\"\"\n        Parse and chunk a code file.\n        \n        Args:\n            file_path: Absolute path to code file\n            \n        Returns:\n            List of CodeChunkIndexed instances\n        \"\"\"\n        chunks = []\n        \n        # Check file exists\n        if not os.path.exists(file_path):\n            logger.warning(f\"File not found: {file_path}\")\n            return chunks\n        \n        # Read file content\n",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca"
      ],
      "ace_line_counts": [
        553,
        55,
        469,
        86,
        147
      ],
      "auggie_files": [
        "ace/code_chunker.py",
        "ace/code_chunker.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"AST-based semantic code chunking module.\n     2\t\n     3\tThis module provides intelligent code chunking that respects language syntax\n     4\tboundaries (functions, classes, methods) rather than arbitrary line counts.\n     5\t\n     6\tSupports multiple languages via tree-sitter:\n     7\t- Python (via built-in ast module or tree-sitter)\n     8\t- JavaScript/TypeScript (via tree-sitter)\n     9\t- Go (via tree-sitter)\n    10\t\n    11\tConfiguration:\n    12\t    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    13\t    ACE_AST_MAX_LINES: Maximum lines per chunk (default: 120)\n... (411 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"abstract syntax tree parser tokenization chunking\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "     1\t\"\"\"AST-based semantic code chunking module.\n     2\t\n     3\tThis module provides intelligent code chunking that respects language syntax\n     4\tboundaries (functions, classes, methods) rather than arbitrary line counts.\n     5\t\n     6\tSupports multiple languages via tree-sitter:\n     7\t- Python (via built-in ast module or tree-sitter)\n     8\t- JavaScript/TypeScript (via tree-sitter)\n     9\t- Go (via tree-sitter)\n    10\t\n    11\tConfiguration:\n    12\t    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    13\t    ACE_AST_MAX_LINES: Maximum lines per chunk (default: 120)\n... (427 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        20,
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)",
        "High confidence top score (0.858)"
      ],
      "auggie_advantages": [
        "Better chunk size (30 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "metadata filtering namespace",
      "category": "EdgeCases",
      "expected_files": [
        "ace/unified_memory.py"
      ],
      "ace_files": [
        "ace/retrieval.py",
        "ace/unified_memory.py",
        "tenant_data/learned_typos.json",
        "ace/delta.py",
        "ace/retrieval.py"
      ],
      "ace_scores": [
        0.5936813,
        0.49066997,
        0.46835256,
        0.45028228,
        0.44135046
      ],
      "ace_contents": [
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non",
        "\"\"\"\nUnified Memory Architecture for ACE Framework\n\nThis module provides a unified storage and retrieval system that merges:\n1. ACE Framework Playbook bullets (task strategies with helpful/harmful counters)\n2. Personal Memory Bank memories (user preferences with severity/reinforcement)\n\nThe unified system uses a single Qdrant collection with namespace separation,\nproviding consistent retrieval logic using ACE Framework's SmartBulletIndex.\n\nArchitecture:\n    Single Qdrant Collection: \"ace_unified\"",
        "    \"session\": \"scession\",\n    \"wired\": \"weird\",\n    \"requests\": \"request\",\n    \"pipeline\": \"pipelines\",\n    \"there\": \"their\",\n    \"feature\": \"future\",\n    \"works\": \"work\",\n    \"anything\": \"anythink\",\n    \"degredation\": \"degradation\",\n    \"com\": \"come\",\n    \"aria\": \"area\",\n    \"eith\": \"either\",\n    \"elf\": \"self\",\n    \"ensbled\": \"enabled\",\n    \"mocking\": \"moking\",\n    \"expectations\": \"expectation\",\n    \"message\": \"massage\",\n    \"command\": \"commande\",\n    \"phase\": \"phrase\",\n    \"tools\": \"tool\",\n  ",
        "\"\"\"Delta operations produced by the ACE Curator.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Iterable, List, Literal, Optional, cast\n\n\nOperationType = Literal[\"ADD\", \"UPDATE\", \"TAG\", \"REMOVE\"]\n\n\n@dataclass\nclass DeltaOperation:\n    \"\"\"Single mutation to apply to the playbook.\n\n    Attributes:\n        type: Operation type (ADD, UPDATE, TAG, REMOVE)\n        section: Section name for the bullet\n        content: Bullet content text (",
        "    def retrieve_task_strategies(\n        self,\n        query: Optional[str] = None,\n        limit: Optional[int] = None,\n    ) -> List[ScoredBullet]:\n        \"\"\"Retrieve task strategies from unified memory.\n\n        Convenience method that filters by TASK_STRATEGIES namespace.\n\n        Args:\n            query: Optional search query\n            limit: Maximum number of results\n\n        Returns:\n            List of task strategy bullets sorted by relevance.\n        \"\"\"\n        try:\n            fr"
      ],
      "ace_line_counts": [
        393,
        222,
        133,
        107,
        53
      ],
      "auggie_files": [
        "ace/unified_memory.py",
        "ace/unified_memory.py"
      ],
      "auggie_contents": [
        "...\n    16\t\n    17\tUsage:\n    18\t    >>> from ace.unified_memory import UnifiedMemoryIndex, UnifiedBullet, UnifiedNamespace\n    19\t    >>> index = UnifiedMemoryIndex(qdrant_url=\"http://localhost:6333\")\n    20\t    >>> bullet = UnifiedBullet(\n    21\t    ...     id=\"test-001\",\n    22\t    ...     namespace=UnifiedNamespace.USER_PREFS,\n    23\t    ...     source=UnifiedSource.USER_FEEDBACK,\n    24\t    ...     content=\"User prefers TypeScript\",\n    25\t    ...     section=\"preferences\"\n    26\t    ... )\n    27\t    >>> index.index_bullet(bullet)\n... (451 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"namespace field in metadata, namespace filtering in queries, how namespaces are used for filtering documents or embeddings\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    16\t\n    17\tUsage:\n    18\t    >>> from ace.unified_memory import UnifiedMemoryIndex, UnifiedBullet, UnifiedNamespace\n    19\t    >>> index = UnifiedMemoryIndex(qdrant_url=\"http://localhost:6333\")\n    20\t    >>> bullet = UnifiedBullet(\n    21\t    ...     id=\"test-001\",\n    22\t    ...     namespace=UnifiedNamespace.USER_PREFS,\n    23\t    ...     source=UnifiedSource.USER_FEEDBACK,\n    24\t    ...     content=\"User prefers TypeScript\",\n    25\t    ...     section=\"preferences\"\n    26\t    ... )\n    27\t    >>> index.index_bullet(bullet)\n... (506 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 0)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    },
    {
      "query": "deduplication similarity threshold",
      "category": "EdgeCases",
      "expected_files": [
        "ace/deduplication.py"
      ],
      "ace_files": [
        "ace/retrieval_presets.py",
        "ace/retrieval.py",
        "ace/deduplication.py",
        "scripts/analyze_memory_duplicates.py",
        "scripts/deduplicate_memories.py"
      ],
      "ace_scores": [
        0.7126760000000001,
        0.66900646,
        0.5872008,
        0.5597298,
        0.5576929
      ],
      "ace_contents": [
        "def get_preset_config(preset: RetrievalPreset) -> RetrievalConfig:\n    \"\"\"Get configuration for a preset.\"\"\"\n    return PRESET_CONFIGS.get(preset, PRESET_CONFIGS[RetrievalPreset.BALANCED])\n\n\ndef cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    if not vec1 or not vec2 or len(vec1) != len(vec2):\n        return 0.0\n\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    norm1 = math.sqrt(sum(a * a for a in vec1))\n",
        "    def _get_dynamic_weights(self, bullet: \"Bullet\") -> Tuple[float, float]:\n        \"\"\"Calculate dynamic weights based on bullet maturity.\n\n        New bullets rely more on similarity (cold start exploration).\n        Mature bullets rely more on outcomes (evidence-based exploitation).\n\n        Args:\n            bullet: The bullet to calculate weights for\n\n        Returns:\n            Tuple of (similarity_weight, outcome_weight) where:\n            - New bullets (0 signals): (0.8, 0.2) - trust si",
        "\"\"\"\nAdvanced Memory Deduplication System for RAG.\n\nThis module provides clustering-based deduplication for Qdrant collections:\n- HDBSCAN/DBSCAN clustering for efficient duplicate detection (O(n log n) vs O(n^2))\n- Multi-collection support (ace_memories_hybrid, ace_unified)\n- Multiple merge strategies (keep_best, merge_content, canonical_form)\n- Cluster quality metrics (silhouette score, Davies-Bouldin index)\n- Dry-run mode for safe preview\n\nArchitecture:\n    1. Load memories with embeddings from",
        "\"\"\"\nAnalyze memory deduplication effectiveness in Qdrant.\n\nBASELINE: Average Precision 75.6%, total memories: 2348\n\nThis script measures:\n1. Duplicate rate in the full collection (random sampling)\n2. Duplicate rate in retrieval results (top-15)\n3. Potential precision gain from better deduplication\n\nKey Hypothesis: High duplicate rate = wasted retrieval slots = lower precision\n\"\"\"\n\nimport httpx\nimport random\nimport numpy as np\nfrom typing import List, Dict, Any, Tuple\nfrom collections import defa",
        "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Deduplicate memories in ace_unified collection\"\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Preview without making changes\"\n    )\n    parser.add_argument(\n        \"--threshold\",\n        type=float,\n        default=DEFAULT_THRESHOLD,\n        help=f\"Similarity threshold for deduplication (default: {DEFAULT_THRESHOLD})\"\n    )\n    parser.add_argument(\n        \"--qdrant-url\",\n    "
      ],
      "ace_line_counts": [
        193,
        235,
        104,
        467,
        76
      ],
      "auggie_files": [
        "scripts/deduplicate_memories.py",
        "scripts/deduplicate_memories.py"
      ],
      "auggie_contents": [
        "     1\t#!/usr/bin/env python3\n     2\t\"\"\"\n     3\tDeduplicate existing memories in ace_unified collection.\n     4\t\n     5\tThis script identifies and consolidates semantically similar memories:\n     6\t1. Scans all memories in the collection\n     7\t2. Groups memories by semantic similarity (>0.92 threshold)\n     8\t3. For each group: keeps the best one, merges reinforcement counts, deletes duplicates\n     9\t\n    10\tUsage:\n    11\t    python scripts/deduplicate_memories.py --dry-run   # Preview without changes\n    12\t    python scripts/deduplicate_memories.py             # Execute deduplication\n    13\t    python scripts/deduplicate_memories.py --threshold 0.90  # Custom threshold\n... (490 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"similarity threshold for deduplicating bullets or playbook entries\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "     1\t#!/usr/bin/env python3\n     2\t\"\"\"\n     3\tDeduplicate existing memories in ace_unified collection.\n     4\t\n     5\tThis script identifies and consolidates semantically similar memories:\n     6\t1. Scans all memories in the collection\n     7\t2. Groups memories by semantic similarity (>0.92 threshold)\n     8\t3. For each group: keeps the best one, merges reinforcement counts, deletes duplicates\n     9\t\n    10\tUsage:\n    11\t    python scripts/deduplicate_memories.py --dry-run   # Preview without changes\n    12\t    python scripts/deduplicate_memories.py             # Execute deduplication\n    13\t    python scripts/deduplicate_memories.py --threshold 0.90  # Custom threshold\n... (467 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE found expected file at rank 3, Auggie missed it",
      "ace_advantages": [
        "Found expected file at rank 3"
      ],
      "auggie_advantages": [],
      "ace_found_expected": true,
      "auggie_found_expected": false,
      "ace_expected_rank": 3,
      "auggie_expected_rank": -1
    },
    {
      "query": "query expansion semantic",
      "category": "EdgeCases",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/retrieval_optimized.py",
        "ace/query_enhancer.py",
        "ace/retrieval_optimized.py",
        "ace/semantic_scorer.py",
        "ace/query_features.py"
      ],
      "ace_scores": [
        0.7924867,
        0.7912376999999999,
        0.7461175,
        0.7417392,
        0.7221112999999999
      ],
      "ace_contents": [
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============",
        "\"\"\"Query Enhancer for ACE Retrieval.\n\nTransforms vague, ambiguous user queries into expanded, domain-specific queries\nthat improve retrieval precision.\n\nBased on the EnginizeAPI enhanced prompt methodology.\n\"\"\"\n\nfrom typing import List, Dict, Optional, Tuple\nimport re\n\n\n# Domain keyword expansions for technical queries\nDOMAIN_EXPANSIONS: Dict[str, List[str]] = {\n    # Vague action words -> technical context\n    \"fix\": [\"debug\", \"error\", \"bug\", \"issue\", \"resolve\", \"repair\", \"patch\", \"troubleshoot",
        "    def _determine_expansion_level(\n        self, word_count: int, specificity_score: float\n    ) -> dict:\n        \"\"\"\n        Determine expansion configuration based on word count and specificity.\n        \n        Strategy:\n        - Ultra-vague queries (specificity < 0.15): SKIP expansion - let semantic search handle\n        - Short queries (\u22643 words): Maximum expansion (unless ultra-vague)\n        - Medium queries (4-8 words): Moderate expansion\n        - Long queries (\u22659 words): Minimal/no e",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_",
        "\"\"\"\nQuery Feature Extractor for LinUCB Bandit.\n\nPart of P7 ARIA (Adaptive Retrieval Intelligence Architecture).\n\nExtracts 10-dimension feature vector from queries for contextual bandit routing decisions.\nOptimized for <5ms extraction latency with >90% detection accuracy.\n\nThis module is an original contribution for adapting contextual bandits to RAG retrieval.\nThe feature set was designed empirically for query complexity classification.\n\"\"\"\n\nfrom typing import List\nimport re\n\n\nclass QueryFeature"
      ],
      "ace_line_counts": [
        424,
        176,
        296,
        22,
        263
      ],
      "auggie_files": [
        "ace/query_enhancer.py",
        "ace/semantic_scorer.py"
      ],
      "auggie_contents": [
        "...\n    96\t\n    97\t\n    98\tdef expand_vague_terms(query: str) -> Tuple[str, List[str]]:\n    99\t    \"\"\"Expand vague terms into domain-specific keywords.\n   100\t    \n   101\t    Args:\n   102\t        query: The user's query string.\n   103\t        \n   104\t    Returns:\n   105\t        Tuple of (expanded query, list of added terms).\n   106\t    \"\"\"\n   107\t    query_lower = query.lower()\n... (462 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"semantic similarity, embedding models, vector search, query rewriting, query augmentation\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n    53\t    \n    54\t    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n    55\t        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    56\t        norm1 = np.linalg.norm(vec1)\n    57\t        norm2 = np.linalg.norm(vec2)\n    58\t        if norm1 == 0 or norm2 == 0:\n    59\t            return 0.0\n    60\t        return float(np.dot(vec1, vec2) / (norm1 * norm2))\n    61\t    \n    62\t    def score_result(self, query: str, result_content: str) -> float:\n    63\t        \"\"\"\n    64\t        Score how relevant a result is to the query using semantic similarity.\n... (530 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (0.792)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "async embedding batch retry error",
      "category": "EdgeCases",
      "expected_files": [
        "ace/async_retrieval.py",
        "ace/resilience.py"
      ],
      "ace_files": [
        "ace/async_retrieval.py",
        "ace/gemini_embeddings.py",
        "ace/openai_embeddings.py",
        "ace/async_adaptation.py",
        "ace/semantic_scorer.py"
      ],
      "ace_scores": [
        0.91192145,
        0.7516132,
        0.716348,
        0.62807811,
        0.5064652
      ],
      "ace_contents": [
        "\"\"\"Async vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides AsyncQdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with async operations.\n\nPhase 4A: Async Operations for ACE Framework.\n\nKey features:\n- Async embedding retrieval via httpx.AsyncClient\n- Parallel batch processing with asyncio.gather\n- Concurrent query handling\n- Non-blocking Qdrant operations\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport hash",
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"OpenAI Embedding Client for ACE Framework.\n\nProvides embeddings using OpenAI's text-embedding-3-large model\nwith configurable dimensions.\n\nUsage:\n    from ace.openai_embeddings import OpenAIEmbeddingClient\n\n    client = OpenAIEmbeddingClient(api_key=\"your-api-key\")\n\n    # Get embedding\n    embedding = client.get_embedding(\"This is a document about...\")\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Optional\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# OpenAI API Configuration",
        "class AsyncOfflineAdapter:\n    \"\"\"Async version of OfflineAdapter for parallel sample processing.\n\n    Enables concurrent processing of multiple samples while respecting\n    max_parallel limits to avoid overwhelming LLM APIs.\n\n    Example:\n        >>> adapter = AsyncOfflineAdapter(playbook, generator, reflector, curator)\n        >>> results = await adapter.run(samples, environment, epochs=3, max_parallel=5)\n    \"\"\"\n\n    def __init__(\n        self,\n        playbook: \"Playbook\",\n        generator:",
        "#!/usr/bin/env python\n\"\"\"\nSemantic Similarity Scorer for ACE Retrieval Quality Measurement.\n\nUses embedding cosine similarity instead of keyword matching to measure\nhow relevant retrieved results are to the original query.\n\nThis provides a more accurate quality metric than keyword-based precision.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nfrom typing import List, Tuple, Optional\nimport httpx\n\n# Add project root to path\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file_"
      ],
      "ace_line_counts": [
        458,
        280,
        211,
        99,
        104
      ],
      "auggie_files": [
        "ace/async_retrieval.py",
        "ace/embedding_finetuning/finetuned_retrieval.py"
      ],
      "auggie_contents": [
        "...\n   188\t\n   189\t    async def batch_get_embeddings(self, texts: List[str]) -> List[List[float]]:\n   190\t        \"\"\"Retrieve embeddings for multiple texts in parallel.\n   191\t\n   192\t        Uses asyncio.gather for concurrent execution.\n   193\t\n   194\t        Args:\n   195\t            texts: List of texts to embed\n   196\t\n   197\t        Returns:\n   198\t            List of embedding vectors (same order as input).\n   199\t\n... (505 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"error handling in embedding operations, batch embedding failures, tenacity retry decorators for embeddings\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "     1\t\"\"\"Production retrieval using fine-tuned embeddings.\n     2\t\n     3\tProvides a drop-in replacement for the baseline retrieval system that uses\n     4\tfine-tuned embeddings with fallback to original nomic embeddings.\n     5\t\n     6\tSupports:\n     7\t- Hybrid search (fine-tuned dense + BM25 sparse + RRF fusion)\n     8\t- Automatic fallback to baseline embeddings\n     9\t- Compatible with existing Qdrant infrastructure\n    10\t\"\"\"\n    11\t\n    12\timport logging\n    13\tfrom typing import Dict, List, Optional\n... (509 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: web-search\u001b[0m\n   query: \"async embedding batch retry error python 2026\"\n   num_results: 5\n\n\u001b[90m\ud83d\udccb Tool result: web-search\u001b[0m"
      ],
      "auggie_line_counts": [
        20,
        39
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (4 vs 1)",
        "High confidence top score (0.912)"
      ],
      "auggie_advantages": [
        "Better chunk size (30 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "config validation environment variable",
      "category": "EdgeCases",
      "expected_files": [
        "ace/config.py"
      ],
      "ace_files": [
        "ace/config.py",
        "ace/config.py",
        "verify_setup.py",
        "rag_training/training_data/crossencoder_training_pairs.json",
        "rag_training/training_data/crossencoder_training_pairs.json"
      ],
      "ace_scores": [
        0.88994325,
        0.8162849,
        0.6581501000000001,
        0.4963076,
        0.49479708
      ],
      "ace_contents": [
        "\"\"\"Centralized ACE configuration.\n\nAll embedding and retrieval settings in one place.\nOverride via environment variables or .env file.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom pathlib import Path\n\n# Load .env if python-dotenv is available\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent.parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n\ndef _get_env(key: str, default: s",
        "class MultiStageConfig:\n    \"\"\"\n    Multi-stage retrieval configuration (coarse-to-fine optimization).\n\n    Implements a 4-stage retrieval pipeline:\n    1. Stage 1 (Coarse): High-recall candidate retrieval (10x limit)\n    2. Stage 2 (Filter): Score-based filtering (DISABLED by default - RRF scores unreliable)\n    3. Stage 3 (Rerank): Cross-encoder reranking on all Stage 1 candidates\n    4. Stage 4 (Final): Deduplication and final selection\n\n    Benefits:\n    - Higher recall by fetching more cand",
        "#!/usr/bin/env python3\n\"\"\"\nSetup verification script for ACE Framework development environment.\n\"\"\"\n\nimport sys\nimport subprocess\nimport os\nfrom pathlib import Path\n\n\ndef run_command(cmd, description, check=True):\n    \"\"\"Run a command and return success status.\"\"\"\n    print(f\"\\n\ud83d\udd0d {description}\")\n    print(f\"Running: {cmd}\")\n    try:\n        result = subprocess.run(cmd, shell=True, check=check,\n                              capture_output=True, text=True, cwd=Path(__file__).parent)\n        if res",
        "  },\n  {\n    \"query\": \"I'm working on a project and need to know how to assert preconditions validate in my codebase for better maintainability\",\n    \"memory\": \"Assert preconditions and validate invariants before processing parameters.\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"I'm working on a project and need to know how to assert preconditions validate in my codebase for better maintainability\",\n    \"memory\": \"Centralize API keys in a single, environment-specific config file.\",\n    \"label\": 0\n  ",
        "  },\n  {\n    \"query\": \"how to structure env\",\n    \"memory\": \"Extract env checks and standardize health endpoints.\",\n    \"label\": 1\n  },\n  {\n    \"query\": \"how to structure env\",\n    \"memory\": \"Define model structure upfront to accelerate development.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"how to structure env\",\n    \"memory\": \"Centralize imports in __init__.py for consistent module access.\",\n    \"label\": 0\n  },\n  {\n    \"query\": \"how to structure env\",\n    \"memory\": \"\ud83d\udfe1 [DIRECTIVE] Always document "
      ],
      "ace_line_counts": [
        221,
        113,
        130,
        1620,
        520
      ],
      "auggie_files": [
        "ace/config.py"
      ],
      "auggie_contents": [
        "     1\t\"\"\"Centralized ACE configuration.\n     2\t\n     3\tAll embedding and retrieval settings in one place.\n     4\tOverride via environment variables or .env file.\n     5\t\"\"\"\n     6\t\n     7\timport os\n     8\tfrom dataclasses import dataclass, field\n     9\tfrom typing import Optional\n    10\tfrom pathlib import Path\n    11\t\n    12\t# Load .env if python-dotenv is available\n    13\ttry:\n... (516 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: view\u001b[0m\n   path: \".\"\n   type: \"directory\"\n\n\u001b[90m\ud83d\udccb Tool result: view\u001b[0m"
      ],
      "auggie_line_counts": [
        41
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 1",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (0.890)"
      ],
      "auggie_advantages": [
        "Better chunk size (41 lines avg)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 1,
      "auggie_expected_rank": 1
    },
    {
      "query": "search retrieval ranking score",
      "category": "EdgeCases",
      "expected_files": [
        "ace/code_retrieval.py"
      ],
      "ace_files": [
        "ace/retrieval.py",
        "ace/retrieval_optimized.py",
        "debug_doc_ranking.py",
        "ace/retrieval.py",
        "ace/retrieval_optimized.py"
      ],
      "ace_scores": [
        0.91051686,
        0.7439303500000001,
        0.7438757,
        0.7343708,
        0.7094392300000001
      ],
      "ace_contents": [
        "\"\"\"Smart retrieval system for purpose-aware bullet retrieval.\n\nThis module provides intelligent retrieval of bullets from a playbook\nusing semantic scaffolding metadata for purpose-aware, multi-dimensional filtering.\n\nELF-Inspired Features (when enabled via config):\n- Confidence Decay: Older knowledge scores lower over time\n- Golden Rules: High-performing strategies get score boost\n- Quality Boost: Helpful/harmful feedback affects ranking\n\nARIA Features (when enabled):\n- LinUCB Bandit: Dynamic p",
        "class RetrievalResult:\n    \"\"\"A single retrieval result with metadata.\"\"\"\n    id: int\n    score: float\n    payload: Dict[str, Any]\n    content: str\n    category: Optional[str] = None\n    reranked: bool = False\n\n\n@dataclass\nclass SearchMetrics:\n    \"\"\"Metrics for a search operation.\"\"\"\n    total_latency_ms: float\n    expansion_latency_ms: float\n    retrieval_latency_ms: float\n    rerank_latency_ms: float\n    num_candidates: int\n    num_results: int\n    expanded_queries: List[str]\n\n\n# ============",
        "\"\"\"Debug doc ranking for pattern queries.\"\"\"\n\nfrom ace.code_retrieval import CodeRetrieval\n\nr = CodeRetrieval()\n\n# Check if docs even get embedded/indexed\nquery = \"error handling patterns in Python with try except\"\nprint(f\"Query: {query}\")\nprint(\"=\" * 60)\n\n# Run search with more results to see docs\nresults = r.search(query, limit=50)\n\n# Separate code and docs\ncode_files = [res for res in results if not res['file_path'].endswith('.md')]\ndoc_files = [res for res in results if res['file_path'].ends",
        "    def retrieve(\n        self,\n        query: Optional[str] = None,\n        task_type: Optional[str] = None,\n        domain: Optional[str] = None,\n        complexity: Optional[str] = None,\n        intent: Optional[IntentType] = None,\n        limit: Optional[int] = None,\n        rank_by_effectiveness: bool = False,\n        min_effectiveness: Optional[float] = None,\n        query_type: Optional[str] = None,\n        trigger_override_threshold: float = 0.3,\n        session_type: Optional[str] = Non",
        "    def search(\n        self,\n        query: str,\n        limit: int = None,\n        return_metrics: bool = False\n    ) -> List[RetrievalResult] | Tuple[List[RetrievalResult], SearchMetrics]:\n        \"\"\"\n        Search for relevant memories.\n\n        Args:\n            query: Search query\n            limit: Maximum results (default from config)\n            return_metrics: Whether to return search metrics\n\n        Returns:\n            List of RetrievalResult, optionally with SearchMetrics\n        "
      ],
      "ace_line_counts": [
        61,
        87,
        34,
        393,
        116
      ],
      "auggie_files": [
        "ace/retrieval.py",
        "ace/retrieval.py"
      ],
      "auggie_contents": [
        "...\n   192\t\n   193\t        # 3. Quality Boost - helpful/harmful feedback affects ranking\n   194\t        # Always applied if data available (no separate flag needed)\n   195\t        helpful = getattr(bullet, 'helpful_count', 0) or 0\n   196\t        harmful = getattr(bullet, 'harmful_count', 0) or 0\n   197\t        total_feedback = helpful + harmful\n   198\t        if total_feedback > 0:\n   199\t            quality_ratio = (helpful - harmful) / total_feedback\n   200\t            # Scale: -1.0 to +1.0 -> -0.15 to +0.15 boost\n   201\t            quality_boost = quality_ratio * 0.15\n   202\t            score = max(0.05, score + quality_boost)  # Ensure minimum score\n   203\t            if abs(quality_boost) > 0.01:\n... (425 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"how are search results ranked and scored, relevance scoring, similarity scoring\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   192\t\n   193\t        # 3. Quality Boost - helpful/harmful feedback affects ranking\n   194\t        # Always applied if data available (no separate flag needed)\n   195\t        helpful = getattr(bullet, 'helpful_count', 0) or 0\n   196\t        harmful = getattr(bullet, 'harmful_count', 0) or 0\n   197\t        total_feedback = helpful + harmful\n   198\t        if total_feedback > 0:\n   199\t            quality_ratio = (helpful - harmful) / total_feedback\n   200\t            # Scale: -1.0 to +1.0 -> -0.15 to +0.15 boost\n   201\t            quality_boost = quality_ratio * 0.15\n   202\t            score = max(0.05, score + quality_boost)  # Ensure minimum score\n   203\t            if abs(quality_boost) > 0.01:\n... (462 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        19
      ],
      "winner": "ACE",
      "reason": "ACE wins with 2 advantages vs 0",
      "ace_advantages": [
        "More unique files (3 vs 0)",
        "High confidence top score (0.911)"
      ],
      "auggie_advantages": [],
      "ace_found_expected": false,
      "auggie_found_expected": false,
      "ace_expected_rank": -1,
      "auggie_expected_rank": -1
    },
    {
      "query": "indexing chunking embedding storage",
      "category": "EdgeCases",
      "expected_files": [
        "ace/code_indexer.py"
      ],
      "ace_files": [
        "ace/gemini_embeddings.py",
        "ace/code_indexer.py",
        "ace/code_retrieval.py",
        "ace/qdrant_retrieval.py",
        "ace/unified_memory.py"
      ],
      "ace_scores": [
        0.7775786,
        0.5607476,
        0.56006616,
        0.55396044,
        0.54930806
      ],
      "ace_contents": [
        "\"\"\"Gemini Embedding Client for ACE Framework.\n\nProvides embeddings using Google's gemini-embedding-001 model\nwith proper task type optimization for retrieval (document vs query).\n\nUsage:\n    from ace.gemini_embeddings import GeminiEmbeddingClient\n\n    client = GeminiEmbeddingClient(api_key=\"your-api-key\")\n\n    # For indexing documents\n    doc_embedding = client.embed_document(\"This is a document about...\")\n\n    # For search queries\n    query_embedding = client.embed_query(\"How do I fix...\")\n\"\"\"\n",
        "\"\"\"Code indexer module for workspace code indexing.\n\nThis module provides code indexing capabilities that scan a workspace,\nparse code files using ASTChunker, and store indexed chunks in Qdrant\nfor semantic code search.\n\nConfiguration:\n    ACE_CODE_COLLECTION: Qdrant collection name (default: ace_code_context)\n    ACE_CODE_EMBEDDING_DIM: Embedding dimension (default: from EmbeddingConfig)\n    QDRANT_URL: Qdrant server URL (default: http://localhost:6333)\n\nThe indexer supports:\n- Multi-language p",
        "\"\"\"\nCode Retrieval - Semantic search for code with Auggie-style output formatting.\n\nThis module provides:\n1. Semantic search over indexed code chunks\n2. Auggie MCP-compatible output formatting\n3. Blended results (code + memory)\n4. Result deduplication and ranking\n\nExample usage:\n    retriever = CodeRetrieval()\n    results = retriever.search(\"unified memory index\")\n    formatted = retriever.format_auggie_style(results)\n\"\"\"\n\nimport os\nimport logging\nfrom typing import List, Dict, Any, Optional, Ca",
        "\"\"\"Vector-based bullet retrieval using Qdrant hybrid search.\n\nThis module provides QdrantBulletIndex for O(1) semantic retrieval of playbook\nbullets using Qdrant vector database with hybrid search (dense + BM25 sparse).\n\nPhase 1: Vector Search Integration for ACE Fortune 100 Production Readiness.\n\nKey features:\n- Dense embeddings via LM Studio (nomic-embed-text-v1.5, 768-dim)\n- BM25 sparse vectors for keyword matching (technical terms)\n- Hybrid search with RRF fusion for best of both approaches\n",
        "class UnifiedMemoryIndex:\n    \"\"\"\n    Unified memory index using Qdrant with namespace support.\n\n    Provides:\n    - Hybrid search (dense + sparse/BM25)\n    - Namespace filtering\n    - Batch operations\n    - Integration with ACE SmartBulletIndex\n\n    Usage:\n        >>> index = UnifiedMemoryIndex(qdrant_url=\"http://localhost:6333\")\n        >>> index.create_collection()\n        >>> index.index_bullet(bullet)\n        >>> results = index.retrieve(\"query\", namespace=UnifiedNamespace.USER_PREFS)\n    \""
      ],
      "ace_line_counts": [
        88,
        55,
        147,
        281,
        114
      ],
      "auggie_files": [
        "ace/code_indexer.py",
        "docs/Fortune100.md",
        "ace/code_chunker.py"
      ],
      "auggie_contents": [
        "...\n    38\t\n    39\t@dataclass\n    40\tclass CodeChunkIndexed:\n    41\t    \"\"\"A code chunk ready for indexing with all metadata.\"\"\"\n    42\t    \n    43\t    content: str\n    44\t    file_path: str  # Relative to workspace root\n    45\t    start_line: int\n    46\t    end_line: int\n    47\t    language: str\n    48\t    symbols: List[str] = field(default_factory=list)\n    49\t    chunk_hash: str = \"\"\n... (427 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"vector database storage Qdrant FAISS embedding models sentence transformers\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "...\n   135\t\n   136\t```python\n   137\t# From ace_qdrant_memory.py - ALREADY IMPLEMENTED\n   138\tQDRANT_URL = \"http://localhost:6333\"\n   139\tLMSTUDIO_URL = \"http://192.168.10.64:1234\"\n   140\tEMBEDDING_MODEL = \"text-embedding-snowflake-arctic-embed-m-v1.5\"  # Snowflake model\n   141\tCOLLECTION_NAME = \"ace_memories_hybrid\"\n   142\tEMBEDDING_DIM = 768  # snowflake-arctic-embed-m dimension\n   143\t\n   144\t# BM25 parameters (already tuned)\n   145\tBM25_K1 = 1.5\n   146\tBM25_B = 0.75\n... (517 more lines)\n\n\u001b[90m\ud83d\udd27 Tool call: codebase-retrieval\u001b[0m\n   information_request: \"document chunking strategies text splitting chunk size overlap\"\n\n\u001b[90m\ud83d\udccb Tool result: codebase-retrieval\u001b[0m\nThe following code sections were retrieved:",
        "     1\t\"\"\"AST-based semantic code chunking module.\n     2\t\n     3\tThis module provides intelligent code chunking that respects language syntax\n     4\tboundaries (functions, classes, methods) rather than arbitrary line counts.\n     5\t\n     6\tSupports multiple languages via tree-sitter:\n     7\t- Python (via built-in ast module or tree-sitter)\n     8\t- JavaScript/TypeScript (via tree-sitter)\n     9\t- Go (via tree-sitter)\n    10\t\n    11\tConfiguration:\n    12\t    ACE_ENABLE_AST_CHUNKING: Enable/disable AST chunking (default: false)\n    13\t    ACE_AST_MAX_LINES: Maximum lines per chunk (default: 120)\n... (469 more lines)\n\n\u26a0\ufe0f The conversation has been paused because maximum iterations reached (1).\nYou can continue the conversation by running the cli with -c command.\n\n"
      ],
      "auggie_line_counts": [
        20,
        20,
        19
      ],
      "winner": "AUGGIE",
      "reason": "Auggie wins with 3 advantages vs 2",
      "ace_advantages": [
        "More unique files (4 vs 2)",
        "High confidence top score (0.778)"
      ],
      "auggie_advantages": [
        "Higher rank for expected file (1 vs 2)"
      ],
      "ace_found_expected": true,
      "auggie_found_expected": true,
      "ace_expected_rank": 2,
      "auggie_expected_rank": 1
    }
  ]
}